# XGet ç¤¾åª’æœç´¢é‡‡é›†ç³»ç»Ÿ - ç”Ÿäº§å°±ç»ªç‰ˆæœ¬

## é¡¹ç›®æ¦‚è¿°

åŸºäºç”²æ–¹éœ€æ±‚æ–‡æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸šçš„X(Twitter)æ•°æ®é‡‡é›†ç³»ç»Ÿï¼Œæä¾›å…³é”®è¯æœç´¢ã€æ•°æ®è§£æã€èµ„æºä¸‹è½½å’ŒAPIæ¥å£æœåŠ¡ã€‚ç³»ç»Ÿæ”¯æŒå®šæ—¶æœç´¢å’ŒæŒ‰éœ€æœç´¢ä¸¤ç§æ¨¡å¼ï¼Œå®Œæ•´å®ç°ç”²ä¹™åŒæ–¹çš„æ•°æ®äº¤äº’éœ€æ±‚ã€‚

### ğŸ¯ **æ ¸å¿ƒä¸šåŠ¡éœ€æ±‚**

1. **å…³é”®è¯æœç´¢é‡‡é›†** - æ”¯æŒå¤šè¯­è¨€å…³é”®è¯çš„Xå¹³å°æœç´¢
2. **åŒæ¨¡å¼ä»»åŠ¡** - å®šæ—¶æœç´¢ + æŒ‰éœ€æœç´¢
3. **å®Œæ•´æ•°æ®è§£æ** - å¸–å­ã€ä½œè€…ã€åª’ä½“ã€äº’åŠ¨æ•°æ®
4. **èµ„æºä¸‹è½½ç®¡ç†** - å›¾ç‰‡/è§†é¢‘ä¸‹è½½åˆ°OSS
5. **æ ‡å‡†APIæ¥å£** - ä»»åŠ¡ä¸‹å‘ã€ç»“æœä¸ŠæŠ¥ã€æ•°æ®ç»Ÿè®¡
6. **æ•°æ®å»é‡ç»Ÿè®¡** - æŒ‰å…³é”®è¯æœˆåº¦å»é‡è®¡é‡

### ğŸ“‹ **ç”²æ–¹éœ€æ±‚åˆ†æ**

#### **æœç´¢ä»»åŠ¡ç±»å‹**
1. **å®šæ—¶æœç´¢ä»»åŠ¡**
   - å‚æ•°ï¼šå…³é”®è¯ã€ä¼˜å…ˆçº§ã€æœç´¢é¢‘ç‡
   - æŒ‰é¢‘ç‡å’Œä¼˜å…ˆçº§è‡ªåŠ¨æ‰§è¡Œ

2. **æŒ‰éœ€æœç´¢ä»»åŠ¡**
   - å‚æ•°ï¼šå…³é”®è¯ã€ä¼˜å…ˆçº§ã€éœ€è¦æ¡æ•°ã€å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´
   - æ¡æ•°=0æ—¶å°½åŠ›é‡‡é›†ï¼Œâ‰ 0æ—¶è¾¾åˆ°ç›®æ ‡åç»“æŸ

#### **æ•°æ®å­—æ®µè¦æ±‚**
- **åŸºç¡€å¸–å­ä¿¡æ¯**ï¼šåœ°å€ã€IDã€ç±»å‹ã€å†…å®¹ã€æ—¶é—´
- **ä½œè€…ä¿¡æ¯**ï¼šå¤´åƒã€åå­—ã€handle
- **äº’åŠ¨æ•°æ®**ï¼šè¯„è®ºæ•°ã€è½¬å‘æ•°ã€ç‚¹èµæ•°ã€æ›å…‰æ•°ã€bookmark
- **åª’ä½“èµ„æº**ï¼šå›¾ç‰‡ã€è§†é¢‘å°é¢ï¼ˆæ•°ç»„å½¢å¼ï¼‰
- **è½¬å‘å¸–ç‰¹æ®Šå­—æ®µ**ï¼šåŸè´´ä¿¡æ¯ã€å…³ç³»æ ‡è¯†
- **å…¶ä»–å­—æ®µ**ï¼šé“¾æ¥ã€OSSåœ°å€ã€åŸå§‹æ•°æ®

## æ ¸å¿ƒåŸåˆ™

1. **ä¸šåŠ¡éœ€æ±‚é©±åŠ¨** - ä¸¥æ ¼æŒ‰ç…§ç”²æ–¹éœ€æ±‚æ–‡æ¡£å®ç°
2. **APIæ ‡å‡†åŒ–** - æä¾›æ ‡å‡†çš„ä»»åŠ¡ä¸‹å‘å’Œç»“æœä¸ŠæŠ¥æ¥å£
3. **æ•°æ®å®Œæ•´æ€§** - ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µçš„å‡†ç¡®é‡‡é›†
4. **èµ„æºç®¡ç†** - å®Œæ•´çš„å›¾ç‰‡/è§†é¢‘ä¸‹è½½å’ŒOSSå­˜å‚¨
5. **å¯æ‰©å±•æ¶æ„** - æ”¯æŒåç»­Facebookã€Instagramç­‰å¹³å°

## æŠ€æœ¯æ ˆé€‰æ‹©

### ğŸš€ **åŸºäºApifyæ¶æ„ä¼˜åŒ–çš„æŠ€æœ¯æ ˆ**

å‚è€ƒApify Twitter Scraperçš„æˆåŠŸç»éªŒï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ä¼˜åŒ–çš„æŠ€æœ¯ç»„åˆï¼š

#### **æ ¸å¿ƒé‡‡é›†æŠ€æœ¯**
- **ç¼–ç¨‹è¯­è¨€**: Python 3.12.11 (å·²éªŒè¯) / Python 3.9+ (æœ€ä½è¦æ±‚)
- **ä¸»è¦çˆ¬å–æ¡†æ¶**: **é›†æˆå¼twscrape** (åŸºäºtwscrape 0.17.0å®šåˆ¶å¼€å‘)
- **ç½‘ç»œå±‚**: httpx 0.28.1 (å¼‚æ­¥HTTPå®¢æˆ·ç«¯)
- **å¤‡ç”¨é‡‡é›†æ–¹æ¡ˆ**: Apify-style Actoræ¨¡å¼ (åŸºäºPlaywright + é«˜çº§åæ£€æµ‹)
- **æµè§ˆå™¨è‡ªåŠ¨åŒ–**: Playwright 1.53.0 (å·²éªŒè¯ï¼Œcookiesç®¡ç† + ç‰¹æ®Šåœºæ™¯)
- **æŸ¥è¯¢ä¼˜åŒ–**: Twitteré«˜çº§æœç´¢è¯­æ³•æ”¯æŒ (å‚è€ƒApify Query Wizard)

#### **ğŸ”§ twscrapeé›†æˆç­–ç•¥**

```text
XGeté¡¹ç›®ç»“æ„
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ xget_core/              # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ xget_scraper/           # é›†æˆçš„twscrape (å®šåˆ¶ç‰ˆ)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py              # åŸºäºtwscrape.APIçš„å¢å¼ºç‰ˆ
â”‚   â”‚   â”œâ”€â”€ models.py           # æ‰©å±•çš„æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ accounts.py         # è´¦å·ç®¡ç†å¢å¼º
â”‚   â”‚   â”œâ”€â”€ utils.py            # å·¥å…·å‡½æ•°
â”‚   â”‚   â””â”€â”€ exceptions.py       # è‡ªå®šä¹‰å¼‚å¸¸
â”‚   â”œâ”€â”€ xget_api/               # FastAPIæ¥å£
â”‚   â””â”€â”€ xget_web/               # Webç®¡ç†ç•Œé¢
â”œâ”€â”€ third_party/
â”‚   â””â”€â”€ twscrape/               # åŸå§‹twscrapeæºç  (ä½œä¸ºå‚è€ƒ)
â””â”€â”€ requirements.txt
```

#### **ç³»ç»Ÿæ¶æ„ç»„ä»¶**
- **ä»»åŠ¡é˜Ÿåˆ—**: Celery + Redis (æ”¯æŒäº‹ä»¶é©±åŠ¨å®šä»·æ¨¡å¼)
- **æ•°æ®å­˜å‚¨**: MongoDB (æ–‡æ¡£å­˜å‚¨) + Redis (ç¼“å­˜/ä¼šè¯)
- **APIæ¡†æ¶**: FastAPI + Uvicorn (é«˜æ€§èƒ½å¼‚æ­¥API)
- **é…ç½®ç®¡ç†**: Pydantic Settings + ç¯å¢ƒå˜é‡
- **æ—¥å¿—ç³»ç»Ÿ**: Structured Logging (JSONæ ¼å¼)
- **ç›‘æ§**: Prometheus + Grafana (å¯é€‰)
- **éƒ¨ç½²**: Docker Compose (å¼€å‘) + Docker Swarm/K8s (ç”Ÿäº§)

### âœ… æŠ€æœ¯éªŒè¯çŠ¶æ€
- **ğŸ‰ æ ¸å¿ƒæŠ€æœ¯æ ˆ**: 100% éªŒè¯é€šè¿‡
- **ğŸª Cookiesæ–¹æ¡ˆ**: å·²è§£å†³ç™»å½•éš¾é¢˜
- **ğŸ“Š æ•°æ®é‡‡é›†**: å·²éªŒè¯å¯è·å–çœŸå®æ•°æ®
- **ğŸ­ æµè§ˆå™¨è‡ªåŠ¨åŒ–**: å…¨åŠŸèƒ½éªŒè¯é€šè¿‡
- **ğŸš€ å¼€å‘å°±ç»ª**: å¯ç«‹å³å¼€å§‹é¡¹ç›®å¼€å‘

### ğŸ¯ **Apify-inspired æŸ¥è¯¢ä¼˜åŒ–å™¨**

åŸºäºApify Twitter Scraperçš„æˆåŠŸç»éªŒï¼Œå®ç°æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–ï¼š

```python
# services/query_optimizer.py
class TwitterQueryOptimizer:
    """
    åŸºäºApifyç»éªŒçš„æŸ¥è¯¢ä¼˜åŒ–å™¨
    æ”¯æŒTwitteré«˜çº§æœç´¢è¯­æ³•ï¼Œæé«˜é‡‡é›†æ•ˆç‡å’Œæˆæœ¬æ§åˆ¶
    """

    def optimize_profile_scraping(self, username: str, date_ranges: List[Tuple]) -> List[str]:
        """
        ä¼˜åŒ–ç”¨æˆ·èµ„æ–™é‡‡é›† - åˆ†æ—¶é—´æ®µæŸ¥è¯¢
        å‚è€ƒApifyå»ºè®®ï¼šæ¯æœˆæœ€å¤š800æ¡æ¨æ–‡ï¼Œéœ€è¦åˆ†æ®µæŸ¥è¯¢
        """
        queries = []
        for start_date, end_date in date_ranges:
            query = f"from:{username} since:{start_date} until:{end_date}"
            queries.append(query)
        return queries

    def build_advanced_search(self, keyword: str, filters: Dict) -> str:
        """
        æ„å»ºé«˜çº§æœç´¢æŸ¥è¯¢ - æ”¯æŒApify-styleçš„å¤æ‚è¿‡æ»¤æ¡ä»¶
        å‚è€ƒ: https://github.com/igorbrigadir/twitter-advanced-search
        """
        query_parts = [keyword]

        # åª’ä½“è¿‡æ»¤ (Apifyæ”¯æŒçš„è¿‡æ»¤å™¨)
        if filters.get('only_images'):
            query_parts.append('filter:images')
        elif filters.get('only_videos'):
            query_parts.append('filter:videos')
        elif filters.get('exclude_images'):
            query_parts.append('-filter:images')

        # äº’åŠ¨è¿‡æ»¤
        if filters.get('min_likes'):
            query_parts.append(f"min_faves:{filters['min_likes']}")
        if filters.get('min_retweets'):
            query_parts.append(f"min_retweets:{filters['min_retweets']}")

        # ç”¨æˆ·ç±»å‹è¿‡æ»¤
        if filters.get('only_verified'):
            query_parts.append('filter:verified')
        if filters.get('only_blue_verified'):
            query_parts.append('filter:blue_verified')
        if filters.get('exclude_retweets'):
            query_parts.append('-filter:nativeretweets')

        # è¯­è¨€è¿‡æ»¤
        if filters.get('language'):
            query_parts.append(f"lang:{filters['language']}")

        return ' '.join(query_parts)

    def generate_conversation_queries(self, tweet_id: str, hashtag: str = None) -> List[str]:
        """
        ç”Ÿæˆå¯¹è¯æŸ¥è¯¢ - è·å–æ¨æ–‡å›å¤
        """
        base_query = f"conversation_id:{tweet_id}"
        if hashtag:
            base_query += f" #{hashtag}"
        return [base_query]

    def optimize_batch_queries(self, queries: List[str]) -> Dict[str, List[str]]:
        """
        æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ– - æ ¹æ®Apifyå®šä»·æ¨¡å‹ä¼˜åŒ–æŸ¥è¯¢åˆ†ç»„
        """
        # æŒ‰æŸ¥è¯¢å¤æ‚åº¦åˆ†ç»„ï¼Œä¼˜åŒ–æˆæœ¬
        simple_queries = []
        complex_queries = []

        for query in queries:
            if len(query.split()) <= 3:
                simple_queries.append(query)
            else:
                complex_queries.append(query)

        return {
            "simple_batch": simple_queries,
            "complex_batch": complex_queries
        }
```

### ğŸ’° **äº‹ä»¶é©±åŠ¨å®šä»·æ¨¡å‹**

å‚è€ƒApifyçš„é€æ˜å®šä»·ç­–ç•¥ï¼Œä¸ºç”²æ–¹æä¾›çµæ´»çš„æˆæœ¬æ§åˆ¶ï¼š

```python
# services/pricing_calculator.py
class PricingCalculator:
    """
    åŸºäºApifyæ¨¡å¼çš„äº‹ä»¶é©±åŠ¨å®šä»·è®¡ç®—å™¨
    """

    # åŸºç¡€å®šä»· (å‚è€ƒApifyæ¨¡å‹)
    QUERY_COSTS = {
        "standard_query": 0.016,      # æ ‡å‡†æŸ¥è¯¢ (åŒ…å«å‰2é¡µçº¦40æ¡æ¨æ–‡)
        "single_tweet": 0.05,         # å•æ¡æ¨æ–‡æŸ¥è¯¢
        "profile_query": 0.016,       # ç”¨æˆ·èµ„æ–™æŸ¥è¯¢
        "search_query": 0.016         # æœç´¢æŸ¥è¯¢
    }

    # åˆ†å±‚å®šä»· (æŒ‰æ‰¹æ¬¡å¤§å°)
    TIER_PRICING = {
        1: {"max_queries": 5, "cost_per_item": 0.0004},      # â‰¤5æŸ¥è¯¢
        2: {"max_queries": 10, "cost_per_item": 0.0008},     # 6-10æŸ¥è¯¢
        3: {"max_queries": 30, "cost_per_item": 0.0012},     # 11-30æŸ¥è¯¢
        4: {"max_queries": 100, "cost_per_item": 0.0016},    # 31-100æŸ¥è¯¢
        5: {"max_queries": float('inf'), "cost_per_item": 0.002}  # >100æŸ¥è¯¢
    }

    def calculate_task_cost(self, task_params: Dict) -> Dict[str, float]:
        """
        è®¡ç®—ä»»åŠ¡æˆæœ¬
        """
        task_type = task_params.get("type")
        query_count = task_params.get("query_count", 1)
        expected_items = task_params.get("expected_items", 0)

        # åŸºç¡€æŸ¥è¯¢æˆæœ¬
        if task_type == "single_tweet":
            base_cost = self.QUERY_COSTS["single_tweet"]
        else:
            base_cost = self.QUERY_COSTS["standard_query"] * query_count

        # ç¡®å®šå®šä»·å±‚çº§
        tier = self._get_pricing_tier(query_count)

        # è®¡ç®—æ•°æ®é¡¹æˆæœ¬ (è¶…å‡ºå…è´¹é¢åº¦çš„éƒ¨åˆ†)
        free_items = min(query_count * 40, expected_items)  # æ¯æŸ¥è¯¢40æ¡å…è´¹
        paid_items = max(0, expected_items - free_items)

        item_cost = paid_items * self.TIER_PRICING[tier]["cost_per_item"]

        total_cost = base_cost + item_cost

        return {
            "base_cost": base_cost,
            "item_cost": item_cost,
            "total_cost": total_cost,
            "tier": tier,
            "free_items": free_items,
            "paid_items": paid_items
        }

    def _get_pricing_tier(self, query_count: int) -> int:
        """ç¡®å®šå®šä»·å±‚çº§"""
        for tier, config in self.TIER_PRICING.items():
            if query_count <= config["max_queries"]:
                return tier
        return 5
```

### ğŸ”§ **twscrapeé›†æˆå®æ–½æ–¹æ¡ˆ**

#### **1. é›†æˆæ–¹å¼é€‰æ‹©**

**æ¨èæ–¹æ¡ˆï¼šFork + å®šåˆ¶å¼€å‘**

```bash
# 1. Fork twscrapeåˆ°æ‚¨çš„ç»„ç»‡
git clone https://github.com/vladkens/twscrape.git third_party/twscrape

# 2. åˆ›å»ºå®šåˆ¶ç‰ˆæœ¬
cp -r third_party/twscrape/twscrape src/xget_scraper/
```

#### **2. å®šåˆ¶å¼€å‘é‡ç‚¹**

```python
# src/xget_scraper/enhanced_api.py
from typing import Dict, List, Optional, AsyncGenerator
from .api import API as BaseAPI
from ..models import XGetTweet, XGetUser

class XGetAPI(BaseAPI):
    """
    åŸºäºtwscrapeçš„å¢å¼ºAPI
    é’ˆå¯¹ç”²æ–¹éœ€æ±‚è¿›è¡Œå®šåˆ¶ä¼˜åŒ–
    """

    def __init__(self, pool_file: str = "accounts.db"):
        super().__init__(pool_file)
        self.request_count = 0
        self.success_count = 0
        self.error_count = 0

    async def search_enhanced(
        self,
        query: str,
        limit: int = 20,
        task_id: str = None,
        **kwargs
    ) -> AsyncGenerator[XGetTweet, None]:
        """
        å¢å¼ºçš„æœç´¢åŠŸèƒ½
        - æ·»åŠ ä»»åŠ¡è¿½è¸ª
        - å¢å¼ºé”™è¯¯å¤„ç†
        - æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
        """
        try:
            self.request_count += 1

            async for tweet in self.search(query, limit, **kwargs):
                # è½¬æ¢ä¸ºç”²æ–¹è¦æ±‚çš„æ•°æ®æ ¼å¼
                xget_tweet = self._convert_to_xget_format(tweet, task_id)
                self.success_count += 1
                yield xget_tweet

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Search failed for query: {query}, error: {str(e)}")
            raise

    def _convert_to_xget_format(self, tweet: Tweet, task_id: str) -> XGetTweet:
        """
        è½¬æ¢ä¸ºç”²æ–¹è¦æ±‚çš„20ä¸ªå­—æ®µæ ¼å¼
        """
        return XGetTweet(
            # ç”²æ–¹å­—æ®µ0: åŸºç¡€ä¿¡æ¯
            post_url=f"https://x.com/{tweet.user.username}/status/{tweet.id}",
            post_id=str(tweet.id),
            post_type=self._determine_post_type(tweet),

            # ç”²æ–¹å­—æ®µ1-3: ä½œè€…ä¿¡æ¯
            author_avatar=tweet.user.profileImageUrl,
            author_name=tweet.user.displayname,
            author_handle=f"@{tweet.user.username}",

            # ç”²æ–¹å­—æ®µ4: æ—¶é—´
            post_time=tweet.date,

            # ç”²æ–¹å­—æ®µ5: å†…å®¹
            post_content=tweet.rawContent,

            # ç”²æ–¹å­—æ®µ6: åª’ä½“ (æ•°ç»„å½¢å¼)
            post_images=self._process_media(tweet.media),

            # ç”²æ–¹å­—æ®µ7-10, 17: äº’åŠ¨æ•°æ®
            comment_count=tweet.replyCount or 0,
            retweet_count=tweet.retweetCount or 0,
            like_count=tweet.likeCount or 0,
            view_count=tweet.viewCount or 0,
            bookmark_count=tweet.bookmarkCount or 0,

            # ç”²æ–¹å­—æ®µ18-19: ç±»å‹æ ‡è¯†
            is_retweet=bool(tweet.retweetedTweet),
            is_quote=bool(tweet.quotedTweet),

            # ç”²æ–¹å­—æ®µ11-16: è½¬å‘å¸–ä¿¡æ¯
            **self._process_retweet_info(tweet),

            # ç”²æ–¹å­—æ®µ20: é“¾æ¥ä¿¡æ¯
            post_links=self._extract_links(tweet),

            # ç”²æ–¹è¦æ±‚çš„å…³ç³»å­—æ®µ
            parent_post_id=tweet.inReplyToTweetId,
            parent_comment_id=tweet.inReplyToUserId,

            # ç³»ç»Ÿå­—æ®µ
            task_id=task_id,
            collected_at=datetime.utcnow(),

            # ç”²æ–¹è¦æ±‚ä¿ç•™åŸå§‹æ•°æ®
            raw_data={
                "twscrape_tweet": tweet.dict(),
                "collection_method": "xget_enhanced_twscrape"
            }
        )

    async def get_account_health(self) -> Dict:
        """
        è·å–è´¦å·æ± å¥åº·çŠ¶æ€
        """
        accounts = await self.pool.accounts()

        health_stats = {
            "total_accounts": len(accounts),
            "active_accounts": 0,
            "suspended_accounts": 0,
            "error_accounts": 0,
            "request_stats": {
                "total_requests": self.request_count,
                "successful_requests": self.success_count,
                "failed_requests": self.error_count,
                "success_rate": self.success_count / max(self.request_count, 1)
            }
        }

        for account in accounts:
            if account.active:
                health_stats["active_accounts"] += 1
            elif "suspended" in str(account.status).lower():
                health_stats["suspended_accounts"] += 1
            else:
                health_stats["error_accounts"] += 1

        return health_stats
```

#### **3. ç‰ˆæœ¬ç®¡ç†ç­–ç•¥**

```python
# src/xget_scraper/__init__.py
"""
XGetå®šåˆ¶ç‰ˆtwscrape
åŸºäºtwscrape 0.17.0å®šåˆ¶å¼€å‘

ç‰ˆæœ¬ç®¡ç†ï¼š
- ä¸Šæ¸¸ç‰ˆæœ¬: twscrape 0.17.0
- XGetç‰ˆæœ¬: 1.0.0
- æœ€ååŒæ­¥: 2024-01-01
"""

__version__ = "1.0.0"
__upstream_version__ = "0.17.0"
__last_sync__ = "2024-01-01"

from .enhanced_api import XGetAPI
from .models import XGetTweet, XGetUser

__all__ = ["XGetAPI", "XGetTweet", "XGetUser"]
```

#### **4. å‡çº§å’Œç»´æŠ¤ç­–ç•¥**

```bash
# scripts/sync_upstream.sh
#!/bin/bash
# åŒæ­¥ä¸Šæ¸¸twscrapeæ›´æ–°çš„è„šæœ¬

echo "å¼€å§‹åŒæ­¥ä¸Šæ¸¸twscrape..."

# 1. è·å–æœ€æ–°çš„twscrape
cd third_party/twscrape
git pull origin main

# 2. æ£€æŸ¥å˜æ›´
git log --oneline --since="2024-01-01" > ../../docs/upstream_changes.log

# 3. åˆ›å»ºåˆå¹¶åˆ†æ”¯
cd ../../
git checkout -b sync-upstream-$(date +%Y%m%d)

# 4. æ‰‹åŠ¨åˆå¹¶å…³é”®æ›´æ–°
echo "è¯·æ‰‹åŠ¨æ£€æŸ¥ docs/upstream_changes.log å¹¶åˆå¹¶å¿…è¦çš„æ›´æ–°"
echo "é‡ç‚¹å…³æ³¨ï¼š"
echo "- APIæ¥å£å˜æ›´"
echo "- æ•°æ®æ¨¡å‹å˜æ›´"
echo "- é”™è¯¯å¤„ç†æ”¹è¿›"
echo "- æ€§èƒ½ä¼˜åŒ–"
```

### å¿…è¦çš„ç”Ÿäº§ç»„ä»¶
- **ä»£ç†IPç®¡ç†** - ç”Ÿäº§ç¯å¢ƒå¿…éœ€
- **è´¦å·æ± ç®¡ç†** - Cookieè½®æ¢å’Œå¥åº·æ£€æŸ¥
- **é”™è¯¯å¤„ç†å’Œé‡è¯•** - æé«˜ç³»ç»Ÿç¨³å®šæ€§
- **æ•°æ®éªŒè¯** - Pydanticæ¨¡å‹éªŒè¯
- **åŸºç¡€ç›‘æ§** - ç³»ç»Ÿå¥åº·çŠ¶æ€ç›‘æ§
- **é…ç½®ç®¡ç†** - ç¯å¢ƒéš”ç¦»å’Œé…ç½®çƒ­æ›´æ–°

## ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            XGet æ¶æ„                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Web API       â”‚   ä»»åŠ¡è°ƒåº¦       â”‚   æ•°æ®å­˜å‚¨       â”‚   åŸºç¡€è®¾æ–½     â”‚
â”‚   (FastAPI)     â”‚   (Celery)      â”‚   (MongoDB)     â”‚   (ç›‘æ§/æ—¥å¿—)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é‡‡é›†å¼•æ“      â”‚    â”‚  èµ„æºç®¡ç†      â”‚    â”‚  æ•°æ®å¤„ç†      â”‚
â”‚  twscrape     â”‚    â”‚  è´¦å·/ä»£ç†æ±     â”‚    â”‚  éªŒè¯/å­˜å‚¨     â”‚
â”‚  Playwright   â”‚    â”‚  å¥åº·æ£€æŸ¥      â”‚    â”‚  Redisç¼“å­˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

1. **é‡‡é›†å¼•æ“å±‚** - è´Ÿè´£å®é™…çš„æ•°æ®æŠ“å–
2. **èµ„æºç®¡ç†å±‚** - ç®¡ç†è´¦å·æ± ã€ä»£ç†IPç­‰èµ„æº
3. **æ•°æ®å¤„ç†å±‚** - æ•°æ®éªŒè¯ã€å­˜å‚¨ã€ç¼“å­˜
4. **APIæœåŠ¡å±‚** - å¯¹å¤–æä¾›æ¥å£æœåŠ¡
5. **ä»»åŠ¡è°ƒåº¦å±‚** - åˆ†å¸ƒå¼ä»»åŠ¡ç®¡ç†
6. **åŸºç¡€è®¾æ–½å±‚** - ç›‘æ§ã€æ—¥å¿—ã€é…ç½®ç®¡ç†

## ğŸ¯ æŠ€æœ¯åˆ†å±‚æ¶æ„è¯¦è§£

åŸºäºæŠ€æœ¯éªŒè¯ç»“æœï¼ŒXGeté¡¹ç›®é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œæ¯å±‚è´Ÿè´£ç‰¹å®šåŠŸèƒ½ï¼Œå®ç°é«˜æ•ˆåä½œï¼š

### ğŸ“‹ **æŠ€æœ¯åˆ†å±‚æ¦‚è§ˆ**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        XGet æŠ€æœ¯åˆ†å±‚æ¶æ„                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸª è®¤è¯å±‚      â”‚  ğŸ“Š æ•°æ®é‡‡é›†å±‚  â”‚   ğŸ”§ ç®¡ç†å±‚     â”‚   ğŸš€ åº”ç”¨å±‚    â”‚
â”‚   (Playwright)  â”‚   (twscrape)    â”‚   (Python)      â”‚   (FastAPI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸª **ç¬¬ä¸€å±‚ï¼šè®¤è¯å±‚ï¼ˆPlaywrightï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šè‡ªåŠ¨åŒ–cookiesè·å–å’Œç™»å½•ç®¡ç†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **è‡ªåŠ¨ç™»å½•è·å–cookies** - è§£æ”¾äººå·¥æ“ä½œ
- **æ‰¹é‡è´¦å·ç®¡ç†** - æ”¯æŒå¤šè´¦å·è‡ªåŠ¨åŒ–
- **cookiesè‡ªåŠ¨åˆ·æ–°** - è¿‡æœŸæ—¶è‡ªåŠ¨æ›´æ–°
- **å¤„ç†ç™»å½•éªŒè¯** - éªŒè¯ç ã€é‚®ç®±éªŒè¯ç­‰

#### ğŸ”§ **æŠ€æœ¯å®ç°**ï¼š
```python
# è‡ªåŠ¨åŒ–cookiesè·å–
async def auto_extract_cookies(account_info):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        # 1. è‡ªåŠ¨ç™»å½•
        await auto_login(page, account_info)

        # 2. æå–cookies
        cookies = await page.context.cookies()

        # 3. å¯¼å…¥åˆ°twscrape
        await import_to_twscrape(cookies)

        return cookies
```

#### ğŸ¯ **åº”ç”¨åœºæ™¯**ï¼š
- **æ‰¹é‡è´¦å·åˆå§‹åŒ–** - æ–°é¡¹ç›®å¯åŠ¨æ—¶
- **å®šæœŸç»´æŠ¤** - cookiesè¿‡æœŸè‡ªåŠ¨åˆ·æ–°
- **æ•…éšœæ¢å¤** - è´¦å·è¢«é™åˆ¶æ—¶é‡æ–°è·å–
- **æ‰©å®¹æ”¯æŒ** - æ–°å¢è´¦å·æ—¶è‡ªåŠ¨é…ç½®

### ğŸ“Š **ç¬¬äºŒå±‚ï¼šæ•°æ®é‡‡é›†å±‚ï¼ˆtwscrapeï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šé«˜æ•ˆçš„Twitteræ•°æ®é‡‡é›†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **æ¨æ–‡æœç´¢** - å…³é”®è¯ã€è¯é¢˜ã€ç”¨æˆ·æœç´¢
- **ç”¨æˆ·ä¿¡æ¯è·å–** - å®Œæ•´çš„ç”¨æˆ·èµ„æ–™
- **é€Ÿç‡é™åˆ¶ç®¡ç†** - å†…ç½®æ™ºèƒ½é™åˆ¶
- **å¤šè´¦å·è½®æ¢** - è‡ªåŠ¨è´¦å·åˆ‡æ¢

#### ğŸ”§ **æŠ€æœ¯ä¼˜åŠ¿**ï¼š
```python
# é«˜æ•ˆæ•°æ®é‡‡é›†
async def efficient_data_collection():
    api = API()  # ä½¿ç”¨Playwrightç»´æŠ¤çš„cookies

    # é«˜æ€§èƒ½æœç´¢
    tweets = []
    async for tweet in api.search("python", limit=1000):
        tweets.append(process_tweet(tweet))

    return tweets
```

#### ğŸ“ˆ **æ€§èƒ½ç‰¹ç‚¹**ï¼š
- **âš¡ é«˜æ•ˆç‡** - APIè°ƒç”¨æ¯”æµè§ˆå™¨å¿«10å€
- **ğŸ›¡ï¸ ç¨³å®šæ€§** - ä¸“é—¨ä¼˜åŒ–çš„åæ£€æµ‹
- **ğŸ”„ æ™ºèƒ½è½®æ¢** - è‡ªåŠ¨ç®¡ç†è´¦å·å’Œè¯·æ±‚
- **ğŸ“Š ç»“æ„åŒ–æ•°æ®** - ç›´æ¥è¿”å›Pythonå¯¹è±¡

### ğŸ”§ **ç¬¬ä¸‰å±‚ï¼šç®¡ç†å±‚ï¼ˆPythonè„šæœ¬ï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šåè°ƒä¸¤ä¸ªå·¥å…·ï¼Œæä¾›ç»Ÿä¸€ç®¡ç†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **å·¥å…·åè°ƒ** - Playwright + twscrapeåä½œ
- **é”™è¯¯å¤„ç†** - ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•
- **æ•°æ®å¤„ç†** - æ¸…æ´—ã€éªŒè¯ã€å­˜å‚¨
- **ç›‘æ§æ—¥å¿—** - ç³»ç»ŸçŠ¶æ€ç›‘æ§

#### ğŸ”§ **æ¶æ„è®¾è®¡**ï¼š
```python
class XGetManager:
    """XGetç»Ÿä¸€ç®¡ç†å™¨"""

    def __init__(self):
        self.playwright_manager = PlaywrightManager()
        self.twscrape_manager = TwscrapeManager()
        self.data_manager = DataManager()

    async def collect_data(self, keyword: str):
        # 1. ç¡®ä¿cookiesæœ‰æ•ˆ
        await self.playwright_manager.ensure_cookies_valid()

        # 2. æ‰§è¡Œæ•°æ®é‡‡é›†
        tweets = await self.twscrape_manager.search(keyword)

        # 3. æ•°æ®å¤„ç†å’Œå­˜å‚¨
        processed_data = await self.data_manager.process(tweets)

        return processed_data
```

#### ğŸ¯ **æ ¸å¿ƒä»·å€¼**ï¼š
- **ğŸ”— æ— ç¼é›†æˆ** - ä¸¤ä¸ªå·¥å…·å®Œç¾é…åˆ
- **ğŸ›¡ï¸ å®¹é”™èƒ½åŠ›** - è‡ªåŠ¨å¤„ç†å„ç§å¼‚å¸¸
- **ğŸ“Š æ•°æ®è´¨é‡** - ç»Ÿä¸€çš„æ•°æ®éªŒè¯
- **ğŸ” å¯è§‚æµ‹æ€§** - å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—

### ğŸš€ **ç¬¬å››å±‚ï¼šåº”ç”¨å±‚ï¼ˆFastAPIï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šå¯¹å¤–æä¾›æœåŠ¡æ¥å£

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **RESTful API** - æ ‡å‡†çš„HTTPæ¥å£
- **ä»»åŠ¡ç®¡ç†** - å¼‚æ­¥ä»»åŠ¡è°ƒåº¦
- **æ•°æ®æŸ¥è¯¢** - çµæ´»çš„æ•°æ®æ£€ç´¢
- **ç³»ç»Ÿç›‘æ§** - å¥åº·çŠ¶æ€æ£€æŸ¥

## ğŸ’¡ **åˆ†å±‚åä½œæµç¨‹**

### ğŸ”„ **å…¸å‹å·¥ä½œæµç¨‹**ï¼š

```text
1. ğŸª Playwrightå±‚ï¼š
   â”œâ”€â”€ æ£€æŸ¥cookiesæœ‰æ•ˆæ€§
   â”œâ”€â”€ è‡ªåŠ¨åˆ·æ–°è¿‡æœŸcookies
   â””â”€â”€ ç»´æŠ¤è´¦å·ç™»å½•çŠ¶æ€

2. ğŸ“Š twscrapeå±‚ï¼š
   â”œâ”€â”€ ä½¿ç”¨æœ‰æ•ˆcookies
   â”œâ”€â”€ æ‰§è¡Œé«˜æ•ˆæ•°æ®é‡‡é›†
   â””â”€â”€ è¿”å›ç»“æ„åŒ–æ•°æ®

3. ğŸ”§ ç®¡ç†å±‚ï¼š
   â”œâ”€â”€ åè°ƒä¸Šè¿°ä¸¤å±‚
   â”œâ”€â”€ å¤„ç†é”™è¯¯å’Œé‡è¯•
   â””â”€â”€ æ•°æ®æ¸…æ´—å’Œå­˜å‚¨

4. ğŸš€ åº”ç”¨å±‚ï¼š
   â”œâ”€â”€ æ¥æ”¶ç”¨æˆ·è¯·æ±‚
   â”œâ”€â”€ è°ƒç”¨ç®¡ç†å±‚æœåŠ¡
   â””â”€â”€ è¿”å›å¤„ç†ç»“æœ
```

### ğŸ¯ **åˆ†å·¥ä¼˜åŠ¿**ï¼š

| å±‚çº§ | ä¸“é•¿ | ä¼˜åŠ¿ |
|------|------|------|
| **Playwright** | æµè§ˆå™¨è‡ªåŠ¨åŒ– | ğŸª è§£å†³ç™»å½•éš¾é¢˜ |
| **twscrape** | APIé«˜æ•ˆè°ƒç”¨ | âš¡ 10å€æ€§èƒ½æå‡ |
| **Pythonç®¡ç†** | ç³»ç»Ÿåè°ƒ | ğŸ”§ ç»Ÿä¸€æ§åˆ¶ |
| **FastAPI** | æœåŠ¡æ¥å£ | ğŸš€ æ ‡å‡†åŒ–æœåŠ¡ |

## ğŸ‰ **æ¶æ„ä¼˜åŠ¿æ€»ç»“**

### âœ… **æŠ€æœ¯ä¼˜åŠ¿**ï¼š
1. **ğŸ”‘ è§£å†³æ ¸å¿ƒéš¾é¢˜** - Playwrightè§£å†³ç™»å½•é—®é¢˜
2. **âš¡ ä¿æŒé«˜æ€§èƒ½** - twscrapeæä¾›é«˜æ•ˆé‡‡é›†
3. **ğŸ›¡ï¸ æé«˜ç¨³å®šæ€§** - åˆ†å±‚è®¾è®¡é™ä½è€¦åˆ
4. **ğŸ”§ ä¾¿äºç»´æŠ¤** - èŒè´£æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•

### âœ… **ä¸šåŠ¡ä¼˜åŠ¿**ï¼š
1. **ğŸ“ˆ å¯æ‰©å±•æ€§** - æ¯å±‚ç‹¬ç«‹æ‰©å±•
2. **ğŸ”„ å¯æ›¿æ¢æ€§** - å•å±‚æ›¿æ¢ä¸å½±å“æ•´ä½“
3. **ğŸ¯ ä¸“ä¸šåŒ–** - æ¯å±‚ä¸“æ³¨æ ¸å¿ƒèƒ½åŠ›
4. **ğŸ’° æˆæœ¬æ•ˆç›Š** - æœ€å¤§åŒ–åˆ©ç”¨å„å·¥å…·ä¼˜åŠ¿

## æ ¸å¿ƒæ¨¡å—è®¾è®¡

### ğŸ¯ **åŸºäºApifyæœ€ä½³å®è·µçš„æ¨¡å—æ¶æ„**

å‚è€ƒApify Twitter Scraperçš„æˆåŠŸæ¶æ„ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹ä¼˜åŒ–æ¨¡å—ï¼š

### 1. æ™ºèƒ½æ•°æ®é‡‡é›†æ¨¡å— (Smart Scraping Module)

**æŠ€æœ¯æ ˆ**: twscrape + httpx + Playwright + Apify-styleä¼˜åŒ–
**æ ¸å¿ƒèŒè´£**: é«˜æ•ˆç¨³å®šçš„Xå¹³å°æ•°æ®é‡‡é›†ï¼Œæ”¯æŒé«˜çº§æŸ¥è¯¢ä¼˜åŒ–

#### ğŸš€ **Apify-inspired æŸ¥è¯¢ä¼˜åŒ–**

```python
# core/smart_scraper.py
class SmartScrapingEngine:
    """
    åŸºäºApifyç»éªŒçš„æ™ºèƒ½é‡‡é›†å¼•æ“
    æ”¯æŒé«˜çº§æŸ¥è¯¢ã€æˆæœ¬ä¼˜åŒ–å’Œåæ£€æµ‹
    """

    def __init__(self):
        self.query_optimizer = TwitterQueryOptimizer()
        self.pricing_calculator = PricingCalculator()
        self.rate_limiter = AdaptiveRateLimiter()

    async def execute_optimized_search(self, task_params: Dict) -> Dict:
        """
        æ‰§è¡Œä¼˜åŒ–çš„æœç´¢ä»»åŠ¡ - å‚è€ƒApifyçš„æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥
        """
        # 1. æŸ¥è¯¢ä¼˜åŒ– (Apify-style)
        if task_params["type"] == "profile_scraping":
            queries = self._optimize_profile_queries(task_params)
        else:
            queries = self._build_advanced_search_queries(task_params)

        # 2. æˆæœ¬é¢„ä¼° (é€æ˜å®šä»·)
        cost_estimate = self.pricing_calculator.calculate_task_cost({
            "type": task_params["type"],
            "query_count": len(queries),
            "expected_items": task_params.get("required_count", 1000)
        })

        # 3. æ‰§è¡Œé‡‡é›†
        results = []
        for query in queries:
            batch_result = await self._execute_single_query(query, task_params)
            results.extend(batch_result)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç”²æ–¹è¦æ±‚çš„æ¡æ•°
            if (task_params.get("required_count", 0) > 0 and
                len(results) >= task_params["required_count"]):
                break

        return {
            "results": results,
            "cost_info": cost_estimate,
            "queries_executed": len(queries),
            "total_collected": len(results)
        }

    def _optimize_profile_queries(self, task_params: Dict) -> List[str]:
        """
        ä¼˜åŒ–ç”¨æˆ·èµ„æ–™æŸ¥è¯¢ - å‚è€ƒApifyåˆ†æ—¶é—´æ®µç­–ç•¥
        æ¯æœˆæœ€å¤š800æ¡æ¨æ–‡ï¼Œéœ€è¦åˆ†æ®µæŸ¥è¯¢
        """
        username = task_params["username"]

        # ç”Ÿæˆæœˆåº¦æ—¶é—´èŒƒå›´
        date_ranges = self._generate_monthly_ranges(
            task_params.get("start_time"),
            task_params.get("end_time")
        )

        return [f"from:{username} since:{start} until:{end}"
                for start, end in date_ranges]

    def _build_advanced_search_queries(self, task_params: Dict) -> List[str]:
        """
        æ„å»ºé«˜çº§æœç´¢æŸ¥è¯¢ - æ”¯æŒApify-styleå¤æ‚è¿‡æ»¤
        å‚è€ƒ: https://github.com/igorbrigadir/twitter-advanced-search
        """
        keyword = task_params["keyword"]
        filters = {
            "only_images": task_params.get("only_images", False),
            "only_videos": task_params.get("only_videos", False),
            "only_verified": task_params.get("only_verified", False),
            "min_likes": task_params.get("min_likes"),
            "language": task_params.get("language")
        }

        # ä½¿ç”¨æŸ¥è¯¢ä¼˜åŒ–å™¨æ„å»ºå¤æ‚æŸ¥è¯¢
        optimized_query = self.query_optimizer.build_advanced_search(keyword, filters)

        # å¦‚æœæœ‰æ—¶é—´èŒƒå›´ï¼Œåˆ†æ®µæŸ¥è¯¢ (æé«˜æˆåŠŸç‡)
        if task_params.get("start_time") and task_params.get("end_time"):
            date_ranges = self._generate_date_ranges(
                task_params["start_time"],
                task_params["end_time"]
            )
            return [f"{optimized_query} since:{start} until:{end}"
                   for start, end in date_ranges]

        return [optimized_query]
```

#### ğŸ“Š **Apify-style æ•°æ®å¤„ç†å™¨**

```python
# core/data_processor.py
class ApifyStyleDataProcessor:
    """
    åŸºäºApifyè¾“å‡ºæ ¼å¼çš„æ•°æ®å¤„ç†å™¨
    ç¡®ä¿æ•°æ®æ ¼å¼ä¸ç”²æ–¹éœ€æ±‚å®Œå…¨åŒ¹é…
    """

    def process_tweet_data(self, raw_tweet: Dict, task_info: Dict) -> Dict:
        """
        å¤„ç†æ¨æ–‡æ•°æ® - å®Œå…¨æŒ‰ç…§ç”²æ–¹20ä¸ªå­—æ®µè¦æ±‚
        å‚è€ƒApifyçš„è¯¦ç»†è¾“å‡ºæ ¼å¼
        """
        processed = {
            # ç”²æ–¹å­—æ®µ0: åŸºç¡€ä¿¡æ¯
            "post_url": f"https://x.com/{raw_tweet['user']['username']}/status/{raw_tweet['id']}",
            "post_id": raw_tweet["id"],
            "post_type": self._determine_post_type(raw_tweet),

            # ç”²æ–¹å­—æ®µ1-3: ä½œè€…ä¿¡æ¯
            "author_avatar": raw_tweet["user"].get("profilePicture", ""),
            "author_name": raw_tweet["user"].get("name", ""),
            "author_handle": f"@{raw_tweet['user'].get('userName', '')}",

            # ç”²æ–¹å­—æ®µ4: æ—¶é—´
            "post_time": self._parse_twitter_date(raw_tweet.get("createdAt")),

            # ç”²æ–¹å­—æ®µ5: å†…å®¹
            "post_content": raw_tweet.get("text", ""),

            # ç”²æ–¹å­—æ®µ6: åª’ä½“ (æ•°ç»„å½¢å¼ï¼Œå¦‚Apify)
            "post_images": self._process_media_array(raw_tweet.get("extendedEntities", {})),

            # ç”²æ–¹å­—æ®µ7-10, 17: äº’åŠ¨æ•°æ®
            "comment_count": raw_tweet.get("replyCount", 0),
            "retweet_count": raw_tweet.get("retweetCount", 0),
            "like_count": raw_tweet.get("likeCount", 0),
            "view_count": raw_tweet.get("viewCount", 0),
            "bookmark_count": raw_tweet.get("bookmarkCount", 0),

            # ç”²æ–¹å­—æ®µ18-19: ç±»å‹æ ‡è¯†
            "is_retweet": raw_tweet.get("isRetweet", False),
            "is_quote": raw_tweet.get("isQuote", False),

            # ç”²æ–¹å­—æ®µ11-16: è½¬å‘å¸–ä¿¡æ¯
            **self._process_retweet_data(raw_tweet),

            # ç”²æ–¹å­—æ®µ20: é“¾æ¥ä¿¡æ¯
            "post_links": self._extract_links(raw_tweet),

            # ç”²æ–¹è¦æ±‚çš„å…³ç³»å­—æ®µ
            "parent_post_id": raw_tweet.get("inReplyToStatusId"),
            "parent_comment_id": raw_tweet.get("inReplyToUserId"),

            # ç³»ç»Ÿå­—æ®µ
            "oss_file_path": None,  # åç»­OSSä¸Šä¼ æ—¶å¡«å……
            "collected_at": datetime.utcnow().isoformat(),
            "keyword": task_info.get("keyword"),
            "task_id": task_info.get("task_id"),

            # ç”²æ–¹è¦æ±‚ä¿ç•™åŸå§‹æ•°æ®
            "raw_data": {
                "twitter_api_response": raw_tweet,
                "collection_method": "twscrape_optimized",
                "apify_style_processing": True
            }
        }

        return processed

    def _process_media_array(self, extended_entities: Dict) -> List[Dict]:
        """
        å¤„ç†åª’ä½“æ•°ç»„ - å‚è€ƒApifyçš„è¯¦ç»†åª’ä½“ä¿¡æ¯
        """
        media_list = []

        for media in extended_entities.get("media", []):
            media_item = {
                "original_url": media.get("media_url_https", ""),
                "oss_url": None,  # åç»­OSSä¸Šä¼ æ—¶å¡«å……
                "type": "video_cover" if media.get("type") == "video" else "image",
                "width": media.get("original_info", {}).get("width"),
                "height": media.get("original_info", {}).get("height")
            }

            # è§†é¢‘ç‰¹æ®Šå¤„ç† (å‚è€ƒApifyè§†é¢‘ä¿¡æ¯)
            if media.get("video_info"):
                media_item.update({
                    "video_duration": media["video_info"].get("duration_millis"),
                    "video_variants": media["video_info"].get("variants", [])
                })

            media_list.append(media_item)

        return media_list

    def _determine_post_type(self, raw_tweet: Dict) -> str:
        """
        ç¡®å®šå¸–å­ç±»å‹ - å‚è€ƒApifyçš„ç±»å‹åˆ†ç±»
        """
        if raw_tweet.get("isRetweet"):
            return "retweet"
        elif raw_tweet.get("isQuote"):
            return "quote"
        elif raw_tweet.get("extendedEntities", {}).get("media"):
            media_types = [m.get("type") for m in raw_tweet["extendedEntities"]["media"]]
            if "video" in media_types:
                return "video"
            elif "photo" in media_types:
                return "image"
        return "text"
```
import asyncio
import random
from twscrape import API
from typing import List, Dict, Optional
from datetime import datetime
import logging
from .proxy_manager import ProxyManager
from .account_manager import AccountManager

class ProductionTwitterScraper:
    """ç”Ÿäº§çº§Twitteré‡‡é›†å™¨"""

    def __init__(self, account_manager: AccountManager, proxy_manager: ProxyManager):
        self.api = API()
        self.account_manager = account_manager
        self.proxy_manager = proxy_manager
        self.logger = logging.getLogger(__name__)

    async def search_tweets(self, keyword: str, count: int = 100) -> List[Dict]:
        """æœç´¢æ¨æ–‡ - å¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•"""
        tweets = []
        retry_count = 0
        max_retries = 3

        while retry_count < max_retries:
            try:
                # è·å–å¯ç”¨è´¦å·å’Œä»£ç†
                account = await self.account_manager.get_available_account()
                proxy = await self.proxy_manager.get_proxy()

                # é…ç½®API
                if proxy:
                    self.api.set_proxy(proxy['url'])

                async for tweet in self.api.search(keyword, limit=count):
                    tweet_data = {
                        'id': tweet.id,
                        'text': tweet.rawContent,
                        'user_id': tweet.user.id,
                        'username': tweet.user.username,
                        'created_at': tweet.date.isoformat(),
                        'retweet_count': tweet.retweetCount,
                        'like_count': tweet.likeCount,
                        'reply_count': tweet.replyCount,
                        'quote_count': tweet.quoteCount,
                        'view_count': getattr(tweet, 'viewCount', 0),
                        'media': [{'url': m.url, 'type': m.type} for m in tweet.media] if tweet.media else [],
                        'hashtags': [tag.text for tag in tweet.hashtags] if tweet.hashtags else [],
                        'urls': [url.expandedUrl for url in tweet.urls] if tweet.urls else [],
                        'collected_at': datetime.utcnow().isoformat(),
                        'source_account': account['id'],
                        'source_proxy': proxy['id'] if proxy else None
                    }
                    tweets.append(tweet_data)

                # æˆåŠŸåæ›´æ–°è´¦å·çŠ¶æ€
                await self.account_manager.update_account_success(account['id'])
                break

            except Exception as e:
                retry_count += 1
                self.logger.warning(f"Search failed (attempt {retry_count}): {str(e)}")

                if retry_count < max_retries:
                    # æ ‡è®°è´¦å·å¯èƒ½æœ‰é—®é¢˜
                    if 'account' in locals():
                        await self.account_manager.mark_account_error(account['id'], str(e))

                    # ç­‰å¾…åé‡è¯•
                    await asyncio.sleep(random.uniform(5, 15))
                else:
                    raise

        return tweets

    async def get_user_profile(self, username: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·èµ„æ–™ - å®Œæ•´å®ç°"""
        try:
            user = await self.api.user_by_login(username)
            if not user:
                return None

            return {
                'user_id': user.id,
                'username': user.username,
                'display_name': user.displayname,
                'description': user.description,
                'followers_count': user.followersCount,
                'following_count': user.followingCount,
                'tweet_count': user.statusesCount,
                'verified': user.verified,
                'created_at': user.created.isoformat() if user.created else None,
                'location': user.location,
                'profile_image_url': user.profileImageUrl,
                'profile_banner_url': user.profileBannerUrl,
                'collected_at': datetime.utcnow().isoformat()
            }
        except Exception as e:
            self.logger.error(f"Failed to get user profile for {username}: {str(e)}")
            raise
```

### 2. è´¦å·ç®¡ç†æ¨¡å—

```python
# core/account_manager.py
import asyncio
import json
import logging
import hashlib
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, asdict
import redis.asyncio as redis
from twscrape import API, Account

class AccountStatus(Enum):
    """è´¦å·çŠ¶æ€æšä¸¾"""
    ACTIVE = "active"           # æ´»è·ƒå¯ç”¨
    SUSPENDED = "suspended"     # æš‚åœä½¿ç”¨
    ERROR = "error"            # é”™è¯¯çŠ¶æ€
    MAINTENANCE = "maintenance" # ç»´æŠ¤ä¸­
    COOLDOWN = "cooldown"      # å†·å´æœŸ
    EXPIRED = "expired"        # å·²è¿‡æœŸ

class AccountPriority(Enum):
    """è´¦å·ä¼˜å…ˆçº§"""
    HIGH = "high"      # é«˜ä¼˜å…ˆçº§ï¼ˆç¨³å®šè´¦å·ï¼‰
    NORMAL = "normal"  # æ™®é€šä¼˜å…ˆçº§
    LOW = "low"        # ä½ä¼˜å…ˆçº§ï¼ˆæµ‹è¯•è´¦å·ï¼‰
    BACKUP = "backup"  # å¤‡ç”¨è´¦å·

@dataclass
class AccountMetrics:
    """è´¦å·æŒ‡æ ‡æ•°æ®"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    rate_limit_hits: int = 0
    last_used: Optional[datetime] = None
    last_success: Optional[datetime] = None
    last_error: Optional[datetime] = None
    consecutive_errors: int = 0
    daily_usage: int = 0
    weekly_usage: int = 0

    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡è®¡ç®—"""
        if self.total_requests == 0:
            return 1.0
        return self.successful_requests / self.total_requests

    @property
    def health_score(self) -> float:
        """å¥åº·åˆ†æ•°è®¡ç®— (0-1)"""
        base_score = self.success_rate

        # è¿ç»­é”™è¯¯æƒ©ç½š
        error_penalty = min(self.consecutive_errors * 0.1, 0.5)

        # ä½¿ç”¨é¢‘ç‡è°ƒæ•´
        usage_factor = 1.0
        if self.daily_usage > 800:  # æ¥è¿‘é™åˆ¶æ—¶é™ä½åˆ†æ•°
            usage_factor = 0.8
        elif self.daily_usage > 600:
            usage_factor = 0.9

        return max(0.0, (base_score - error_penalty) * usage_factor)

@dataclass
class AccountConfig:
    """è´¦å·é…ç½®"""
    account_id: str
    username: str
    email: str
    status: AccountStatus
    priority: AccountPriority
    daily_limit: int = 1000
    hourly_limit: int = 100
    cooldown_minutes: int = 30
    max_consecutive_errors: int = 5
    auto_recovery: bool = True
    tags: List[str] = None

    def __post_init__(self):
        if self.tags is None:
            self.tags = []

class ProductionAccountManager:
    """ç”Ÿäº§çº§è´¦å·æ± ç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis, twscrape_api: API = None):
        self.redis = redis_client
        self.api = twscrape_api or API()
        self.logger = logging.getLogger(__name__)

        # é…ç½®å‚æ•°
        self.health_threshold = 0.7
        self.max_daily_usage = 1000
        self.cooldown_duration = 1800  # 30åˆ†é’Ÿ
        self.error_threshold = 5

    async def initialize(self):
        """åˆå§‹åŒ–è´¦å·ç®¡ç†å™¨"""
        await self._sync_twscrape_accounts()
        await self._setup_redis_structures()

    async def _sync_twscrape_accounts(self):
        """åŒæ­¥twscrapeè´¦å·åˆ°Redis"""
        try:
            accounts = await self.api.pool.get_all()
            self.logger.info(f"Found {len(accounts)} accounts in twscrape")

            for account in accounts:
                await self._import_account_from_twscrape(account)

        except Exception as e:
            self.logger.error(f"Failed to sync twscrape accounts: {e}")

    async def _import_account_from_twscrape(self, account: Account):
        """ä»twscrapeå¯¼å…¥è´¦å·"""
        account_config = AccountConfig(
            account_id=f"tw_{account.username}",
            username=account.username,
            email=account.email or f"{account.username}@unknown.com",
            status=AccountStatus.ACTIVE if account.active else AccountStatus.SUSPENDED,
            priority=AccountPriority.NORMAL
        )

        # ä¿å­˜è´¦å·é…ç½®
        await self._save_account_config(account_config)

        # ä¿å­˜cookies
        if hasattr(account, 'cookies') and account.cookies:
            await self._save_account_cookies(account_config.account_id, account.cookies)

    async def _setup_redis_structures(self):
        """è®¾ç½®Redisæ•°æ®ç»“æ„"""
        # åˆ›å»ºç´¢å¼•é›†åˆ
        await self.redis.sadd('account_manager:initialized', '1')

    async def get_available_account(self,
                                  priority: Optional[AccountPriority] = None,
                                  tags: Optional[List[str]] = None,
                                  exclude_accounts: Optional[List[str]] = None) -> Optional[Dict]:
        """è·å–å¯ç”¨è´¦å· - æ™ºèƒ½é€‰æ‹©ç®—æ³•"""
        try:
            # è·å–å€™é€‰è´¦å·
            candidates = await self._get_candidate_accounts(priority, tags, exclude_accounts)

            if not candidates:
                self.logger.warning("No candidate accounts available")
                return None

            # æ™ºèƒ½é€‰æ‹©æœ€ä½³è´¦å·
            best_account = await self._select_best_account(candidates)

            if best_account:
                # æ›´æ–°ä½¿ç”¨è®°å½•
                await self._record_account_usage(best_account['account_id'])

                # è·å–å®Œæ•´è´¦å·ä¿¡æ¯
                return await self._get_full_account_info(best_account['account_id'])

            return None

        except Exception as e:
            self.logger.error(f"Failed to get available account: {e}")
            return None

    async def _get_candidate_accounts(self,
                                    priority: Optional[AccountPriority],
                                    tags: Optional[List[str]],
                                    exclude_accounts: Optional[List[str]]) -> List[Dict]:
        """è·å–å€™é€‰è´¦å·åˆ—è¡¨"""
        candidates = []

        # è·å–æ‰€æœ‰æ´»è·ƒè´¦å·
        active_accounts = await self.redis.smembers('accounts:active')

        for account_id in active_accounts:
            account_id = account_id.decode()

            # æ’é™¤æŒ‡å®šè´¦å·
            if exclude_accounts and account_id in exclude_accounts:
                continue

            # æ£€æŸ¥è´¦å·é…ç½®
            config = await self._get_account_config(account_id)
            if not config:
                continue

            # ä¼˜å…ˆçº§è¿‡æ»¤
            if priority and config.priority != priority:
                continue

            # æ ‡ç­¾è¿‡æ»¤
            if tags and not any(tag in config.tags for tag in tags):
                continue

            # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸ
            if await self._is_account_in_cooldown(account_id):
                continue

            # æ£€æŸ¥ä½¿ç”¨é™åˆ¶
            if await self._is_account_over_limit(account_id):
                continue

            # è·å–è´¦å·æŒ‡æ ‡
            metrics = await self._get_account_metrics(account_id)

            candidates.append({
                'account_id': account_id,
                'config': config,
                'metrics': metrics,
                'health_score': metrics.health_score
            })

        return candidates

    async def _select_best_account(self, candidates: List[Dict]) -> Optional[Dict]:
        """é€‰æ‹©æœ€ä½³è´¦å· - ç»¼åˆè¯„åˆ†ç®—æ³•"""
        if not candidates:
            return None

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for candidate in candidates:
            score = await self._calculate_account_score(candidate)
            candidate['final_score'] = score

        # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†
        candidates.sort(key=lambda x: x['final_score'], reverse=True)

        return candidates[0]

    async def _calculate_account_score(self, candidate: Dict) -> float:
        """è®¡ç®—è´¦å·ç»¼åˆè¯„åˆ†"""
        config = candidate['config']
        metrics = candidate['metrics']

        # åŸºç¡€å¥åº·åˆ†æ•° (40%)
        health_score = metrics.health_score * 0.4

        # ä½¿ç”¨é¢‘ç‡åˆ†æ•° (30%) - ä½¿ç”¨è¶Šå°‘åˆ†æ•°è¶Šé«˜
        usage_ratio = metrics.daily_usage / config.daily_limit
        usage_score = (1 - usage_ratio) * 0.3

        # ä¼˜å…ˆçº§åˆ†æ•° (20%)
        priority_scores = {
            AccountPriority.HIGH: 1.0,
            AccountPriority.NORMAL: 0.8,
            AccountPriority.LOW: 0.6,
            AccountPriority.BACKUP: 0.4
        }
        priority_score = priority_scores.get(config.priority, 0.8) * 0.2

        # æœ€è¿‘æˆåŠŸæ—¶é—´åˆ†æ•° (10%)
        time_score = 0.1
        if metrics.last_success:
            hours_since_success = (datetime.utcnow() - metrics.last_success).total_seconds() / 3600
            time_score = max(0, (24 - hours_since_success) / 24) * 0.1

        return health_score + usage_score + priority_score + time_score

    async def update_account_success(self, account_id: str, operation_type: str = "general"):
        """æ›´æ–°è´¦å·æˆåŠŸè®°å½•"""
        try:
            current_time = datetime.utcnow()

            # æ›´æ–°åŸºç¡€æŒ‡æ ‡
            await self.redis.hincrby(f'account:{account_id}:metrics', 'total_requests', 1)
            await self.redis.hincrby(f'account:{account_id}:metrics', 'successful_requests', 1)
            await self.redis.hset(f'account:{account_id}:metrics', 'last_success', current_time.isoformat())
            await self.redis.hset(f'account:{account_id}:metrics', 'consecutive_errors', 0)

            # æ›´æ–°ä½¿ç”¨è®¡æ•°
            await self._update_usage_counters(account_id)

            # è®°å½•æ“ä½œå†å²
            await self._record_operation_history(account_id, 'success', operation_type)

            # å¦‚æœè´¦å·ä¹‹å‰æœ‰é—®é¢˜ï¼Œå°è¯•æ¢å¤
            await self._try_account_recovery(account_id)

            self.logger.debug(f"Account {account_id} success updated for {operation_type}")

        except Exception as e:
            self.logger.error(f"Failed to update account success: {e}")

    async def mark_account_error(self, account_id: str, error: str, error_type: str = "general"):
        """æ ‡è®°è´¦å·é”™è¯¯ - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            current_time = datetime.utcnow()

            # æ›´æ–°é”™è¯¯æŒ‡æ ‡
            await self.redis.hincrby(f'account:{account_id}:metrics', 'total_requests', 1)
            await self.redis.hincrby(f'account:{account_id}:metrics', 'failed_requests', 1)
            await self.redis.hincrby(f'account:{account_id}:metrics', 'consecutive_errors', 1)
            await self.redis.hset(f'account:{account_id}:metrics', 'last_error', current_time.isoformat())

            # è®°å½•é”™è¯¯è¯¦æƒ…
            await self._record_error_details(account_id, error, error_type)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†
            await self._handle_specific_errors(account_id, error, error_type)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœè´¦å·
            consecutive_errors = await self.redis.hget(f'account:{account_id}:metrics', 'consecutive_errors')
            if consecutive_errors and int(consecutive_errors) >= self.error_threshold:
                await self._suspend_account(account_id, f"Too many consecutive errors: {consecutive_errors}")

            self.logger.warning(f"Account {account_id} error marked: {error_type} - {error}")

        except Exception as e:
            self.logger.error(f"Failed to mark account error: {e}")

    async def _handle_specific_errors(self, account_id: str, error: str, error_type: str):
        """å¤„ç†ç‰¹å®šç±»å‹çš„é”™è¯¯"""
        error_lower = error.lower()

        if 'rate limit' in error_lower or 'too many requests' in error_lower:
            # é€Ÿç‡é™åˆ¶é”™è¯¯ - è®¾ç½®å†·å´æœŸ
            await self._set_account_cooldown(account_id, self.cooldown_duration)
            await self.redis.hincrby(f'account:{account_id}:metrics', 'rate_limit_hits', 1)

        elif 'unauthorized' in error_lower or 'forbidden' in error_lower:
            # è®¤è¯é”™è¯¯ - å¯èƒ½éœ€è¦é‡æ–°ç™»å½•
            await self._mark_account_needs_reauth(account_id)

        elif 'suspended' in error_lower or 'locked' in error_lower:
            # è´¦å·è¢«å° - ç«‹å³æš‚åœ
            await self._suspend_account(account_id, f"Account suspended by platform: {error}")

        elif 'not found' in error_lower:
            # èµ„æºä¸å­˜åœ¨ - ä¸ç®—ä¸¥é‡é”™è¯¯
            pass  # ä¸å¢åŠ è¿ç»­é”™è¯¯è®¡æ•°

    async def get_account_statistics(self) -> Dict:
        """è·å–è´¦å·æ± ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_accounts': 0,
                'active_accounts': 0,
                'suspended_accounts': 0,
                'error_accounts': 0,
                'maintenance_accounts': 0,
                'cooldown_accounts': 0,
                'health_distribution': {'high': 0, 'medium': 0, 'low': 0},
                'priority_distribution': {},
                'daily_usage_total': 0,
                'average_health_score': 0.0
            }

            # è·å–æ‰€æœ‰è´¦å·
            all_accounts = await self.redis.smembers('accounts:all')
            stats['total_accounts'] = len(all_accounts)

            total_health = 0.0

            for account_id in all_accounts:
                account_id = account_id.decode()

                # è·å–è´¦å·é…ç½®å’ŒæŒ‡æ ‡
                config = await self._get_account_config(account_id)
                metrics = await self._get_account_metrics(account_id)

                if not config or not metrics:
                    continue

                # çŠ¶æ€ç»Ÿè®¡
                status_key = f"{config.status.value}_accounts"
                if status_key in stats:
                    stats[status_key] += 1

                # ä¼˜å…ˆçº§ç»Ÿè®¡
                priority = config.priority.value
                stats['priority_distribution'][priority] = stats['priority_distribution'].get(priority, 0) + 1

                # å¥åº·åˆ†æ•°ç»Ÿè®¡
                health_score = metrics.health_score
                total_health += health_score

                if health_score >= 0.8:
                    stats['health_distribution']['high'] += 1
                elif health_score >= 0.6:
                    stats['health_distribution']['medium'] += 1
                else:
                    stats['health_distribution']['low'] += 1

                # ä½¿ç”¨é‡ç»Ÿè®¡
                stats['daily_usage_total'] += metrics.daily_usage

            # è®¡ç®—å¹³å‡å¥åº·åˆ†æ•°
            if stats['total_accounts'] > 0:
                stats['average_health_score'] = total_health / stats['total_accounts']

            return stats

        except Exception as e:
            self.logger.error(f"Failed to get account statistics: {e}")
            return {}

    async def batch_health_check(self) -> Dict:
        """æ‰¹é‡å¥åº·æ£€æŸ¥"""
        try:
            results = {
                'checked': 0,
                'healthy': 0,
                'unhealthy': 0,
                'recovered': 0,
                'suspended': 0,
                'details': []
            }

            all_accounts = await self.redis.smembers('accounts:all')

            for account_id in all_accounts:
                account_id = account_id.decode()

                try:
                    # æ‰§è¡Œå•ä¸ªè´¦å·å¥åº·æ£€æŸ¥
                    health_result = await self._check_single_account_health(account_id)
                    results['details'].append(health_result)
                    results['checked'] += 1

                    if health_result['healthy']:
                        results['healthy'] += 1

                        # å°è¯•æ¢å¤ä¹‹å‰æœ‰é—®é¢˜çš„è´¦å·
                        if health_result.get('recovered'):
                            results['recovered'] += 1
                    else:
                        results['unhealthy'] += 1

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                        if health_result.get('should_suspend'):
                            await self._suspend_account(account_id, health_result.get('reason', 'Health check failed'))
                            results['suspended'] += 1

                except Exception as e:
                    self.logger.error(f"Health check failed for account {account_id}: {e}")
                    results['details'].append({
                        'account_id': account_id,
                        'healthy': False,
                        'error': str(e)
                    })

            self.logger.info(f"Batch health check completed: {results['checked']} accounts checked")
            return results

        except Exception as e:
            self.logger.error(f"Batch health check failed: {e}")
            return {'error': str(e)}

    async def _check_single_account_health(self, account_id: str) -> Dict:
        """å•ä¸ªè´¦å·å¥åº·æ£€æŸ¥"""
        try:
            config = await self._get_account_config(account_id)
            metrics = await self._get_account_metrics(account_id)

            if not config or not metrics:
                return {
                    'account_id': account_id,
                    'healthy': False,
                    'reason': 'Missing config or metrics'
                }

            health_score = metrics.health_score
            is_healthy = health_score >= self.health_threshold

            result = {
                'account_id': account_id,
                'healthy': is_healthy,
                'health_score': health_score,
                'status': config.status.value,
                'daily_usage': metrics.daily_usage,
                'success_rate': metrics.success_rate,
                'consecutive_errors': metrics.consecutive_errors
            }

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†
            if not is_healthy:
                if metrics.consecutive_errors >= config.max_consecutive_errors:
                    result['should_suspend'] = True
                    result['reason'] = f"Too many consecutive errors: {metrics.consecutive_errors}"
                elif metrics.success_rate < 0.5:
                    result['should_suspend'] = True
                    result['reason'] = f"Low success rate: {metrics.success_rate:.2%}"

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
            elif config.status in [AccountStatus.SUSPENDED, AccountStatus.ERROR]:
                if health_score >= 0.8 and metrics.consecutive_errors == 0:
                    result['recovered'] = True
                    await self._recover_account(account_id)

            return result

        except Exception as e:
            return {
                'account_id': account_id,
                'healthy': False,
                'error': str(e)
            }

    async def add_account(self, username: str, password: str, email: str,
                         priority: AccountPriority = AccountPriority.NORMAL,
                         tags: List[str] = None) -> bool:
        """æ·»åŠ æ–°è´¦å·"""
        try:
            # æ·»åŠ åˆ°twscrape
            await self.api.pool.add_account(username, password, email, password)

            # åˆ›å»ºè´¦å·é…ç½®
            account_config = AccountConfig(
                account_id=f"tw_{username}",
                username=username,
                email=email,
                status=AccountStatus.MAINTENANCE,  # æ–°è´¦å·å…ˆè®¾ä¸ºç»´æŠ¤çŠ¶æ€
                priority=priority,
                tags=tags or []
            )

            # ä¿å­˜é…ç½®
            await self._save_account_config(account_config)

            # åˆå§‹åŒ–æŒ‡æ ‡
            await self._initialize_account_metrics(account_config.account_id)

            self.logger.info(f"Account {username} added successfully")
            return True

        except Exception as e:
            self.logger.error(f"Failed to add account {username}: {e}")
            return False

    async def remove_account(self, account_id: str) -> bool:
        """ç§»é™¤è´¦å·"""
        try:
            config = await self._get_account_config(account_id)
            if not config:
                return False

            # ä»twscrapeç§»é™¤
            await self.api.pool.delete(config.username)

            # ä»Redisæ¸…ç†æ•°æ®
            await self._cleanup_account_data(account_id)

            self.logger.info(f"Account {account_id} removed successfully")
            return True

        except Exception as e:
            self.logger.error(f"Failed to remove account {account_id}: {e}")
            return False

    # ========== è¾…åŠ©æ–¹æ³• ==========

    async def _get_account_config(self, account_id: str) -> Optional[AccountConfig]:
        """è·å–è´¦å·é…ç½®"""
        try:
            config_data = await self.redis.hgetall(f'account:{account_id}:config')
            if not config_data:
                return None

            # è½¬æ¢å­—èŠ‚æ•°æ®
            config_dict = {k.decode(): v.decode() for k, v in config_data.items()}

            # å¤„ç†æšä¸¾ç±»å‹
            config_dict['status'] = AccountStatus(config_dict['status'])
            config_dict['priority'] = AccountPriority(config_dict['priority'])
            config_dict['tags'] = json.loads(config_dict.get('tags', '[]'))

            # å¤„ç†æ•°å€¼ç±»å‹
            for field in ['daily_limit', 'hourly_limit', 'cooldown_minutes', 'max_consecutive_errors']:
                if field in config_dict:
                    config_dict[field] = int(config_dict[field])

            config_dict['auto_recovery'] = config_dict.get('auto_recovery', 'true').lower() == 'true'

            return AccountConfig(**config_dict)

        except Exception as e:
            self.logger.error(f"Failed to get account config for {account_id}: {e}")
            return None

    async def _save_account_config(self, config: AccountConfig):
        """ä¿å­˜è´¦å·é…ç½®"""
        try:
            config_dict = asdict(config)
            config_dict['status'] = config.status.value
            config_dict['priority'] = config.priority.value
            config_dict['tags'] = json.dumps(config.tags)

            await self.redis.hset(f'account:{config.account_id}:config', mapping=config_dict)
            await self.redis.sadd('accounts:all', config.account_id)

            # æ›´æ–°çŠ¶æ€ç´¢å¼•
            await self._update_status_index(config.account_id, config.status)

        except Exception as e:
            self.logger.error(f"Failed to save account config: {e}")

    async def _get_account_metrics(self, account_id: str) -> AccountMetrics:
        """è·å–è´¦å·æŒ‡æ ‡"""
        try:
            metrics_data = await self.redis.hgetall(f'account:{account_id}:metrics')

            if not metrics_data:
                return AccountMetrics()

            # è½¬æ¢æ•°æ®ç±»å‹
            metrics_dict = {}
            for k, v in metrics_data.items():
                key = k.decode()
                value = v.decode()

                if key in ['total_requests', 'successful_requests', 'failed_requests',
                          'rate_limit_hits', 'consecutive_errors', 'daily_usage', 'weekly_usage']:
                    metrics_dict[key] = int(value)
                elif key in ['last_used', 'last_success', 'last_error']:
                    if value:
                        metrics_dict[key] = datetime.fromisoformat(value)

            return AccountMetrics(**metrics_dict)

        except Exception as e:
            self.logger.error(f"Failed to get account metrics for {account_id}: {e}")
            return AccountMetrics()

    async def _save_account_metrics(self, account_id: str, metrics: AccountMetrics):
        """ä¿å­˜è´¦å·æŒ‡æ ‡"""
        try:
            metrics_dict = asdict(metrics)

            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            for key, value in metrics_dict.items():
                if isinstance(value, datetime):
                    metrics_dict[key] = value.isoformat()
                elif value is None:
                    metrics_dict[key] = ''

            await self.redis.hset(f'account:{account_id}:metrics', mapping=metrics_dict)

        except Exception as e:
            self.logger.error(f"Failed to save account metrics: {e}")

    async def _initialize_account_metrics(self, account_id: str):
        """åˆå§‹åŒ–è´¦å·æŒ‡æ ‡"""
        metrics = AccountMetrics()
        await self._save_account_metrics(account_id, metrics)

    async def _update_status_index(self, account_id: str, status: AccountStatus):
        """æ›´æ–°çŠ¶æ€ç´¢å¼•"""
        # ä»æ‰€æœ‰çŠ¶æ€é›†åˆä¸­ç§»é™¤
        for s in AccountStatus:
            await self.redis.srem(f'accounts:{s.value}', account_id)

        # æ·»åŠ åˆ°æ–°çŠ¶æ€é›†åˆ
        await self.redis.sadd(f'accounts:{status.value}', account_id)

    async def _is_account_in_cooldown(self, account_id: str) -> bool:
        """æ£€æŸ¥è´¦å·æ˜¯å¦åœ¨å†·å´æœŸ"""
        cooldown_until = await self.redis.get(f'account:{account_id}:cooldown')
        if not cooldown_until:
            return False

        cooldown_time = datetime.fromisoformat(cooldown_until.decode())
        return datetime.utcnow() < cooldown_time

    async def _is_account_over_limit(self, account_id: str) -> bool:
        """æ£€æŸ¥è´¦å·æ˜¯å¦è¶…è¿‡ä½¿ç”¨é™åˆ¶"""
        config = await self._get_account_config(account_id)
        metrics = await self._get_account_metrics(account_id)

        if not config or not metrics:
            return True

        return metrics.daily_usage >= config.daily_limit

    async def _set_account_cooldown(self, account_id: str, duration_seconds: int):
        """è®¾ç½®è´¦å·å†·å´æœŸ"""
        cooldown_until = datetime.utcnow() + timedelta(seconds=duration_seconds)
        await self.redis.set(f'account:{account_id}:cooldown',
                           cooldown_until.isoformat(),
                           ex=duration_seconds)

    async def _suspend_account(self, account_id: str, reason: str):
        """æš‚åœè´¦å·"""
        config = await self._get_account_config(account_id)
        if config:
            config.status = AccountStatus.SUSPENDED
            await self._save_account_config(config)

        # è®°å½•æš‚åœåŸå› 
        await self.redis.hset(f'account:{account_id}:suspension',
                            'reason', reason,
                            'suspended_at', datetime.utcnow().isoformat())

        self.logger.warning(f"Account {account_id} suspended: {reason}")

    async def _recover_account(self, account_id: str):
        """æ¢å¤è´¦å·"""
        config = await self._get_account_config(account_id)
        if config:
            config.status = AccountStatus.ACTIVE
            await self._save_account_config(config)

        # æ¸…ç†æš‚åœè®°å½•
        await self.redis.delete(f'account:{account_id}:suspension')
        await self.redis.delete(f'account:{account_id}:cooldown')

        self.logger.info(f"Account {account_id} recovered")

    async def _record_account_usage(self, account_id: str):
        """è®°å½•è´¦å·ä½¿ç”¨"""
        await self.redis.hincrby(f'account:{account_id}:metrics', 'daily_usage', 1)
        await self.redis.hset(f'account:{account_id}:metrics', 'last_used', datetime.utcnow().isoformat())

    async def _get_full_account_info(self, account_id: str) -> Dict:
        """è·å–å®Œæ•´è´¦å·ä¿¡æ¯"""
        config = await self._get_account_config(account_id)
        metrics = await self._get_account_metrics(account_id)

        # è·å–cookies
        cookies_data = await self.redis.hget(f'account:{account_id}', 'cookies')
        cookies = json.loads(cookies_data.decode()) if cookies_data else {}

        return {
            'account_id': account_id,
            'username': config.username,
            'email': config.email,
            'status': config.status.value,
            'priority': config.priority.value,
            'health_score': metrics.health_score,
            'daily_usage': metrics.daily_usage,
            'success_rate': metrics.success_rate,
            'cookies': cookies,
            'last_used': metrics.last_used.isoformat() if metrics.last_used else None
        }

    async def _cleanup_account_data(self, account_id: str):
        """æ¸…ç†è´¦å·æ•°æ®"""
        # åˆ é™¤æ‰€æœ‰ç›¸å…³çš„Redisé”®
        keys_to_delete = [
            f'account:{account_id}:config',
            f'account:{account_id}:metrics',
            f'account:{account_id}:cookies',
            f'account:{account_id}:suspension',
            f'account:{account_id}:cooldown',
            f'account:{account_id}:history'
        ]

        for key in keys_to_delete:
            await self.redis.delete(key)

        # ä»æ‰€æœ‰é›†åˆä¸­ç§»é™¤
        await self.redis.srem('accounts:all', account_id)
        for status in AccountStatus:
            await self.redis.srem(f'accounts:{status.value}', account_id)

# ========== è´¦å·ç®¡ç†ä½¿ç”¨ç¤ºä¾‹ ==========

class AccountManagerExample:
    """è´¦å·ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹"""

    async def example_usage(self):
        """å®Œæ•´ä½¿ç”¨ç¤ºä¾‹"""
        import redis.asyncio as redis
        from twscrape import API

        # åˆå§‹åŒ–
        redis_client = redis.Redis(host='localhost', port=6379, db=0)
        api = API()
        account_manager = ProductionAccountManager(redis_client, api)

        # åˆå§‹åŒ–è´¦å·ç®¡ç†å™¨
        await account_manager.initialize()

        # 1. æ·»åŠ æ–°è´¦å·
        success = await account_manager.add_account(
            username="test_account",
            password="password123",
            email="test@example.com",
            priority=AccountPriority.NORMAL,
            tags=["test", "development"]
        )

        # 2. è·å–å¯ç”¨è´¦å·
        account = await account_manager.get_available_account(
            priority=AccountPriority.HIGH,
            tags=["production"],
            exclude_accounts=["problematic_account_id"]
        )

        if account:
            print(f"Selected account: {account['username']}")
            print(f"Health score: {account['health_score']}")

            # 3. ä½¿ç”¨è´¦å·æ‰§è¡Œæ“ä½œ
            try:
                # æ¨¡æ‹ŸæˆåŠŸæ“ä½œ
                await account_manager.update_account_success(
                    account['account_id'],
                    operation_type="tweet_search"
                )
            except Exception as e:
                # è®°å½•é”™è¯¯
                await account_manager.mark_account_error(
                    account['account_id'],
                    str(e),
                    error_type="api_error"
                )

        # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await account_manager.get_account_statistics()
        print(f"Total accounts: {stats['total_accounts']}")
        print(f"Active accounts: {stats['active_accounts']}")
        print(f"Average health: {stats['average_health_score']:.2f}")

        # 5. æ‰§è¡Œå¥åº·æ£€æŸ¥
        health_results = await account_manager.batch_health_check()
        print(f"Health check: {health_results['healthy']}/{health_results['checked']} healthy")

# ========== é…ç½®ç®¡ç† ==========

class AccountManagerConfig:
    """è´¦å·ç®¡ç†å™¨é…ç½®ç±»"""

    def __init__(self):
        # åŸºç¡€é…ç½®
        self.HEALTH_THRESHOLD = 0.7
        self.MAX_DAILY_USAGE = 1000
        self.COOLDOWN_DURATION = 1800  # 30åˆ†é’Ÿ
        self.ERROR_THRESHOLD = 5

        # é™åˆ¶é…ç½®
        self.DEFAULT_DAILY_LIMIT = 1000
        self.DEFAULT_HOURLY_LIMIT = 100
        self.HIGH_PRIORITY_DAILY_LIMIT = 1500
        self.LOW_PRIORITY_DAILY_LIMIT = 500

        # æ¢å¤é…ç½®
        self.AUTO_RECOVERY_ENABLED = True
        self.RECOVERY_HEALTH_THRESHOLD = 0.8
        self.RECOVERY_CHECK_INTERVAL = 3600  # 1å°æ—¶

        # ç›‘æ§é…ç½®
        self.HEALTH_CHECK_INTERVAL = 300  # 5åˆ†é’Ÿ
        self.METRICS_RETENTION_DAYS = 30
        self.ALERT_THRESHOLDS = {
            'low_health_accounts': 0.3,  # 30%ä»¥ä¸‹å¥åº·è´¦å·æ—¶å‘Šè­¦
            'high_error_rate': 0.2,      # 20%ä»¥ä¸Šé”™è¯¯ç‡æ—¶å‘Šè­¦
            'account_shortage': 5         # å¯ç”¨è´¦å·å°‘äº5ä¸ªæ—¶å‘Šè­¦
        }

# ========== Redisæ•°æ®ç»“æ„è®¾è®¡ ==========

class RedisDataStructure:
    """Redisæ•°æ®ç»“æ„è¯´æ˜"""

    def __init__(self):
        self.structures = {
            # è´¦å·é…ç½®
            "account:{account_id}:config": {
                "type": "hash",
                "fields": [
                    "account_id", "username", "email", "status", "priority",
                    "daily_limit", "hourly_limit", "cooldown_minutes",
                    "max_consecutive_errors", "auto_recovery", "tags"
                ],
                "example": {
                    "account_id": "tw_testuser",
                    "username": "testuser",
                    "email": "test@example.com",
                    "status": "active",
                    "priority": "normal",
                    "daily_limit": "1000",
                    "hourly_limit": "100",
                    "cooldown_minutes": "30",
                    "max_consecutive_errors": "5",
                    "auto_recovery": "true",
                    "tags": '["test", "development"]'
                }
            },

            # è´¦å·æŒ‡æ ‡
            "account:{account_id}:metrics": {
                "type": "hash",
                "fields": [
                    "total_requests", "successful_requests", "failed_requests",
                    "rate_limit_hits", "last_used", "last_success", "last_error",
                    "consecutive_errors", "daily_usage", "weekly_usage"
                ],
                "example": {
                    "total_requests": "150",
                    "successful_requests": "142",
                    "failed_requests": "8",
                    "rate_limit_hits": "2",
                    "last_used": "2024-01-01T12:00:00",
                    "last_success": "2024-01-01T11:58:00",
                    "last_error": "2024-01-01T10:30:00",
                    "consecutive_errors": "0",
                    "daily_usage": "45",
                    "weekly_usage": "320"
                }
            },

            # è´¦å·Cookies
            "account:{account_id}:cookies": {
                "type": "hash",
                "fields": ["cookies_data", "updated_at", "expires_at"],
                "example": {
                    "cookies_data": '{"auth_token": "...", "ct0": "..."}',
                    "updated_at": "2024-01-01T08:00:00",
                    "expires_at": "2024-01-08T08:00:00"
                }
            },

            # çŠ¶æ€ç´¢å¼•é›†åˆ
            "accounts:all": {
                "type": "set",
                "description": "æ‰€æœ‰è´¦å·IDé›†åˆ"
            },
            "accounts:active": {
                "type": "set",
                "description": "æ´»è·ƒè´¦å·IDé›†åˆ"
            },
            "accounts:suspended": {
                "type": "set",
                "description": "æš‚åœè´¦å·IDé›†åˆ"
            },

            # å†·å´æœŸç®¡ç†
            "account:{account_id}:cooldown": {
                "type": "string",
                "description": "å†·å´æœŸç»“æŸæ—¶é—´",
                "ttl": "è‡ªåŠ¨è¿‡æœŸ"
            },

            # æš‚åœä¿¡æ¯
            "account:{account_id}:suspension": {
                "type": "hash",
                "fields": ["reason", "suspended_at", "suspended_by"],
                "example": {
                    "reason": "Too many consecutive errors: 5",
                    "suspended_at": "2024-01-01T12:00:00",
                    "suspended_by": "auto_system"
                }
            }
        }

# ========== ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ ==========

class AccountMonitoringSystem:
    """è´¦å·ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ"""

    def __init__(self, account_manager: ProductionAccountManager):
        self.account_manager = account_manager
        self.logger = logging.getLogger(__name__)

    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§ç³»ç»Ÿ"""
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡
        asyncio.create_task(self._health_check_loop())
        asyncio.create_task(self._metrics_collection_loop())
        asyncio.create_task(self._alert_check_loop())

    async def _health_check_loop(self):
        """å¥åº·æ£€æŸ¥å¾ªç¯"""
        while True:
            try:
                await self.account_manager.batch_health_check()
                await asyncio.sleep(300)  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                self.logger.error(f"Health check loop error: {e}")
                await asyncio.sleep(60)

    async def _metrics_collection_loop(self):
        """æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        while True:
            try:
                await self._collect_system_metrics()
                await asyncio.sleep(60)  # 1åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡
            except Exception as e:
                self.logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(30)

    async def _alert_check_loop(self):
        """å‘Šè­¦æ£€æŸ¥å¾ªç¯"""
        while True:
            try:
                await self._check_alerts()
                await asyncio.sleep(120)  # 2åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                self.logger.error(f"Alert check error: {e}")
                await asyncio.sleep(60)

    async def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        stats = await self.account_manager.get_account_statistics()
        timestamp = datetime.utcnow().isoformat()

        # ä¿å­˜åˆ°æ—¶åºæ•°æ®
        metrics_key = f"metrics:system:{datetime.utcnow().strftime('%Y%m%d%H%M')}"
        await self.account_manager.redis.hset(metrics_key, mapping={
            'timestamp': timestamp,
            'total_accounts': stats['total_accounts'],
            'active_accounts': stats['active_accounts'],
            'suspended_accounts': stats['suspended_accounts'],
            'average_health_score': stats['average_health_score'],
            'daily_usage_total': stats['daily_usage_total']
        })

        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä¿ç•™30å¤©ï¼‰
        await self.account_manager.redis.expire(metrics_key, 86400 * 30)

    async def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        stats = await self.account_manager.get_account_statistics()

        alerts = []

        # æ£€æŸ¥å¥åº·è´¦å·æ¯”ä¾‹
        if stats['total_accounts'] > 0:
            healthy_ratio = stats['active_accounts'] / stats['total_accounts']
            if healthy_ratio < 0.3:
                alerts.append({
                    'type': 'low_healthy_accounts',
                    'severity': 'critical',
                    'message': f"Only {healthy_ratio:.1%} accounts are healthy",
                    'value': healthy_ratio,
                    'threshold': 0.3
                })

        # æ£€æŸ¥å¯ç”¨è´¦å·æ•°é‡
        if stats['active_accounts'] < 5:
            alerts.append({
                'type': 'account_shortage',
                'severity': 'warning',
                'message': f"Only {stats['active_accounts']} active accounts available",
                'value': stats['active_accounts'],
                'threshold': 5
            })

        # æ£€æŸ¥å¹³å‡å¥åº·åˆ†æ•°
        if stats['average_health_score'] < 0.6:
            alerts.append({
                'type': 'low_health_score',
                'severity': 'warning',
                'message': f"Average health score is {stats['average_health_score']:.2f}",
                'value': stats['average_health_score'],
                'threshold': 0.6
            })

        # å‘é€å‘Šè­¦
        for alert in alerts:
            await self._send_alert(alert)

    async def _send_alert(self, alert: Dict):
        """å‘é€å‘Šè­¦"""
        # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡ç›¸åŒå‘Šè­¦ï¼ˆé¿å…é‡å¤ï¼‰
        alert_key = f"alert:{alert['type']}:{datetime.utcnow().strftime('%Y%m%d%H')}"
        if await self.account_manager.redis.exists(alert_key):
            return

        # è®°å½•å‘Šè­¦
        await self.account_manager.redis.setex(alert_key, 3600, "sent")

        # è®°å½•åˆ°æ—¥å¿—
        self.logger.warning(f"ALERT [{alert['severity'].upper()}] {alert['message']}")

        # è¿™é‡Œå¯ä»¥é›†æˆå…¶ä»–å‘Šè­¦æ¸ é“ï¼ˆé‚®ä»¶ã€Slackã€é’‰é’‰ç­‰ï¼‰
        await self._send_to_external_channels(alert)

    async def _send_to_external_channels(self, alert: Dict):
        """å‘é€åˆ°å¤–éƒ¨å‘Šè­¦æ¸ é“"""
        # ç¤ºä¾‹ï¼šå‘é€åˆ°Webhook
        # webhook_url = "https://hooks.slack.com/services/..."
        # payload = {
        #     "text": f"XGet Alert: {alert['message']}",
        #     "severity": alert['severity']
        # }
        # async with aiohttp.ClientSession() as session:
        #     await session.post(webhook_url, json=payload)
        pass

# ========== æ€§èƒ½ä¼˜åŒ–å»ºè®® ==========

class PerformanceOptimizations:
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®å’Œæœ€ä½³å®è·µ"""

    def __init__(self):
        self.optimizations = {
            "redis_connection_pool": {
                "description": "ä½¿ç”¨Redisè¿æ¥æ± ",
                "implementation": """
                # ä½¿ç”¨è¿æ¥æ± 
                redis_pool = redis.ConnectionPool(
                    host='localhost',
                    port=6379,
                    db=0,
                    max_connections=20,
                    retry_on_timeout=True
                )
                redis_client = redis.Redis(connection_pool=redis_pool)
                """
            },

            "batch_operations": {
                "description": "æ‰¹é‡æ“ä½œå‡å°‘Redisè°ƒç”¨",
                "implementation": """
                # æ‰¹é‡æ›´æ–°æŒ‡æ ‡
                async def batch_update_metrics(self, updates: List[Dict]):
                    pipe = self.redis.pipeline()
                    for update in updates:
                        pipe.hincrby(f"account:{update['id']}:metrics",
                                   update['field'], update['value'])
                    await pipe.execute()
                """
            },

            "caching_strategy": {
                "description": "ç¼“å­˜é¢‘ç¹è®¿é—®çš„æ•°æ®",
                "implementation": """
                # å†…å­˜ç¼“å­˜è´¦å·é…ç½®
                from functools import lru_cache

                @lru_cache(maxsize=1000)
                async def get_cached_account_config(self, account_id: str):
                    # å®ç°ç¼“å­˜é€»è¾‘
                    pass
                """
            },

            "async_optimization": {
                "description": "å¼‚æ­¥æ“ä½œä¼˜åŒ–",
                "implementation": """
                # å¹¶å‘å¤„ç†å¤šä¸ªè´¦å·
                async def parallel_health_check(self, account_ids: List[str]):
                    tasks = [self._check_single_account_health(aid)
                            for aid in account_ids]
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    return results
                """
            }
        }

# ========== éƒ¨ç½²å’Œè¿ç»´æŒ‡å— ==========

class DeploymentGuide:
    """éƒ¨ç½²å’Œè¿ç»´æŒ‡å—"""

    def __init__(self):
        self.deployment_steps = [
            {
                "step": 1,
                "title": "ç¯å¢ƒå‡†å¤‡",
                "tasks": [
                    "å®‰è£…RedisæœåŠ¡å™¨",
                    "é…ç½®RedisæŒä¹…åŒ–",
                    "è®¾ç½®Rediså†…å­˜é™åˆ¶",
                    "é…ç½®Rediså®‰å…¨è®¤è¯"
                ]
            },
            {
                "step": 2,
                "title": "è´¦å·ç®¡ç†å™¨éƒ¨ç½²",
                "tasks": [
                    "åˆ›å»ºcoreç›®å½•ç»“æ„",
                    "éƒ¨ç½²account_manager.py",
                    "é…ç½®ç¯å¢ƒå˜é‡",
                    "åˆå§‹åŒ–Redisæ•°æ®ç»“æ„"
                ]
            },
            {
                "step": 3,
                "title": "ç›‘æ§ç³»ç»Ÿé…ç½®",
                "tasks": [
                    "é…ç½®PrometheusæŒ‡æ ‡æ”¶é›†",
                    "è®¾ç½®Grafanaä»ªè¡¨æ¿",
                    "é…ç½®å‘Šè­¦è§„åˆ™",
                    "æµ‹è¯•å‘Šè­¦é€šé“"
                ]
            },
            {
                "step": 4,
                "title": "ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–",
                "tasks": [
                    "è°ƒæ•´Redisé…ç½®å‚æ•°",
                    "è®¾ç½®æ—¥å¿—è½®è½¬",
                    "é…ç½®å¥åº·æ£€æŸ¥",
                    "åˆ¶å®šå¤‡ä»½ç­–ç•¥"
                ]
            }
        ]

        self.redis_config = """
        # Redisç”Ÿäº§é…ç½®å»ºè®®
        maxmemory 2gb
        maxmemory-policy allkeys-lru
        save 900 1
        save 300 10
        save 60 10000
        appendonly yes
        appendfsync everysec
        """

        self.monitoring_config = """
        # Prometheusé…ç½®
        global:
          scrape_interval: 15s

        scrape_configs:
          - job_name: 'xget-accounts'
            static_configs:
              - targets: ['localhost:8000']
            metrics_path: '/metrics'
            scrape_interval: 30s
        """

# ========== æ€»ç»“å’Œä¸‹ä¸€æ­¥ ==========

class AccountManagerSummary:
    """è´¦å·ç®¡ç†æ¨¡å—æ€»ç»“"""

    def __init__(self):
        self.features = {
            "âœ… å·²å®Œå–„çš„åŠŸèƒ½": [
                "æ™ºèƒ½è´¦å·é€‰æ‹©ç®—æ³•",
                "å¥åº·åˆ†æ•°è®¡ç®—",
                "é”™è¯¯å¤„ç†å’Œæ¢å¤",
                "ä½¿ç”¨é™åˆ¶å’Œå†·å´",
                "æ‰¹é‡å¥åº·æ£€æŸ¥",
                "ç»Ÿè®¡åˆ†æåŠŸèƒ½",
                "ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ",
                "Redisæ•°æ®ç»“æ„è®¾è®¡"
            ],

            "ğŸš€ æ ¸å¿ƒä¼˜åŠ¿": [
                "ç”Ÿäº§çº§ç¨³å®šæ€§",
                "æ™ºèƒ½è½®æ¢ç­–ç•¥",
                "è‡ªåŠ¨æ•…éšœæ¢å¤",
                "å…¨é¢ç›‘æ§ä½“ç³»",
                "çµæ´»é…ç½®ç®¡ç†",
                "é«˜æ€§èƒ½è®¾è®¡"
            ],

            "ğŸ“‹ å®æ–½å»ºè®®": [
                "å…ˆå®ç°æ ¸å¿ƒAccountManagerç±»",
                "é€æ­¥æ·»åŠ ç›‘æ§åŠŸèƒ½",
                "æµ‹è¯•å„ç§é”™è¯¯åœºæ™¯",
                "ä¼˜åŒ–æ€§èƒ½å‚æ•°",
                "å®Œå–„å‘Šè­¦æœºåˆ¶"
            ]
        }

        self.next_steps = [
            "åˆ›å»ºcore/account_manager.pyæ–‡ä»¶",
            "å®ç°åŸºç¡€çš„AccountManagerç±»",
            "é›†æˆåˆ°ç°æœ‰çš„twscrapeæµ‹è¯•ä¸­",
            "æ·»åŠ Redisæ”¯æŒå’Œé…ç½®",
            "å®ç°ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½",
            "ç¼–å†™å•å…ƒæµ‹è¯•",
            "æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–",
            "æ–‡æ¡£å®Œå–„å’Œéƒ¨ç½²æŒ‡å—"
        ]
```

### 3. ä»£ç†ç®¡ç†æ¨¡å— - SOCKS5ä¸“ç”¨ç‰ˆ

```python
# core/proxy_manager.py
import asyncio
import aiohttp
import aiosocks
import random
import json
import logging
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, asdict
import redis.asyncio as redis

class ProxyProtocol(Enum):
    """ä»£ç†åè®®ç±»å‹"""
    SOCKS5 = "socks5"
    SOCKS4 = "socks4"
    HTTP = "http"
    HTTPS = "https"

class ProxyStatus(Enum):
    """ä»£ç†çŠ¶æ€"""
    ACTIVE = "active"           # æ´»è·ƒå¯ç”¨
    INACTIVE = "inactive"       # ä¸æ´»è·ƒ
    ERROR = "error"            # é”™è¯¯çŠ¶æ€
    TESTING = "testing"        # æµ‹è¯•ä¸­
    BANNED = "banned"          # è¢«å°ç¦
    MAINTENANCE = "maintenance" # ç»´æŠ¤ä¸­

class ProxyRegion(Enum):
    """ä»£ç†åœ°åŒº"""
    US = "us"          # ç¾å›½
    EU = "eu"          # æ¬§æ´²
    ASIA = "asia"      # äºšæ´²
    GLOBAL = "global"  # å…¨çƒ

@dataclass
class ProxyMetrics:
    """ä»£ç†æŒ‡æ ‡æ•°æ®"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    timeout_requests: int = 0
    banned_requests: int = 0
    last_used: Optional[datetime] = None
    last_success: Optional[datetime] = None
    last_error: Optional[datetime] = None
    consecutive_errors: int = 0
    average_response_time: float = 0.0
    daily_usage: int = 0
    weekly_usage: int = 0

    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡è®¡ç®—"""
        if self.total_requests == 0:
            return 1.0
        return self.successful_requests / self.total_requests

    @property
    def health_score(self) -> float:
        """å¥åº·åˆ†æ•°è®¡ç®— (0-1)"""
        base_score = self.success_rate

        # è¿ç»­é”™è¯¯æƒ©ç½š
        error_penalty = min(self.consecutive_errors * 0.15, 0.6)

        # å“åº”æ—¶é—´å› å­
        time_factor = 1.0
        if self.average_response_time > 5.0:  # è¶…è¿‡5ç§’å“åº”æ—¶é—´
            time_factor = 0.7
        elif self.average_response_time > 3.0:  # è¶…è¿‡3ç§’
            time_factor = 0.85

        # ä½¿ç”¨é¢‘ç‡è°ƒæ•´
        usage_factor = 1.0
        if self.daily_usage > 800:  # æ¥è¿‘é™åˆ¶æ—¶é™ä½åˆ†æ•°
            usage_factor = 0.8
        elif self.daily_usage > 600:
            usage_factor = 0.9

        return max(0.0, (base_score - error_penalty) * time_factor * usage_factor)

@dataclass
class ProxyConfig:
    """SOCKS5ä»£ç†é…ç½®"""
    proxy_id: str
    host: str
    port: int
    username: str
    password: str
    protocol: ProxyProtocol = ProxyProtocol.SOCKS5
    region: ProxyRegion = ProxyRegion.GLOBAL
    status: ProxyStatus = ProxyStatus.ACTIVE
    max_concurrent: int = 10
    daily_limit: int = 1000
    max_consecutive_errors: int = 5
    timeout_seconds: int = 10
    provider: str = "unknown"
    cost_per_gb: float = 0.0
    expires_at: Optional[datetime] = None
    tags: List[str] = None

    def __post_init__(self):
        if self.tags is None:
            self.tags = []

    @property
    def proxy_url(self) -> str:
        """ç”Ÿæˆä»£ç†URL"""
        if self.protocol == ProxyProtocol.SOCKS5:
            return f"socks5://{self.username}:{self.password}@{self.host}:{self.port}"
        elif self.protocol == ProxyProtocol.HTTP:
            return f"http://{self.username}:{self.password}@{self.host}:{self.port}"
        else:
            return f"{self.protocol.value}://{self.username}:{self.password}@{self.host}:{self.port}"

    @property
    def connection_info(self) -> Dict:
        """è·å–è¿æ¥ä¿¡æ¯"""
        return {
            'proxy_type': aiosocks.SOCKS5 if self.protocol == ProxyProtocol.SOCKS5 else aiosocks.SOCKS4,
            'addr': self.host,
            'port': self.port,
            'username': self.username,
            'password': self.password
        }

class ProductionProxyManager:
    """ç”Ÿäº§çº§SOCKS5ä»£ç†ç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.logger = logging.getLogger(__name__)

        # é…ç½®å‚æ•°
        self.health_threshold = 0.7
        self.max_concurrent_per_proxy = 10
        self.health_check_interval = 300  # 5åˆ†é’Ÿ
        self.test_urls = [
            'https://httpbin.org/ip',
            'https://api.ipify.org?format=json',
            'https://ifconfig.me/ip'
        ]

        # æ€§èƒ½ç»Ÿè®¡
        self.response_times = {}

    async def initialize(self):
        """åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨"""
        await self._setup_redis_structures()
        self.logger.info("Proxy manager initialized")

    async def _setup_redis_structures(self):
        """è®¾ç½®Redisæ•°æ®ç»“æ„"""
        await self.redis.sadd('proxy_manager:initialized', '1')

    async def add_proxy_batch(self, proxy_list: List[Dict]) -> Dict[str, bool]:
        """æ‰¹é‡æ·»åŠ SOCKS5ä»£ç†"""
        results = {}

        for proxy_data in proxy_list:
            try:
                proxy_config = ProxyConfig(
                    proxy_id=f"socks5_{proxy_data['host']}_{proxy_data['port']}",
                    host=proxy_data['host'],
                    port=int(proxy_data['port']),
                    username=proxy_data['username'],
                    password=proxy_data['password'],
                    protocol=ProxyProtocol.SOCKS5,
                    region=ProxyRegion(proxy_data.get('region', 'global')),
                    provider=proxy_data.get('provider', 'unknown'),
                    cost_per_gb=float(proxy_data.get('cost_per_gb', 0.0)),
                    tags=proxy_data.get('tags', [])
                )

                success = await self._add_single_proxy(proxy_config)
                results[proxy_config.proxy_id] = success

            except Exception as e:
                self.logger.error(f"Failed to add proxy {proxy_data}: {e}")
                results[f"error_{proxy_data.get('host', 'unknown')}"] = False

        self.logger.info(f"Batch add completed: {sum(results.values())}/{len(proxy_list)} successful")
        return results

    async def _add_single_proxy(self, config: ProxyConfig) -> bool:
        """æ·»åŠ å•ä¸ªä»£ç†"""
        try:
            # ä¿å­˜ä»£ç†é…ç½®
            await self._save_proxy_config(config)

            # åˆå§‹åŒ–æŒ‡æ ‡
            await self._initialize_proxy_metrics(config.proxy_id)

            # æ‰§è¡Œåˆå§‹å¥åº·æ£€æŸ¥
            is_healthy = await self._test_proxy_connection(config)

            if is_healthy:
                await self._update_proxy_status(config.proxy_id, ProxyStatus.ACTIVE)
                self.logger.info(f"Proxy {config.proxy_id} added and verified")
            else:
                await self._update_proxy_status(config.proxy_id, ProxyStatus.ERROR)
                self.logger.warning(f"Proxy {config.proxy_id} added but failed initial test")

            return True

        except Exception as e:
            self.logger.error(f"Failed to add proxy {config.proxy_id}: {e}")
            return False

    async def get_available_proxy(self,
                                region: Optional[ProxyRegion] = None,
                                tags: Optional[List[str]] = None,
                                exclude_proxies: Optional[List[str]] = None,
                                min_health_score: float = 0.7) -> Optional[Dict]:
        """è·å–å¯ç”¨ä»£ç† - æ™ºèƒ½é€‰æ‹©ç®—æ³•"""
        try:
            # è·å–å€™é€‰ä»£ç†
            candidates = await self._get_candidate_proxies(region, tags, exclude_proxies, min_health_score)

            if not candidates:
                self.logger.warning("No candidate proxies available")
                return None

            # æ™ºèƒ½é€‰æ‹©æœ€ä½³ä»£ç†
            best_proxy = await self._select_best_proxy(candidates)

            if best_proxy:
                # æ›´æ–°ä½¿ç”¨è®°å½•
                await self._record_proxy_usage(best_proxy['proxy_id'])

                # è¿”å›è¿æ¥ä¿¡æ¯
                return {
                    'proxy_id': best_proxy['proxy_id'],
                    'proxy_url': best_proxy['config'].proxy_url,
                    'connection_info': best_proxy['config'].connection_info,
                    'health_score': best_proxy['metrics'].health_score,
                    'region': best_proxy['config'].region.value,
                    'response_time': best_proxy['metrics'].average_response_time
                }

            return None

        except Exception as e:
            self.logger.error(f"Failed to get available proxy: {e}")
            return None

    async def _get_candidate_proxies(self,
                                   region: Optional[ProxyRegion],
                                   tags: Optional[List[str]],
                                   exclude_proxies: Optional[List[str]],
                                   min_health_score: float) -> List[Dict]:
        """è·å–å€™é€‰ä»£ç†åˆ—è¡¨"""
        candidates = []

        # è·å–æ‰€æœ‰æ´»è·ƒä»£ç†
        active_proxies = await self.redis.smembers('proxies:active')

        for proxy_id in active_proxies:
            proxy_id = proxy_id.decode()

            # æ’é™¤æŒ‡å®šä»£ç†
            if exclude_proxies and proxy_id in exclude_proxies:
                continue

            # æ£€æŸ¥ä»£ç†é…ç½®
            config = await self._get_proxy_config(proxy_id)
            if not config:
                continue

            # åœ°åŒºè¿‡æ»¤
            if region and config.region != region:
                continue

            # æ ‡ç­¾è¿‡æ»¤
            if tags and not any(tag in config.tags for tag in tags):
                continue

            # æ£€æŸ¥å¹¶å‘é™åˆ¶
            current_usage = await self._get_current_concurrent_usage(proxy_id)
            if current_usage >= config.max_concurrent:
                continue

            # æ£€æŸ¥ä½¿ç”¨é™åˆ¶
            if await self._is_proxy_over_limit(proxy_id):
                continue

            # è·å–ä»£ç†æŒ‡æ ‡
            metrics = await self._get_proxy_metrics(proxy_id)

            # å¥åº·åˆ†æ•°è¿‡æ»¤
            if metrics.health_score < min_health_score:
                continue

            candidates.append({
                'proxy_id': proxy_id,
                'config': config,
                'metrics': metrics,
                'current_usage': current_usage
            })

        return candidates

    async def _select_best_proxy(self, candidates: List[Dict]) -> Optional[Dict]:
        """é€‰æ‹©æœ€ä½³ä»£ç† - ç»¼åˆè¯„åˆ†ç®—æ³•"""
        if not candidates:
            return None

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for candidate in candidates:
            score = await self._calculate_proxy_score(candidate)
            candidate['final_score'] = score

        # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†
        candidates.sort(key=lambda x: x['final_score'], reverse=True)

        return candidates[0]

    async def _calculate_proxy_score(self, candidate: Dict) -> float:
        """è®¡ç®—ä»£ç†ç»¼åˆè¯„åˆ†"""
        config = candidate['config']
        metrics = candidate['metrics']
        current_usage = candidate['current_usage']

        # åŸºç¡€å¥åº·åˆ†æ•° (40%)
        health_score = metrics.health_score * 0.4

        # å“åº”æ—¶é—´åˆ†æ•° (25%) - å“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
        max_acceptable_time = 5.0  # 5ç§’
        time_score = max(0, (max_acceptable_time - metrics.average_response_time) / max_acceptable_time) * 0.25

        # ä½¿ç”¨é¢‘ç‡åˆ†æ•° (20%) - ä½¿ç”¨è¶Šå°‘åˆ†æ•°è¶Šé«˜
        usage_ratio = metrics.daily_usage / config.daily_limit
        usage_score = (1 - usage_ratio) * 0.2

        # å¹¶å‘ä½¿ç”¨åˆ†æ•° (15%) - å¹¶å‘è¶Šå°‘åˆ†æ•°è¶Šé«˜
        concurrent_ratio = current_usage / config.max_concurrent
        concurrent_score = (1 - concurrent_ratio) * 0.15

        return health_score + time_score + usage_score + concurrent_score

    async def _test_proxy_connection(self, config: ProxyConfig) -> bool:
        """æµ‹è¯•SOCKS5ä»£ç†è¿æ¥"""
        try:
            start_time = time.time()

            # ä½¿ç”¨aiosocksè¿›è¡ŒSOCKS5è¿æ¥æµ‹è¯•
            conn_info = config.connection_info

            # æµ‹è¯•è¿æ¥åˆ°ç›®æ ‡URL
            test_url = random.choice(self.test_urls)

            async with aiosocks.Socks5Connector(
                proxy_host=conn_info['addr'],
                proxy_port=conn_info['port'],
                username=conn_info['username'],
                password=conn_info['password']
            ) as connector:

                timeout = aiohttp.ClientTimeout(total=config.timeout_seconds)
                async with aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout
                ) as session:

                    async with session.get(test_url) as response:
                        if response.status == 200:
                            response_time = time.time() - start_time
                            await self._update_response_time(config.proxy_id, response_time)
                            return True
                        else:
                            return False

        except asyncio.TimeoutError:
            self.logger.warning(f"Proxy {config.proxy_id} timeout during test")
            return False
        except Exception as e:
            self.logger.warning(f"Proxy {config.proxy_id} test failed: {e}")
            return False

    async def update_proxy_success(self, proxy_id: str, response_time: float = 0.0):
        """æ›´æ–°ä»£ç†æˆåŠŸè®°å½•"""
        try:
            current_time = datetime.utcnow()

            # æ›´æ–°åŸºç¡€æŒ‡æ ‡
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'total_requests', 1)
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'successful_requests', 1)
            await self.redis.hset(f'proxy:{proxy_id}:metrics', 'last_success', current_time.isoformat())
            await self.redis.hset(f'proxy:{proxy_id}:metrics', 'consecutive_errors', 0)

            # æ›´æ–°å“åº”æ—¶é—´
            if response_time > 0:
                await self._update_response_time(proxy_id, response_time)

            # æ›´æ–°ä½¿ç”¨è®¡æ•°
            await self._update_usage_counters(proxy_id)

            # å°è¯•æ¢å¤ä»£ç†çŠ¶æ€
            await self._try_proxy_recovery(proxy_id)

            self.logger.debug(f"Proxy {proxy_id} success updated")

        except Exception as e:
            self.logger.error(f"Failed to update proxy success: {e}")

    async def mark_proxy_error(self, proxy_id: str, error: str, error_type: str = "general"):
        """æ ‡è®°ä»£ç†é”™è¯¯"""
        try:
            current_time = datetime.utcnow()

            # æ›´æ–°é”™è¯¯æŒ‡æ ‡
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'total_requests', 1)
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'failed_requests', 1)
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'consecutive_errors', 1)
            await self.redis.hset(f'proxy:{proxy_id}:metrics', 'last_error', current_time.isoformat())

            # å¤„ç†ç‰¹å®šé”™è¯¯ç±»å‹
            await self._handle_specific_proxy_errors(proxy_id, error, error_type)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœä»£ç†
            consecutive_errors = await self.redis.hget(f'proxy:{proxy_id}:metrics', 'consecutive_errors')
            if consecutive_errors and int(consecutive_errors) >= 5:
                await self._suspend_proxy(proxy_id, f"Too many consecutive errors: {consecutive_errors}")

            self.logger.warning(f"Proxy {proxy_id} error marked: {error_type} - {error}")

        except Exception as e:
            self.logger.error(f"Failed to mark proxy error: {e}")

    async def _handle_specific_proxy_errors(self, proxy_id: str, error: str, error_type: str):
        """å¤„ç†ç‰¹å®šç±»å‹çš„ä»£ç†é”™è¯¯"""
        error_lower = error.lower()

        if 'timeout' in error_lower or 'connection timeout' in error_lower:
            # è¶…æ—¶é”™è¯¯
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'timeout_requests', 1)

        elif 'banned' in error_lower or 'blocked' in error_lower or '403' in error:
            # è¢«å°ç¦é”™è¯¯
            await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'banned_requests', 1)
            await self._suspend_proxy(proxy_id, f"Proxy appears to be banned: {error}")

        elif 'authentication' in error_lower or 'auth' in error_lower:
            # è®¤è¯é”™è¯¯ - å¯èƒ½ç”¨æˆ·åå¯†ç æœ‰é—®é¢˜
            await self._mark_proxy_auth_error(proxy_id, error)

        elif 'connection refused' in error_lower or 'unreachable' in error_lower:
            # è¿æ¥è¢«æ‹’ç» - ä»£ç†æœåŠ¡å™¨å¯èƒ½ä¸‹çº¿
            await self._suspend_proxy(proxy_id, f"Proxy server unreachable: {error}")

    async def batch_health_check(self) -> Dict:
        """æ‰¹é‡å¥åº·æ£€æŸ¥"""
        try:
            results = {
                'checked': 0,
                'healthy': 0,
                'unhealthy': 0,
                'recovered': 0,
                'suspended': 0,
                'details': []
            }

            all_proxies = await self.redis.smembers('proxies:all')

            # å¹¶å‘æ£€æŸ¥ä»£ç†å¥åº·çŠ¶æ€
            tasks = []
            for proxy_id in all_proxies:
                proxy_id = proxy_id.decode()
                tasks.append(self._check_single_proxy_health(proxy_id))

            # æ‰§è¡Œå¹¶å‘æ£€æŸ¥
            health_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in health_results:
                if isinstance(result, Exception):
                    self.logger.error(f"Health check error: {result}")
                    continue

                results['details'].append(result)
                results['checked'] += 1

                if result['healthy']:
                    results['healthy'] += 1
                    if result.get('recovered'):
                        results['recovered'] += 1
                else:
                    results['unhealthy'] += 1
                    if result.get('suspended'):
                        results['suspended'] += 1

            self.logger.info(f"Batch health check completed: {results['healthy']}/{results['checked']} healthy")
            return results

        except Exception as e:
            self.logger.error(f"Batch health check failed: {e}")
            return {'error': str(e)}

    async def _check_single_proxy_health(self, proxy_id: str) -> Dict:
        """å•ä¸ªä»£ç†å¥åº·æ£€æŸ¥"""
        try:
            config = await self._get_proxy_config(proxy_id)
            metrics = await self._get_proxy_metrics(proxy_id)

            if not config or not metrics:
                return {
                    'proxy_id': proxy_id,
                    'healthy': False,
                    'reason': 'Missing config or metrics'
                }

            # æ‰§è¡Œè¿æ¥æµ‹è¯•
            is_connected = await self._test_proxy_connection(config)
            health_score = metrics.health_score

            result = {
                'proxy_id': proxy_id,
                'healthy': is_connected and health_score >= self.health_threshold,
                'connected': is_connected,
                'health_score': health_score,
                'status': config.status.value,
                'response_time': metrics.average_response_time,
                'success_rate': metrics.success_rate,
                'consecutive_errors': metrics.consecutive_errors
            }

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
            if not result['healthy']:
                if metrics.consecutive_errors >= config.max_consecutive_errors:
                    result['should_suspend'] = True
                    result['reason'] = f"Too many consecutive errors: {metrics.consecutive_errors}"
                elif not is_connected:
                    result['should_suspend'] = True
                    result['reason'] = "Connection test failed"
                elif metrics.success_rate < 0.3:
                    result['should_suspend'] = True
                    result['reason'] = f"Low success rate: {metrics.success_rate:.2%}"

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
            elif config.status in [ProxyStatus.ERROR, ProxyStatus.INACTIVE]:
                if is_connected and health_score >= 0.8:
                    result['recovered'] = True
                    await self._recover_proxy(proxy_id)

            return result

        except Exception as e:
            return {
                'proxy_id': proxy_id,
                'healthy': False,
                'error': str(e)
            }

    async def get_proxy_statistics(self) -> Dict:
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_proxies': 0,
                'active_proxies': 0,
                'inactive_proxies': 0,
                'error_proxies': 0,
                'banned_proxies': 0,
                'region_distribution': {},
                'provider_distribution': {},
                'health_distribution': {'high': 0, 'medium': 0, 'low': 0},
                'average_health_score': 0.0,
                'average_response_time': 0.0,
                'total_daily_usage': 0,
                'total_concurrent_usage': 0
            }

            # è·å–æ‰€æœ‰ä»£ç†
            all_proxies = await self.redis.smembers('proxies:all')
            stats['total_proxies'] = len(all_proxies)

            total_health = 0.0
            total_response_time = 0.0
            active_count = 0

            for proxy_id in all_proxies:
                proxy_id = proxy_id.decode()

                config = await self._get_proxy_config(proxy_id)
                metrics = await self._get_proxy_metrics(proxy_id)

                if not config or not metrics:
                    continue

                # çŠ¶æ€ç»Ÿè®¡
                status_key = f"{config.status.value}_proxies"
                if status_key in stats:
                    stats[status_key] += 1

                # åœ°åŒºç»Ÿè®¡
                region = config.region.value
                stats['region_distribution'][region] = stats['region_distribution'].get(region, 0) + 1

                # æä¾›å•†ç»Ÿè®¡
                provider = config.provider
                stats['provider_distribution'][provider] = stats['provider_distribution'].get(provider, 0) + 1

                # å¥åº·åˆ†æ•°ç»Ÿè®¡
                health_score = metrics.health_score
                total_health += health_score

                if health_score >= 0.8:
                    stats['health_distribution']['high'] += 1
                elif health_score >= 0.6:
                    stats['health_distribution']['medium'] += 1
                else:
                    stats['health_distribution']['low'] += 1

                # å“åº”æ—¶é—´ç»Ÿè®¡
                if metrics.average_response_time > 0:
                    total_response_time += metrics.average_response_time
                    active_count += 1

                # ä½¿ç”¨é‡ç»Ÿè®¡
                stats['total_daily_usage'] += metrics.daily_usage

                # å¹¶å‘ä½¿ç”¨ç»Ÿè®¡
                current_usage = await self._get_current_concurrent_usage(proxy_id)
                stats['total_concurrent_usage'] += current_usage

            # è®¡ç®—å¹³å‡å€¼
            if stats['total_proxies'] > 0:
                stats['average_health_score'] = total_health / stats['total_proxies']

            if active_count > 0:
                stats['average_response_time'] = total_response_time / active_count

            return stats

        except Exception as e:
            self.logger.error(f"Failed to get proxy statistics: {e}")
            return {}

    async def remove_proxy(self, proxy_id: str) -> bool:
        """ç§»é™¤ä»£ç†"""
        try:
            # æ¸…ç†ä»£ç†æ•°æ®
            await self._cleanup_proxy_data(proxy_id)

            self.logger.info(f"Proxy {proxy_id} removed successfully")
            return True

        except Exception as e:
            self.logger.error(f"Failed to remove proxy {proxy_id}: {e}")
            return False

    # ========== è¾…åŠ©æ–¹æ³• ==========

    async def _save_proxy_config(self, config: ProxyConfig):
        """ä¿å­˜ä»£ç†é…ç½®"""
        try:
            config_dict = asdict(config)
            config_dict['protocol'] = config.protocol.value
            config_dict['region'] = config.region.value
            config_dict['status'] = config.status.value
            config_dict['tags'] = json.dumps(config.tags)

            if config.expires_at:
                config_dict['expires_at'] = config.expires_at.isoformat()

            await self.redis.hset(f'proxy:{config.proxy_id}:config', mapping=config_dict)
            await self.redis.sadd('proxies:all', config.proxy_id)

            # æ›´æ–°çŠ¶æ€ç´¢å¼•
            await self._update_proxy_status_index(config.proxy_id, config.status)

        except Exception as e:
            self.logger.error(f"Failed to save proxy config: {e}")

    async def _get_proxy_config(self, proxy_id: str) -> Optional[ProxyConfig]:
        """è·å–ä»£ç†é…ç½®"""
        try:
            config_data = await self.redis.hgetall(f'proxy:{proxy_id}:config')
            if not config_data:
                return None

            # è½¬æ¢å­—èŠ‚æ•°æ®
            config_dict = {k.decode(): v.decode() for k, v in config_data.items()}

            # å¤„ç†æšä¸¾ç±»å‹
            config_dict['protocol'] = ProxyProtocol(config_dict['protocol'])
            config_dict['region'] = ProxyRegion(config_dict['region'])
            config_dict['status'] = ProxyStatus(config_dict['status'])
            config_dict['tags'] = json.loads(config_dict.get('tags', '[]'))

            # å¤„ç†æ•°å€¼ç±»å‹
            for field in ['port', 'max_concurrent', 'daily_limit', 'max_consecutive_errors', 'timeout_seconds']:
                if field in config_dict:
                    config_dict[field] = int(config_dict[field])

            config_dict['cost_per_gb'] = float(config_dict.get('cost_per_gb', 0.0))

            # å¤„ç†æ—¥æœŸæ—¶é—´
            if 'expires_at' in config_dict and config_dict['expires_at']:
                config_dict['expires_at'] = datetime.fromisoformat(config_dict['expires_at'])

            return ProxyConfig(**config_dict)

        except Exception as e:
            self.logger.error(f"Failed to get proxy config for {proxy_id}: {e}")
            return None

    async def _initialize_proxy_metrics(self, proxy_id: str):
        """åˆå§‹åŒ–ä»£ç†æŒ‡æ ‡"""
        metrics = ProxyMetrics()
        await self._save_proxy_metrics(proxy_id, metrics)

    async def _get_proxy_metrics(self, proxy_id: str) -> ProxyMetrics:
        """è·å–ä»£ç†æŒ‡æ ‡"""
        try:
            metrics_data = await self.redis.hgetall(f'proxy:{proxy_id}:metrics')

            if not metrics_data:
                return ProxyMetrics()

            # è½¬æ¢æ•°æ®ç±»å‹
            metrics_dict = {}
            for k, v in metrics_data.items():
                key = k.decode()
                value = v.decode()

                if key in ['total_requests', 'successful_requests', 'failed_requests',
                          'timeout_requests', 'banned_requests', 'consecutive_errors',
                          'daily_usage', 'weekly_usage']:
                    metrics_dict[key] = int(value) if value else 0
                elif key == 'average_response_time':
                    metrics_dict[key] = float(value) if value else 0.0
                elif key in ['last_used', 'last_success', 'last_error']:
                    if value:
                        metrics_dict[key] = datetime.fromisoformat(value)

            return ProxyMetrics(**metrics_dict)

        except Exception as e:
            self.logger.error(f"Failed to get proxy metrics for {proxy_id}: {e}")
            return ProxyMetrics()

    async def _save_proxy_metrics(self, proxy_id: str, metrics: ProxyMetrics):
        """ä¿å­˜ä»£ç†æŒ‡æ ‡"""
        try:
            metrics_dict = asdict(metrics)

            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            for key, value in metrics_dict.items():
                if isinstance(value, datetime):
                    metrics_dict[key] = value.isoformat()
                elif value is None:
                    metrics_dict[key] = ''

            await self.redis.hset(f'proxy:{proxy_id}:metrics', mapping=metrics_dict)

        except Exception as e:
            self.logger.error(f"Failed to save proxy metrics: {e}")

    async def _update_proxy_status_index(self, proxy_id: str, status: ProxyStatus):
        """æ›´æ–°ä»£ç†çŠ¶æ€ç´¢å¼•"""
        # ä»æ‰€æœ‰çŠ¶æ€é›†åˆä¸­ç§»é™¤
        for s in ProxyStatus:
            await self.redis.srem(f'proxies:{s.value}', proxy_id)

        # æ·»åŠ åˆ°æ–°çŠ¶æ€é›†åˆ
        await self.redis.sadd(f'proxies:{status.value}', proxy_id)

    async def _update_proxy_status(self, proxy_id: str, status: ProxyStatus):
        """æ›´æ–°ä»£ç†çŠ¶æ€"""
        await self.redis.hset(f'proxy:{proxy_id}:config', 'status', status.value)
        await self._update_proxy_status_index(proxy_id, status)

    async def _get_current_concurrent_usage(self, proxy_id: str) -> int:
        """è·å–å½“å‰å¹¶å‘ä½¿ç”¨æ•°"""
        usage = await self.redis.get(f'proxy:{proxy_id}:concurrent')
        return int(usage) if usage else 0

    async def _is_proxy_over_limit(self, proxy_id: str) -> bool:
        """æ£€æŸ¥ä»£ç†æ˜¯å¦è¶…è¿‡ä½¿ç”¨é™åˆ¶"""
        config = await self._get_proxy_config(proxy_id)
        metrics = await self._get_proxy_metrics(proxy_id)

        if not config or not metrics:
            return True

        return metrics.daily_usage >= config.daily_limit

    async def _record_proxy_usage(self, proxy_id: str):
        """è®°å½•ä»£ç†ä½¿ç”¨"""
        await self.redis.hincrby(f'proxy:{proxy_id}:metrics', 'daily_usage', 1)
        await self.redis.hset(f'proxy:{proxy_id}:metrics', 'last_used', datetime.utcnow().isoformat())

        # å¢åŠ å¹¶å‘è®¡æ•°
        await self.redis.incr(f'proxy:{proxy_id}:concurrent')
        await self.redis.expire(f'proxy:{proxy_id}:concurrent', 300)  # 5åˆ†é’Ÿè¿‡æœŸ

    async def _update_response_time(self, proxy_id: str, response_time: float):
        """æ›´æ–°å“åº”æ—¶é—´"""
        # è·å–å½“å‰å¹³å‡å“åº”æ—¶é—´
        current_avg = await self.redis.hget(f'proxy:{proxy_id}:metrics', 'average_response_time')
        current_avg = float(current_avg) if current_avg else 0.0

        # è®¡ç®—æ–°çš„å¹³å‡å“åº”æ—¶é—´ï¼ˆç®€å•ç§»åŠ¨å¹³å‡ï¼‰
        new_avg = (current_avg * 0.8) + (response_time * 0.2)

        await self.redis.hset(f'proxy:{proxy_id}:metrics', 'average_response_time', str(new_avg))

    async def _suspend_proxy(self, proxy_id: str, reason: str):
        """æš‚åœä»£ç†"""
        await self._update_proxy_status(proxy_id, ProxyStatus.ERROR)

        # è®°å½•æš‚åœåŸå› 
        await self.redis.hset(f'proxy:{proxy_id}:suspension',
                            'reason', reason,
                            'suspended_at', datetime.utcnow().isoformat())

        self.logger.warning(f"Proxy {proxy_id} suspended: {reason}")

    async def _recover_proxy(self, proxy_id: str):
        """æ¢å¤ä»£ç†"""
        await self._update_proxy_status(proxy_id, ProxyStatus.ACTIVE)

        # æ¸…ç†æš‚åœè®°å½•
        await self.redis.delete(f'proxy:{proxy_id}:suspension')

        self.logger.info(f"Proxy {proxy_id} recovered")

    async def _cleanup_proxy_data(self, proxy_id: str):
        """æ¸…ç†ä»£ç†æ•°æ®"""
        # åˆ é™¤æ‰€æœ‰ç›¸å…³çš„Redisé”®
        keys_to_delete = [
            f'proxy:{proxy_id}:config',
            f'proxy:{proxy_id}:metrics',
            f'proxy:{proxy_id}:suspension',
            f'proxy:{proxy_id}:concurrent'
        ]

        for key in keys_to_delete:
            await self.redis.delete(key)

        # ä»æ‰€æœ‰é›†åˆä¸­ç§»é™¤
        await self.redis.srem('proxies:all', proxy_id)
        for status in ProxyStatus:
            await self.redis.srem(f'proxies:{status.value}', proxy_id)

### 4. æ•°æ®å­˜å‚¨æ¨¡å—

```python
# core/storage.py
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel, ValidationError
from typing import Dict, List, Optional
import logging
from datetime import datetime

class TweetModel(BaseModel):
    """æ¨æ–‡æ•°æ®æ¨¡å‹"""
    id: str
    text: str
    user_id: str
    username: str
    created_at: str
    retweet_count: int = 0
    like_count: int = 0
    reply_count: int = 0
    quote_count: int = 0
    view_count: int = 0
    media: List[Dict] = []
    hashtags: List[str] = []
    urls: List[str] = []
    collected_at: str
    source_account: Optional[str] = None
    source_proxy: Optional[str] = None

class ProductionDataManager:
    """ç”Ÿäº§çº§æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, mongodb_uri: str, redis_client):
        self.client = AsyncIOMotorClient(mongodb_uri)
        self.db = self.client.xget
        self.redis = redis_client
        self.logger = logging.getLogger(__name__)

    async def save_tweets(self, tweets: List[Dict]) -> Dict[str, int]:
        """æ‰¹é‡ä¿å­˜æ¨æ–‡ - å¸¦éªŒè¯å’Œç»Ÿè®¡"""
        stats = {'saved': 0, 'failed': 0, 'duplicates': 0}

        for tweet_data in tweets:
            try:
                # æ•°æ®éªŒè¯
                tweet = TweetModel(**tweet_data)

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = await self.db.tweets.find_one({'id': tweet.id})
                if existing:
                    # æ›´æ–°äº’åŠ¨æ•°æ®
                    await self.db.tweets.update_one(
                        {'id': tweet.id},
                        {'$set': {
                            'retweet_count': tweet.retweet_count,
                            'like_count': tweet.like_count,
                            'reply_count': tweet.reply_count,
                            'quote_count': tweet.quote_count,
                            'view_count': tweet.view_count,
                            'updated_at': datetime.utcnow()
                        }}
                    )
                    stats['duplicates'] += 1
                else:
                    # æ’å…¥æ–°æ¨æ–‡
                    await self.db.tweets.insert_one(tweet.dict())
                    stats['saved'] += 1

            except ValidationError as e:
                self.logger.error(f"Tweet validation failed: {e}")
                stats['failed'] += 1
            except Exception as e:
                self.logger.error(f"Failed to save tweet: {e}")
                stats['failed'] += 1

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        await self._update_collection_stats('tweets', stats)
        return stats

    async def _update_collection_stats(self, collection: str, stats: Dict):
        """æ›´æ–°é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        date_key = datetime.utcnow().strftime('%Y-%m-%d')
        for key, value in stats.items():
            await self.redis.hincrby(f'stats:{collection}:{date_key}', key, value)
        await self.redis.expire(f'stats:{collection}:{date_key}', 86400 * 30)
```

### 5. ä»»åŠ¡è°ƒåº¦æ¨¡å—

```python
# core/tasks.py
from celery import Celery
from celery.exceptions import Retry
import asyncio
import logging
from datetime import datetime
from .scraper import ProductionTwitterScraper
from .storage import ProductionDataManager
from .account_manager import AccountManager
from .proxy_manager import ProxyManager

app = Celery('xget')
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_routes={
        'core.tasks.search_task': {'queue': 'search'},
        'core.tasks.user_profile_task': {'queue': 'profile'},
        'core.tasks.health_check_task': {'queue': 'maintenance'},
    }
)

@app.task(bind=True, max_retries=3, default_retry_delay=60)
def search_task(self, keyword: str, count: int = 100, priority: str = 'normal'):
    """æœç´¢ä»»åŠ¡ - å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)
        scraper = ProductionTwitterScraper(account_manager, proxy_manager)
        data_manager = ProductionDataManager(MONGODB_URI, redis_client)

        # æ‰§è¡Œæœç´¢
        tweets = asyncio.run(scraper.search_tweets(keyword, count))

        # ä¿å­˜æ•°æ®
        stats = asyncio.run(data_manager.save_tweets(tweets))

        # è®°å½•ä»»åŠ¡å®Œæˆ
        task_result = {
            "status": "success",
            "keyword": keyword,
            "requested_count": count,
            "collected_count": len(tweets),
            "save_stats": stats,
            "completed_at": datetime.utcnow().isoformat()
        }

        logging.info(f"Search task completed: {keyword}, collected {len(tweets)} tweets")
        return task_result

    except Exception as e:
        logging.error(f"Search task failed: {keyword}, error: {str(e)}")

        # é‡è¯•é€»è¾‘
        if self.request.retries < self.max_retries:
            logging.info(f"Retrying search task for {keyword} (attempt {self.request.retries + 1})")
            raise self.retry(countdown=60 * (2 ** self.request.retries))

        return {
            "status": "failed",
            "keyword": keyword,
            "error": str(e),
            "retries": self.request.retries,
            "failed_at": datetime.utcnow().isoformat()
        }

@app.task(bind=True, max_retries=2)
def user_profile_task(self, username: str):
    """ç”¨æˆ·èµ„æ–™é‡‡é›†ä»»åŠ¡"""
    try:
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)
        scraper = ProductionTwitterScraper(account_manager, proxy_manager)
        data_manager = ProductionDataManager(MONGODB_URI, redis_client)

        # è·å–ç”¨æˆ·èµ„æ–™
        user_data = asyncio.run(scraper.get_user_profile(username))
        if not user_data:
            return {"status": "not_found", "username": username}

        # ä¿å­˜ç”¨æˆ·æ•°æ®
        await data_manager.save_user(user_data)

        return {
            "status": "success",
            "username": username,
            "user_id": user_data['user_id'],
            "completed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logging.error(f"User profile task failed: {username}, error: {str(e)}")

        if self.request.retries < self.max_retries:
            raise self.retry(countdown=30)

        return {
            "status": "failed",
            "username": username,
            "error": str(e),
            "failed_at": datetime.utcnow().isoformat()
        }

@app.task
def health_check_task():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)

        # æ£€æŸ¥è´¦å·å¥åº·çŠ¶æ€
        asyncio.run(account_manager.check_all_accounts_health())

        # æ£€æŸ¥ä»£ç†å¥åº·çŠ¶æ€
        asyncio.run(proxy_manager.batch_health_check())

        return {
            "status": "completed",
            "checked_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logging.error(f"Health check failed: {str(e)}")
        return {"status": "failed", "error": str(e)}

# å®šæ—¶ä»»åŠ¡é…ç½®
app.conf.beat_schedule = {
    'health-check': {
        'task': 'core.tasks.health_check_task',
        'schedule': 300.0,  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    },
}
```

## éƒ¨ç½²é…ç½®

### ç”Ÿäº§çº§ Docker Compose é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  mongodb:
    image: mongo:6
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD}
      - MONGO_INITDB_DATABASE=xget
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    restart: unless-stopped
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3

  xget-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENVIRONMENT=${ENVIRONMENT:-production}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  celery-worker-search:
    build: .
    command: celery -A core.tasks worker --loglevel=info --queues=search --concurrency=4
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      replicas: 2

  celery-worker-profile:
    build: .
    command: celery -A core.tasks worker --loglevel=info --queues=profile --concurrency=2
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped

  celery-beat:
    build: .
    command: celery -A core.tasks beat --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped

  # å¯é€‰ï¼šç›‘æ§ç»„ä»¶
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis_data:
  mongodb_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

### ç¯å¢ƒé…ç½®æ–‡ä»¶

```bash
# .env.production
MONGO_USER=xget_user
MONGO_PASSWORD=your_secure_password
LOG_LEVEL=INFO
ENVIRONMENT=production
GRAFANA_PASSWORD=your_grafana_password

# ä»£ç†é…ç½®
PROXY_API_KEY=your_proxy_api_key
PROXY_ENDPOINT=https://your-proxy-provider.com/api

# Twitterè´¦å·åŠ å¯†å¯†é’¥
ACCOUNT_ENCRYPTION_KEY=your_32_char_encryption_key

# ç›‘æ§é…ç½®
PROMETHEUS_ENABLED=true
GRAFANA_ENABLED=true
```

### Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨
RUN playwright install chromium

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash xget
RUN chown -R xget:xget /app
USER xget

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## å®æ–½è®¡åˆ’

### ğŸ‰ æŠ€æœ¯éªŒè¯é˜¶æ®µ (å·²å®Œæˆ)

- [x] **Python 3.12 ç¯å¢ƒæ­å»º** - å®Œæˆ
- [x] **twscrape åŠŸèƒ½éªŒè¯** - 100% é€šè¿‡
- [x] **Playwright åŠŸèƒ½éªŒè¯** - 100% é€šè¿‡
- [x] **Cookies è‡ªåŠ¨åŒ–æ–¹æ¡ˆ** - æ ¸å¿ƒçªç ´å®Œæˆ
- [x] **Twitter æ•°æ®é‡‡é›†æµ‹è¯•** - è·å–çœŸå®æ•°æ®æˆåŠŸ
- [x] **æŠ€æœ¯åˆ†å±‚æ¶æ„è®¾è®¡** - å®Œæˆ
- [x] **å¼€å‘ç¯å¢ƒé…ç½®** - å®Œæˆ
- [x] **é¡¹ç›®æ–‡æ¡£å’Œå·¥å…·** - å®Œæˆ

### ç¬¬ä¸€é˜¶æ®µ (2-3å‘¨): æ ¸å¿ƒåŠŸèƒ½å¼€å‘

- [x] é¡¹ç›®ç»“æ„æ­å»ºå’ŒæŠ€æœ¯éªŒè¯
- [ ] è´¦å·ç®¡ç†æ¨¡å—å¼€å‘ (åŸºäºPlaywrightè‡ªåŠ¨åŒ–)
- [ ] ä»£ç†ç®¡ç†æ¨¡å—å¼€å‘
- [ ] åŸºç¡€çˆ¬å–æ¨¡å—å¼€å‘ (twscrapeé›†æˆï¼Œå·²éªŒè¯å¯è¡Œ)
- [ ] æ•°æ®å­˜å‚¨æ¨¡å—å¼€å‘ (MongoDB + æ•°æ®éªŒè¯)
- [ ] ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ (Celery + é˜Ÿåˆ—åˆ†ç¦»)
- [ ] Dockerç¯å¢ƒé…ç½®

### ç¬¬äºŒé˜¶æ®µ (1-2å‘¨): APIå’Œç›‘æ§

- [ ] FastAPIæ¥å£å¼€å‘
- [ ] ä»»åŠ¡ç®¡ç†API (æäº¤ã€æŸ¥è¯¢ã€å–æ¶ˆ)
- [ ] æ•°æ®æŸ¥è¯¢API (æ”¯æŒå¤æ‚æŸ¥è¯¢)
- [ ] ç³»ç»Ÿç›‘æ§API (å¥åº·çŠ¶æ€ã€ç»Ÿè®¡ä¿¡æ¯)
- [ ] åŸºç¡€Webç®¡ç†ç•Œé¢
- [ ] Prometheus + Grafanaé›†æˆ

### ç¬¬ä¸‰é˜¶æ®µ (1-2å‘¨): ç”Ÿäº§ä¼˜åŒ–

- [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶å®Œå–„
- [ ] æ€§èƒ½ä¼˜åŒ– (æ‰¹é‡å¤„ç†ã€è¿æ¥æ± )
- [ ] å®‰å…¨åŠ å›º (è¾“å…¥éªŒè¯ã€è®¿é—®æ§åˆ¶)
- [ ] æ—¥å¿—ç³»ç»Ÿå®Œå–„ (ç»“æ„åŒ–æ—¥å¿—)
- [ ] å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½è°ƒä¼˜
- [ ] éƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œ

### ç¬¬å››é˜¶æ®µ (1å‘¨): æµ‹è¯•å’Œå‘å¸ƒ

- [ ] é›†æˆæµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] ç›‘æ§å‘Šè­¦é…ç½®
- [ ] ç”¨æˆ·åŸ¹è®­å’Œæ–‡æ¡£

## æŠ€æœ¯å€ºåŠ¡ç®¡ç†

### å¿…é¡»å®ç°çš„æ ¸å¿ƒåŠŸèƒ½

1. **è´¦å·æ± ç®¡ç†** - ä¸å¯çœç•¥ï¼Œç›´æ¥å½±å“ç³»ç»Ÿç¨³å®šæ€§
2. **ä»£ç†IPè½®æ¢** - ç”Ÿäº§ç¯å¢ƒå¿…éœ€
3. **æ•°æ®éªŒè¯** - ç¡®ä¿æ•°æ®è´¨é‡
4. **é”™è¯¯å¤„ç†** - æé«˜ç³»ç»Ÿé²æ£’æ€§
5. **åŸºç¡€ç›‘æ§** - ç”Ÿäº§è¿ç»´å¿…éœ€

### å¯ä»¥åæœŸæ·»åŠ çš„åŠŸèƒ½

1. **å¤æ‚çš„MLé¢„æµ‹** - éæ ¸å¿ƒåŠŸèƒ½
2. **ä¼ä¸šçº§æƒé™ç³»ç»Ÿ** - å¯ç”¨ç®€å•è®¤è¯æ›¿ä»£
3. **æ•°æ®å…³ç³»å›¾è°±** - å¯åœ¨MongoDBä¸­ç®€å•å­˜å‚¨
4. **é«˜çº§åˆ†æåŠŸèƒ½** - ä¸šåŠ¡ä»·å€¼éªŒè¯åæ·»åŠ 

## é£é™©è¯„ä¼°ä¸åº”å¯¹

### æŠ€æœ¯é£é™© âš ï¸

**é£é™©**: Twitteråçˆ¬è™«æœºåˆ¶å‡çº§
**åº”å¯¹**:
- å¤šæ ·åŒ–çš„çˆ¬å–ç­–ç•¥ (twscrape + Playwright)
- çµæ´»çš„è´¦å·å’Œä»£ç†è½®æ¢
- å¿«é€Ÿé€‚åº”æœºåˆ¶

**é£é™©**: è´¦å·å°ç¦ç‡è¿‡é«˜
**åº”å¯¹**:
- è´¦å·å¥åº·ç›‘æ§
- ä½¿ç”¨é¢‘ç‡æ§åˆ¶
- å¤šè´¦å·æ± å¤‡ä»½

### ä¸šåŠ¡é£é™© âš ï¸

**é£é™©**: æ•°æ®é‡è¶…å‡ºé¢„æœŸ
**åº”å¯¹**:
- æ°´å¹³æ‰©å±•è®¾è®¡
- æ•°æ®åˆ†ç‰‡ç­–ç•¥
- æ€§èƒ½ç›‘æ§å’Œé¢„è­¦

### è¿ç»´é£é™© âœ…

**é£é™©**: ç³»ç»Ÿç»´æŠ¤å¤æ‚
**åº”å¯¹**:
- å®Œå–„çš„ç›‘æ§ä½“ç³»
- è‡ªåŠ¨åŒ–éƒ¨ç½²
- è¯¦ç»†çš„è¿ç»´æ–‡æ¡£

## æˆæœ¬ä¼°ç®—

### å¼€å‘æˆæœ¬

- **äººåŠ›**: 2-3åå¼€å‘äººå‘˜ï¼Œ6-8å‘¨
- **æŠ€æœ¯æ ˆ**: å…¨éƒ¨ä½¿ç”¨å¼€æºæŠ€æœ¯ï¼Œæ— æˆæƒè´¹ç”¨
- **æ€»ä½“**: ç›¸æ¯”åŸæ–¹æ¡ˆå‡å°‘60%çš„å¼€å‘æ—¶é—´

### è¿ç»´æˆæœ¬

- **æœåŠ¡å™¨**: ä¸­ç­‰é…ç½®å³å¯æ»¡è¶³åˆæœŸéœ€æ±‚
- **ä»£ç†IP**: æ ¹æ®é‡‡é›†é‡æŒ‰éœ€è´­ä¹°
- **ç›‘æ§**: ä½¿ç”¨å¼€æºæ–¹æ¡ˆï¼Œæˆæœ¬å¯æ§

## Webå±•ç¤ºé¡µé¢è®¾è®¡

### ç®¡ç†åå°ç•Œé¢

åŸºäºFastAPI + Vue.jsæ„å»ºçš„ç°ä»£åŒ–Webç®¡ç†ç•Œé¢ï¼Œæä¾›å®Œæ•´çš„ç³»ç»Ÿç®¡ç†åŠŸèƒ½ã€‚

#### ğŸ¯ **æ ¸å¿ƒé¡µé¢æ¨¡å—**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        XGet Webç®¡ç†ç•Œé¢                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸ“Š æ•°æ®æŸ¥è¯¢    â”‚   ğŸ”§ ç³»ç»Ÿç®¡ç†    â”‚   ğŸ“ˆ ç»Ÿè®¡åˆ†æ    â”‚   âš™ï¸ ç³»ç»Ÿè®¾ç½®   â”‚
â”‚   æ¨æ–‡æœç´¢      â”‚   è´¦å·ç®¡ç†      â”‚   é‡‡é›†ç»Ÿè®¡      â”‚   é…ç½®ç®¡ç†     â”‚
â”‚   ç”¨æˆ·æŸ¥è¯¢      â”‚   ä»£ç†ç®¡ç†      â”‚   æ€§èƒ½ç›‘æ§      â”‚   æƒé™ç®¡ç†     â”‚
â”‚   æ•°æ®å¯¼å‡º      â”‚   ä»»åŠ¡ç®¡ç†      â”‚   é”™è¯¯åˆ†æ      â”‚   ç³»ç»Ÿæ—¥å¿—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ“Š **æ•°æ®æŸ¥è¯¢é¡µé¢**

**æ¨æ–‡æœç´¢ç•Œé¢**
```html
<!-- æ¨æ–‡æœç´¢é¡µé¢ -->
<template>
  <div class="tweet-search-page">
    <!-- æœç´¢è¡¨å• -->
    <el-card class="search-form">
      <el-form :model="searchForm" label-width="120px">
        <el-row :gutter="20">
          <el-col :span="8">
            <el-form-item label="å…³é”®è¯">
              <el-input v-model="searchForm.keyword" placeholder="è¾“å…¥æœç´¢å…³é”®è¯" />
            </el-form-item>
          </el-col>
          <el-col :span="8">
            <el-form-item label="æ—¶é—´èŒƒå›´">
              <el-date-picker
                v-model="searchForm.dateRange"
                type="datetimerange"
                range-separator="è‡³"
                start-placeholder="å¼€å§‹æ—¶é—´"
                end-placeholder="ç»“æŸæ—¶é—´"
              />
            </el-form-item>
          </el-col>
          <el-col :span="8">
            <el-form-item label="ç”¨æˆ·å">
              <el-input v-model="searchForm.username" placeholder="æŒ‡å®šç”¨æˆ·å" />
            </el-form-item>
          </el-col>
        </el-row>
        <el-row>
          <el-col :span="24">
            <el-button type="primary" @click="searchTweets">æœç´¢</el-button>
            <el-button @click="resetForm">é‡ç½®</el-button>
            <el-button type="success" @click="exportData">å¯¼å‡ºæ•°æ®</el-button>
          </el-col>
        </el-row>
      </el-form>
    </el-card>

    <!-- æœç´¢ç»“æœ -->
    <el-card class="search-results">
      <el-table :data="tweets" v-loading="loading">
        <el-table-column prop="text" label="æ¨æ–‡å†…å®¹" width="400" show-overflow-tooltip />
        <el-table-column prop="username" label="ç”¨æˆ·" width="120" />
        <el-table-column prop="created_at" label="å‘å¸ƒæ—¶é—´" width="180" />
        <el-table-column prop="like_count" label="ç‚¹èµ" width="80" />
        <el-table-column prop="retweet_count" label="è½¬å‘" width="80" />
        <el-table-column label="æ“ä½œ" width="120">
          <template #default="scope">
            <el-button size="small" @click="viewDetail(scope.row)">è¯¦æƒ…</el-button>
          </template>
        </el-table-column>
      </el-table>

      <el-pagination
        v-model:current-page="pagination.page"
        v-model:page-size="pagination.size"
        :total="pagination.total"
        layout="total, sizes, prev, pager, next, jumper"
        @size-change="handleSizeChange"
        @current-change="handleCurrentChange"
      />
    </el-card>
  </div>
</template>
```

#### ğŸ”§ **ç³»ç»Ÿç®¡ç†é¡µé¢**

**è´¦å·ç®¡ç†ç•Œé¢**
```html
<!-- è´¦å·ç®¡ç†é¡µé¢ -->
<template>
  <div class="account-management">
    <el-card>
      <template #header>
        <div class="card-header">
          <span>X.comè´¦å·ç®¡ç†</span>
          <el-button type="primary" @click="addAccount">æ·»åŠ è´¦å·</el-button>
        </div>
      </template>

      <!-- è´¦å·çŠ¶æ€ç»Ÿè®¡ -->
      <el-row :gutter="20" class="stats-row">
        <el-col :span="6">
          <el-statistic title="æ€»è´¦å·æ•°" :value="accountStats.total" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="å¥åº·è´¦å·" :value="accountStats.healthy" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="æš‚åœè´¦å·" :value="accountStats.suspended" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="é”™è¯¯è´¦å·" :value="accountStats.error" />
        </el-col>
      </el-row>

      <!-- è´¦å·åˆ—è¡¨ -->
      <el-table :data="accounts" v-loading="loading">
        <el-table-column prop="username" label="ç”¨æˆ·å" width="150" />
        <el-table-column prop="status" label="çŠ¶æ€" width="100">
          <template #default="scope">
            <el-tag :type="getStatusType(scope.row.status)">
              {{ getStatusText(scope.row.status) }}
            </el-tag>
          </template>
        </el-table-column>
        <el-table-column prop="last_used" label="æœ€åä½¿ç”¨" width="180" />
        <el-table-column prop="success_count" label="æˆåŠŸæ¬¡æ•°" width="100" />
        <el-table-column prop="error_count" label="é”™è¯¯æ¬¡æ•°" width="100" />
        <el-table-column prop="health_score" label="å¥åº·åˆ†æ•°" width="100">
          <template #default="scope">
            <el-progress :percentage="scope.row.health_score" :color="getHealthColor(scope.row.health_score)" />
          </template>
        </el-table-column>
        <el-table-column label="æ“ä½œ" width="200">
          <template #default="scope">
            <el-button size="small" @click="testAccount(scope.row)">æµ‹è¯•</el-button>
            <el-button size="small" @click="refreshCookies(scope.row)">åˆ·æ–°</el-button>
            <el-button size="small" type="danger" @click="deleteAccount(scope.row)">åˆ é™¤</el-button>
          </template>
        </el-table-column>
      </el-table>
    </el-card>
  </div>
</template>
```

**ä»£ç†IPç®¡ç†ç•Œé¢**
```html
<!-- ä»£ç†ç®¡ç†é¡µé¢ -->
<template>
  <div class="proxy-management">
    <el-card>
      <template #header>
        <div class="card-header">
          <span>ä»£ç†IPæ± ç®¡ç†</span>
          <el-button type="primary" @click="addProxy">æ·»åŠ ä»£ç†</el-button>
        </div>
      </template>

      <!-- ä»£ç†çŠ¶æ€ç»Ÿè®¡ -->
      <el-row :gutter="20" class="stats-row">
        <el-col :span="6">
          <el-statistic title="æ€»ä»£ç†æ•°" :value="proxyStats.total" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="å¯ç”¨ä»£ç†" :value="proxyStats.available" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="å¹³å‡å»¶è¿Ÿ" :value="proxyStats.avgLatency" suffix="ms" />
        </el-col>
        <el-col :span="6">
          <el-statistic title="æˆåŠŸç‡" :value="proxyStats.successRate" suffix="%" />
        </el-col>
      </el-row>

      <!-- ä»£ç†åˆ—è¡¨ -->
      <el-table :data="proxies" v-loading="loading">
        <el-table-column prop="host" label="ä¸»æœº" width="150" />
        <el-table-column prop="port" label="ç«¯å£" width="80" />
        <el-table-column prop="type" label="ç±»å‹" width="100" />
        <el-table-column prop="location" label="ä½ç½®" width="120" />
        <el-table-column prop="status" label="çŠ¶æ€" width="100">
          <template #default="scope">
            <el-tag :type="getProxyStatusType(scope.row.status)">
              {{ scope.row.status }}
            </el-tag>
          </template>
        </el-table-column>
        <el-table-column prop="latency" label="å»¶è¿Ÿ" width="80" />
        <el-table-column prop="success_rate" label="æˆåŠŸç‡" width="100" />
        <el-table-column label="æ“ä½œ" width="200">
          <template #default="scope">
            <el-button size="small" @click="testProxy(scope.row)">æµ‹è¯•</el-button>
            <el-button size="small" type="warning" @click="toggleProxy(scope.row)">
              {{ scope.row.status === 'active' ? 'ç¦ç”¨' : 'å¯ç”¨' }}
            </el-button>
            <el-button size="small" type="danger" @click="deleteProxy(scope.row)">åˆ é™¤</el-button>
          </template>
        </el-table-column>
      </el-table>
    </el-card>
  </div>
</template>
```

#### ğŸ“ˆ **ç»Ÿè®¡åˆ†æé¡µé¢**

**æ•°æ®é‡‡é›†ç»Ÿè®¡**
```html
<!-- ç»Ÿè®¡åˆ†æé¡µé¢ -->
<template>
  <div class="analytics-dashboard">
    <!-- å…³é”®æŒ‡æ ‡å¡ç‰‡ -->
    <el-row :gutter="20" class="metrics-row">
      <el-col :span="6">
        <el-card class="metric-card">
          <el-statistic title="ä»Šæ—¥é‡‡é›†" :value="todayStats.collected" />
          <div class="metric-trend">
            <span :class="todayStats.trend > 0 ? 'trend-up' : 'trend-down'">
              {{ todayStats.trend > 0 ? 'â†—' : 'â†˜' }} {{ Math.abs(todayStats.trend) }}%
            </span>
          </div>
        </el-card>
      </el-col>
      <el-col :span="6">
        <el-card class="metric-card">
          <el-statistic title="æˆåŠŸç‡" :value="todayStats.successRate" suffix="%" />
        </el-card>
      </el-col>
      <el-col :span="6">
        <el-card class="metric-card">
          <el-statistic title="æ´»è·ƒä»»åŠ¡" :value="todayStats.activeTasks" />
        </el-card>
      </el-col>
      <el-col :span="6">
        <el-card class="metric-card">
          <el-statistic title="æ•°æ®æ€»é‡" :value="todayStats.totalData" />
        </el-card>
      </el-col>
    </el-row>

    <!-- å›¾è¡¨åŒºåŸŸ -->
    <el-row :gutter="20">
      <el-col :span="12">
        <el-card title="é‡‡é›†è¶‹åŠ¿">
          <div ref="collectionChart" style="height: 300px;"></div>
        </el-card>
      </el-col>
      <el-col :span="12">
        <el-card title="è´¦å·ä½¿ç”¨åˆ†å¸ƒ">
          <div ref="accountChart" style="height: 300px;"></div>
        </el-card>
      </el-col>
    </el-row>

    <el-row :gutter="20">
      <el-col :span="24">
        <el-card title="å®æ—¶ä»»åŠ¡ç›‘æ§">
          <el-table :data="realtimeTasks" v-loading="loading">
            <el-table-column prop="task_id" label="ä»»åŠ¡ID" width="200" />
            <el-table-column prop="type" label="ç±»å‹" width="100" />
            <el-table-column prop="keyword" label="å…³é”®è¯" width="150" />
            <el-table-column prop="status" label="çŠ¶æ€" width="100">
              <template #default="scope">
                <el-tag :type="getTaskStatusType(scope.row.status)">
                  {{ scope.row.status }}
                </el-tag>
              </template>
            </el-table-column>
            <el-table-column prop="progress" label="è¿›åº¦" width="150">
              <template #default="scope">
                <el-progress :percentage="scope.row.progress" />
              </template>
            </el-table-column>
            <el-table-column prop="created_at" label="åˆ›å»ºæ—¶é—´" width="180" />
            <el-table-column label="æ“ä½œ" width="120">
              <template #default="scope">
                <el-button size="small" @click="viewTaskDetail(scope.row)">è¯¦æƒ…</el-button>
                <el-button size="small" type="danger" @click="cancelTask(scope.row)">å–æ¶ˆ</el-button>
              </template>
            </el-table-column>
          </el-table>
        </el-card>
      </el-col>
    </el-row>
  </div>
</template>
```

## æ•°æ®åº“ç»“æ„è®¾è®¡

### MongoDBé›†åˆè®¾è®¡

åŸºäºæ–‡æ¡£å‹æ•°æ®åº“çš„ç‰¹ç‚¹ï¼Œè®¾è®¡çµæ´»ä¸”é«˜æ•ˆçš„æ•°æ®ç»“æ„ã€‚

#### ğŸ“Š **æ ¸å¿ƒæ•°æ®é›†åˆ**

```javascript
// æ¨æ–‡é›†åˆ (tweets) - å®Œå…¨æŒ‰ç…§ç”²æ–¹éœ€æ±‚å­—æ®µè®¾è®¡
{
  "_id": ObjectId("..."),

  // åŸºç¡€ä¿¡æ¯ (ç”²æ–¹å­—æ®µ0)
  "post_url": "https://x.com/0xairdropfarmer/status/1940603123074228282",
  "post_id": "1940603123074228282",  // ä»URLä¸­æå–çš„ID
  "post_type": "text",  // text/image/video/retweet/quote

  // ä½œè€…ä¿¡æ¯ (ç”²æ–¹å­—æ®µ1-3)
  "author_avatar": "https://pbs.twimg.com/profile_images/...",
  "author_name": "ç¤ºä¾‹ç”¨æˆ·",
  "author_handle": "@example_user",

  // æ—¶é—´ä¿¡æ¯ (ç”²æ–¹å­—æ®µ4)
  "post_time": ISODate("2024-01-01T12:00:00Z"),

  // å†…å®¹ä¿¡æ¯ (ç”²æ–¹å­—æ®µ5)
  "post_content": "æ¨æ–‡å†…å®¹...",

  // åª’ä½“èµ„æº (ç”²æ–¹å­—æ®µ6) - æ•°ç»„å½¢å¼
  "post_images": [
    {
      "original_url": "https://pbs.twimg.com/media/...",
      "oss_url": "https://oss.example.com/images/2024/01/01/img_001.jpg",
      "type": "image",  // image/video_cover
      "width": 1200,
      "height": 800
    }
  ],

  // äº’åŠ¨æ•°æ® (ç”²æ–¹å­—æ®µ7-10, 17)
  "comment_count": 25,
  "retweet_count": 50,
  "like_count": 100,
  "view_count": 5000,
  "bookmark_count": 15,

  // è½¬å‘å¸–ç‰¹æ®Šå­—æ®µ (ç”²æ–¹å­—æ®µ11-16, 18-19)
  "is_retweet": false,
  "is_quote": false,
  "original_post_id": null,
  "original_author_avatar": null,
  "original_author_name": null,
  "original_author_handle": null,
  "original_post_time": null,
  "original_post_content": null,
  "original_post_images": [],

  // é“¾æ¥ä¿¡æ¯ (ç”²æ–¹å­—æ®µ20)
  "post_links": [
    {
      "url": "https://t.co/abc123",
      "expanded_url": "https://example.com/article",
      "display_url": "example.com/article"
    }
  ],

  // OSSæ–‡ä»¶åœ°å€ (ç”²æ–¹è¦æ±‚)
  "oss_file_path": "data/2024/01/01/task_20240101_001_batch_001.json",

  // ç”²æ–¹è¦æ±‚çš„Postå…³ç³»
  "parent_post_id": null,
  "parent_comment_id": null,

  // åŸå§‹æ•°æ® (ç”²æ–¹è¦æ±‚ä¿ç•™TwitteråŸå§‹æ•°æ®)
  "raw_data": {
    "twitter_api_response": "åŸå§‹APIå“åº”æ•°æ®...",
    "collection_method": "twscrape",
    "raw_json": "å®Œæ•´çš„åŸå§‹JSONæ•°æ®"
  },

  // ç³»ç»Ÿå…ƒæ•°æ®
  "metadata": {
    "collected_at": ISODate("2024-01-01T12:05:00Z"),
    "search_keyword": "äººå·¥æ™ºèƒ½",  // é‡‡é›†å…³é”®è¯
    "task_id": "task_20240101_001",
    "source_account": "account_001",
    "source_proxy": "proxy_001",
    "batch_id": "batch_001",
    "collection_method": "search"  // search/timeline/profile
  }
}

// ç”¨æˆ·é›†åˆ (users)
{
  "_id": ObjectId("..."),
  "user_id": "987654321",
  "username": "example_user",
  "display_name": "ç¤ºä¾‹ç”¨æˆ·",
  "description": "ç”¨æˆ·ç®€ä»‹...",
  "profile": {
    "verified": false,
    "protected": false,
    "location": "åŒ—äº¬",
    "website": "https://example.com",
    "profile_image_url": "https://pbs.twimg.com/profile_images/...",
    "profile_banner_url": "https://pbs.twimg.com/profile_banners/..."
  },
  "metrics": {
    "followers_count": 1000,
    "following_count": 500,
    "tweet_count": 2000,
    "listed_count": 10
  },
  "metadata": {
    "created_at": ISODate("2020-01-01T00:00:00Z"),
    "collected_at": ISODate("2024-01-01T12:00:00Z"),
    "last_updated": ISODate("2024-01-01T12:00:00Z")
  },
  "analysis": {
    "activity_score": 0.7,
    "influence_score": 0.5,
    "topics": ["technology", "ai"],
    "sentiment_trend": "positive"
  }
}

// é‡‡é›†ä»»åŠ¡é›†åˆ (collection_tasks) - æŒ‰ç”²æ–¹éœ€æ±‚è®¾è®¡
{
  "_id": ObjectId("..."),
  "task_id": "task_20240101_001",  // ç”²æ–¹è¦æ±‚çš„ä»»åŠ¡ID

  // ä»»åŠ¡ç±»å‹ (ç”²æ–¹ä¸¤ç§æœç´¢æ¨¡å¼)
  "task_type": "scheduled",  // scheduled(å®šæ—¶æœç´¢) / ondemand(æŒ‰éœ€æœç´¢)

  // ç”²æ–¹ä¸»è¦å‚æ•°
  "parameters": {
    "keyword": "äººå·¥æ™ºèƒ½",  // å…³é”®è¯ (æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€å°è¯­ç§)
    "priority": 1,         // ä¼˜å…ˆçº§ (1-10, 1æœ€é«˜)

    // å®šæ—¶æœç´¢å‚æ•°
    "frequency": "daily",      // æœç´¢é¢‘ç‡ (ä»…å®šæ—¶ä»»åŠ¡)
    "frequency_count": 3,      // æ¯æ—¥æœç´¢æ¬¡æ•° (ä»…å®šæ—¶ä»»åŠ¡)

    // æŒ‰éœ€æœç´¢å‚æ•°
    "required_count": 1000,    // éœ€è¦çš„æ¡æ•° (0=å°½åŠ›é‡‡é›†, ä»…æŒ‰éœ€ä»»åŠ¡)
    "start_time": ISODate("2024-01-01T00:00:00Z"),  // å¼€å§‹æ—¶é—´ (ä»…æŒ‰éœ€ä»»åŠ¡)
    "end_time": ISODate("2024-01-01T23:59:59Z"),    // ç»“æŸæ—¶é—´ (ä»…æŒ‰éœ€ä»»åŠ¡)
  },

  // ä»»åŠ¡çŠ¶æ€
  "status": "completed",  // pending, running, completed, failed, cancelled

  // æ‰§è¡Œè¿›åº¦
  "progress": {
    "target_count": 1000,      // ç›®æ ‡æ•°é‡
    "collected_count": 856,    // å·²é‡‡é›†æ•°é‡
    "failed_count": 12,        // å¤±è´¥æ•°é‡
    "percentage": 85.6,        // å®Œæˆç™¾åˆ†æ¯”
    "current_batch": 9,        // å½“å‰æ‰¹æ¬¡
    "total_batches": 10        // æ€»æ‰¹æ¬¡æ•°
  },

  // èµ„æºåˆ†é…
  "resources": {
    "assigned_accounts": ["account_001", "account_002"],
    "used_proxies": ["proxy_001", "proxy_002"],
    "worker_id": "worker_001"
  },

  // æ—¶é—´ä¿¡æ¯
  "timing": {
    "created_at": ISODate("2024-01-01T10:00:00Z"),
    "started_at": ISODate("2024-01-01T10:01:00Z"),
    "completed_at": ISODate("2024-01-01T12:00:00Z"),
    "duration_seconds": 7140,
    "next_execution": ISODate("2024-01-02T10:00:00Z")  // ä»…å®šæ—¶ä»»åŠ¡
  },

  // æ‰§è¡Œç»“æœ
  "results": {
    "posts_collected": 856,
    "unique_posts": 834,       // å»é‡åæ•°é‡
    "duplicate_posts": 22,     // é‡å¤æ•°é‡
    "images_downloaded": 245,  // ä¸‹è½½çš„å›¾ç‰‡æ•°
    "oss_files_created": 9,    // åˆ›å»ºçš„OSSæ–‡ä»¶æ•°
    "data_reported": true,     // æ˜¯å¦å·²ä¸ŠæŠ¥ç»™ç”²æ–¹
    "errors": [
      {
        "type": "rate_limit",
        "count": 5,
        "last_occurrence": ISODate("2024-01-01T11:30:00Z")
      }
    ]
  },

  // ç”²æ–¹ä¸ŠæŠ¥ä¿¡æ¯
  "reporting": {
    "batches_reported": [
      {
        "batch_id": "batch_001",
        "oss_file_path": "data/2024/01/01/task_20240101_001_batch_001.json",
        "post_count": 100,
        "reported_at": ISODate("2024-01-01T11:00:00Z"),
        "report_status": "success"
      }
    ],
    "total_reported": 856,
    "last_report_at": ISODate("2024-01-01T12:00:00Z")
  }
}

// è´¦å·ç®¡ç†é›†åˆ (accounts)
{
  "_id": ObjectId("..."),
  "account_id": "account_001",
  "username": "scraper_account_1",
  "email": "account1@example.com",
  "status": "active",  // active, suspended, error, maintenance
  "health": {
    "score": 0.85,
    "last_check": ISODate("2024-01-01T12:00:00Z"),
    "consecutive_errors": 0,
    "total_requests": 10000,
    "successful_requests": 9500,
    "success_rate": 0.95
  },
  "usage": {
    "daily_limit": 1000,
    "daily_used": 234,
    "last_used": ISODate("2024-01-01T11:45:00Z"),
    "cooldown_until": null
  },
  "authentication": {
    "cookies_updated": ISODate("2024-01-01T08:00:00Z"),
    "cookies_expires": ISODate("2024-01-08T08:00:00Z"),
    "login_method": "playwright_auto"
  },
  "metadata": {
    "created_at": ISODate("2024-01-01T00:00:00Z"),
    "last_maintenance": ISODate("2024-01-01T08:00:00Z"),
    "notes": "ä¸»è¦ç”¨äºæŠ€æœ¯ç±»æ¨æ–‡é‡‡é›†"
  }
}

// ä»£ç†ç®¡ç†é›†åˆ (proxies)
{
  "_id": ObjectId("..."),
  "proxy_id": "proxy_001",
  "config": {
    "host": "proxy.example.com",
    "port": 8080,
    "protocol": "http",  // http, https, socks5
    "username": "proxy_user",
    "password": "proxy_pass",
    "location": "US-East"
  },
  "status": "active",  // active, inactive, error
  "performance": {
    "latency_ms": 150,
    "success_rate": 0.92,
    "total_requests": 5000,
    "failed_requests": 400,
    "last_test": ISODate("2024-01-01T12:00:00Z")
  },
  "usage": {
    "concurrent_limit": 10,
    "current_usage": 3,
    "daily_limit": 10000,
    "daily_used": 2340
  },
  "metadata": {
    "provider": "ProxyProvider Inc",
    "cost_per_gb": 0.1,
    "created_at": ISODate("2024-01-01T00:00:00Z"),
    "expires_at": ISODate("2024-02-01T00:00:00Z")
  }
}
```

#### ğŸ” **ç´¢å¼•è®¾è®¡**

```javascript
// æ¨æ–‡é›†åˆç´¢å¼•
db.tweets.createIndex({ "tweet_id": 1 }, { unique: true })
db.tweets.createIndex({ "metadata.created_at": -1 })
db.tweets.createIndex({ "metadata.search_keyword": 1 })
db.tweets.createIndex({ "user.username": 1 })
db.tweets.createIndex({ "content.hashtags": 1 })
db.tweets.createIndex({ "metadata.collected_at": -1 })

// å¤åˆç´¢å¼•ç”¨äºå¤æ‚æŸ¥è¯¢
db.tweets.createIndex({
  "metadata.search_keyword": 1,
  "metadata.created_at": -1
})
db.tweets.createIndex({
  "user.username": 1,
  "metadata.created_at": -1
})

// ç”¨æˆ·é›†åˆç´¢å¼•
db.users.createIndex({ "user_id": 1 }, { unique: true })
db.users.createIndex({ "username": 1 }, { unique: true })
db.users.createIndex({ "metrics.followers_count": -1 })
db.users.createIndex({ "metadata.collected_at": -1 })

// ä»»åŠ¡é›†åˆç´¢å¼•
db.collection_tasks.createIndex({ "task_id": 1 }, { unique: true })
db.collection_tasks.createIndex({ "status": 1 })
db.collection_tasks.createIndex({ "timing.created_at": -1 })
db.collection_tasks.createIndex({ "type": 1, "status": 1 })

// è´¦å·é›†åˆç´¢å¼•
db.accounts.createIndex({ "account_id": 1 }, { unique: true })
db.accounts.createIndex({ "username": 1 }, { unique: true })
db.accounts.createIndex({ "status": 1 })
db.accounts.createIndex({ "health.score": -1 })

// ä»£ç†é›†åˆç´¢å¼•
db.proxies.createIndex({ "proxy_id": 1 }, { unique: true })
db.proxies.createIndex({ "status": 1 })
db.proxies.createIndex({ "performance.success_rate": -1 })
```

## ä¸šåŠ¡APIæ¥å£è®¾è®¡

### ç”²ä¹™åŒæ–¹æ¥å£è§„èŒƒ

åŸºäºç”²æ–¹éœ€æ±‚æ–‡æ¡£ï¼Œè®¾è®¡æ ‡å‡†åŒ–çš„APIæ¥å£ä½“ç³»ï¼Œå®ç°ä»»åŠ¡ä¸‹å‘ã€æ•°æ®ä¸ŠæŠ¥å’Œç»Ÿè®¡æŸ¥è¯¢ã€‚

#### ğŸ”„ **æ¥å£äº¤äº’æµç¨‹**

```text
ç”²æ–¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¹™æ–¹(XGetç³»ç»Ÿ)
  â”‚                                                      â”‚
  â”œâ”€ 1. æœç´¢ä»»åŠ¡ä¸‹å‘æ¥å£1 (å®šæ—¶æœç´¢) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚
  â”œâ”€ 2. æœç´¢ä»»åŠ¡ä¸‹å‘æ¥å£2 (æŒ‰éœ€æœç´¢) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚
  â”‚                                                      â”‚
  â”‚                                                      â”œâ”€ 3. æ‰§è¡Œæœç´¢é‡‡é›†
  â”‚                                                      â”œâ”€ 4. æ•°æ®è§£æå¤„ç†
  â”‚                                                      â”œâ”€ 5. èµ„æºä¸‹è½½åˆ°OSS
  â”‚                                                      â”‚
  â”œâ”€ 6. æœç´¢ç»“æœä¸ŠæŠ¥æ¥å£ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
  â”œâ”€ 7. æ•°æ®ç»Ÿè®¡æŸ¥è¯¢æ¥å£ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
```

#### ğŸ“¤ **ä¹™æ–¹æä¾›çš„æ¥å£ (ç”²æ–¹è°ƒç”¨)**

##### **1. å®šæ—¶æœç´¢ä»»åŠ¡ä¸‹å‘æ¥å£**
```python
# POST /api/v1/tasks/scheduled
{
    "keyword": "äººå·¥æ™ºèƒ½",           # å…³é”®è¯ (æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€å°è¯­ç§)
    "priority": 1,                  # ä¼˜å…ˆçº§ (1-10, 1æœ€é«˜)
    "frequency": "daily",           # æœç´¢é¢‘ç‡ (daily/hourly/weekly)
    "frequency_count": 3,           # æ¯æ—¥æœç´¢æ¬¡æ•°
    "task_id": "task_20240101_001"  # ä»»åŠ¡ID
}

# å“åº”
{
    "status": "success",
    "task_id": "task_20240101_001",
    "message": "å®šæ—¶æœç´¢ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
    "next_execution": "2024-01-01T08:00:00Z"
}
```

##### **2. æŒ‰éœ€æœç´¢ä»»åŠ¡ä¸‹å‘æ¥å£**
```python
# POST /api/v1/tasks/ondemand
{
    "keyword": "ChatGPT",           # å…³é”®è¯
    "priority": 2,                  # ä¼˜å…ˆçº§
    "required_count": 1000,         # éœ€è¦çš„æ¡æ•° (0=å°½åŠ›é‡‡é›†)
    "start_time": "2024-01-01T00:00:00Z",  # å¼€å§‹æ—¶é—´
    "end_time": "2024-01-01T23:59:59Z",    # ç»“æŸæ—¶é—´
    "task_id": "task_20240101_002"  # ä»»åŠ¡ID
}

# å“åº”
{
    "status": "success",
    "task_id": "task_20240101_002",
    "message": "æŒ‰éœ€æœç´¢ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
    "estimated_completion": "2024-01-01T12:00:00Z"
}
```

##### **3. ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£**
```python
# GET /api/v1/tasks/{task_id}/status
{
    "task_id": "task_20240101_001",
    "status": "running",            # pending/running/completed/failed
    "progress": {
        "collected": 856,           # å·²é‡‡é›†æ•°é‡
        "target": 1000,            # ç›®æ ‡æ•°é‡
        "percentage": 85.6         # å®Œæˆç™¾åˆ†æ¯”
    },
    "created_at": "2024-01-01T08:00:00Z",
    "updated_at": "2024-01-01T10:30:00Z"
}
```

#### ğŸ“¥ **ç”²æ–¹æä¾›çš„æ¥å£ (ä¹™æ–¹è°ƒç”¨)**

##### **4. æœç´¢ç»“æœä¸ŠæŠ¥æ¥å£**
```python
# POST {ç”²æ–¹æä¾›çš„ä¸ŠæŠ¥URL}
{
    "post_id": "1940603123074228282",    # å¸–å­ID
    "keyword": "äººå·¥æ™ºèƒ½",                # å…³é”®è¯
    "task_id": "task_20240101_001",      # ä»»åŠ¡ID
    "oss_file_path": "data/2024/01/01/task_20240101_001_batch_001.json",
    "batch_size": 100,                   # æœ¬æ‰¹æ¬¡æ•°æ®é‡
    "total_collected": 856,              # ç´¯è®¡é‡‡é›†é‡
    "completion_status": "partial"       # partial/completed
}

# ç”²æ–¹å“åº”
{
    "status": "received",
    "message": "æ•°æ®æ¥æ”¶æˆåŠŸ",
    "next_batch_allowed": true
}
```

### æ•°æ®æ ¼å¼è§„èŒƒ

#### ğŸ“Š **æ ‡å‡†æ•°æ®å­—æ®µå®šä¹‰**

æ ¹æ®ç”²æ–¹éœ€æ±‚ï¼Œå®šä¹‰å®Œæ•´çš„æ•°æ®å­—æ®µç»“æ„ï¼š

```python
# æ ‡å‡†å¸–å­æ•°æ®ç»“æ„
{
    # åŸºç¡€ä¿¡æ¯ (å­—æ®µ0)
    "post_url": "https://x.com/0xairdropfarmer/status/1940603123074228282",
    "post_id": "1940603123074228282",
    "post_type": "text",  # text/image/video/retweet/quote

    # ä½œè€…ä¿¡æ¯ (å­—æ®µ1-3)
    "author_avatar": "https://pbs.twimg.com/profile_images/...",
    "author_name": "ç¤ºä¾‹ç”¨æˆ·",
    "author_handle": "@example_user",

    # æ—¶é—´ä¿¡æ¯ (å­—æ®µ4)
    "post_time": "2024-01-01T12:00:00Z",

    # å†…å®¹ä¿¡æ¯ (å­—æ®µ5)
    "post_content": "è¿™æ˜¯ä¸€æ¡ç¤ºä¾‹æ¨æ–‡å†…å®¹...",

    # åª’ä½“èµ„æº (å­—æ®µ6)
    "post_images": [
        {
            "original_url": "https://pbs.twimg.com/media/...",
            "oss_url": "https://oss.example.com/images/2024/01/01/img_001.jpg",
            "type": "image",
            "width": 1200,
            "height": 800
        }
    ],

    # äº’åŠ¨æ•°æ® (å­—æ®µ7-10)
    "comment_count": 25,
    "retweet_count": 50,
    "like_count": 100,
    "view_count": 5000,
    "bookmark_count": 15,  # å­—æ®µ17

    # è½¬å‘å¸–ç‰¹æ®Šå­—æ®µ (å­—æ®µ11-16)
    "is_retweet": false,        # å­—æ®µ18
    "is_quote": false,          # å­—æ®µ19
    "original_post_id": null,
    "original_author_avatar": null,
    "original_author_name": null,
    "original_author_handle": null,
    "original_post_time": null,
    "original_post_content": null,
    "original_post_images": [],

    # é“¾æ¥ä¿¡æ¯ (å­—æ®µ20)
    "post_links": [
        {
            "url": "https://t.co/abc123",
            "expanded_url": "https://example.com/article",
            "display_url": "example.com/article"
        }
    ],

    # ç³»ç»Ÿå­—æ®µ
    "oss_file_path": "data/2024/01/01/post_1940603123074228282.json",
    "collected_at": "2024-01-01T12:05:00Z",
    "keyword": "äººå·¥æ™ºèƒ½",
    "task_id": "task_20240101_001",

    # åŸå§‹æ•°æ® (ç”²æ–¹è¦æ±‚)
    "raw_data": {
        "twitter_api_response": "åŸå§‹APIå“åº”æ•°æ®..."
    },

    # å…³ç³»å­—æ®µ (ç”²æ–¹è¦æ±‚çš„Postå…³ç³»)
    "parent_post_id": null,
    "parent_comment_id": null
}
```

### OSSèµ„æºç®¡ç†

#### ğŸ“ **æ–‡ä»¶å­˜å‚¨è§„èŒƒ**

æ ¹æ®ç”²æ–¹è¦æ±‚ï¼Œå®ç°å›¾ç‰‡å’Œæ•°æ®æ–‡ä»¶çš„OSSå­˜å‚¨ç®¡ç†ï¼š

```python
# OSSå­˜å‚¨ç»“æ„
oss_bucket/
â”œâ”€â”€ images/                    # å›¾ç‰‡èµ„æº
â”‚   â”œâ”€â”€ 2024/01/01/           # æŒ‰æ—¥æœŸåˆ†ç›®å½•
â”‚   â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”‚   â”œâ”€â”€ img_002.png
â”‚   â”‚   â””â”€â”€ video_001_cover.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ videos/                    # è§†é¢‘å°é¢
â”‚   â”œâ”€â”€ 2024/01/01/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                      # JSONæ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ 2024/01/01/
â”‚   â”‚   â”œâ”€â”€ task_20240101_001_batch_001.json
â”‚   â”‚   â”œâ”€â”€ task_20240101_001_batch_002.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â””â”€â”€ raw_data/                  # åŸå§‹æ•°æ®å¤‡ä»½
    â”œâ”€â”€ 2024/01/01/
    â””â”€â”€ ...
```

#### ğŸ”§ **OSSä¸Šä¼ æœåŠ¡**

```python
# services/oss_service.py
import asyncio
import aiofiles
from datetime import datetime
from typing import List, Dict, Optional
import hashlib
import os

class OSSService:
    """OSSèµ„æºç®¡ç†æœåŠ¡"""

    def __init__(self, oss_config: Dict):
        self.bucket_name = oss_config["bucket_name"]
        self.endpoint = oss_config["endpoint"]
        self.access_key = oss_config["access_key"]
        self.secret_key = oss_config["secret_key"]
        self.base_url = f"https://{self.bucket_name}.{self.endpoint}"

    async def upload_image(self, image_url: str, task_id: str) -> Dict[str, str]:
        """ä¸Šä¼ å›¾ç‰‡åˆ°OSS"""
        try:
            # ä¸‹è½½å›¾ç‰‡
            image_data = await self._download_media(image_url)

            # ç”ŸæˆOSSè·¯å¾„
            date_path = datetime.now().strftime("%Y/%m/%d")
            file_hash = hashlib.md5(image_data).hexdigest()[:8]
            file_ext = self._get_file_extension(image_url)
            oss_path = f"images/{date_path}/{task_id}_{file_hash}{file_ext}"

            # ä¸Šä¼ åˆ°OSS
            oss_url = await self._upload_to_oss(oss_path, image_data, "image")

            return {
                "original_url": image_url,
                "oss_url": oss_url,
                "oss_path": oss_path,
                "file_size": len(image_data)
            }

        except Exception as e:
            self.logger.error(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {image_url}, é”™è¯¯: {str(e)}")
            return {
                "original_url": image_url,
                "oss_url": None,
                "error": str(e)
            }

    async def upload_data_file(self, data: Dict, task_id: str, batch_id: str) -> str:
        """ä¸Šä¼ JSONæ•°æ®æ–‡ä»¶åˆ°OSS"""
        try:
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            date_path = datetime.now().strftime("%Y/%m/%d")
            filename = f"{task_id}_batch_{batch_id}.json"
            oss_path = f"data/{date_path}/{filename}"

            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            import json
            json_data = json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8')

            # ä¸Šä¼ åˆ°OSS
            oss_url = await self._upload_to_oss(oss_path, json_data, "application/json")

            return oss_url

        except Exception as e:
            self.logger.error(f"æ•°æ®æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")
            raise

    async def batch_upload_images(self, image_urls: List[str], task_id: str) -> List[Dict]:
        """æ‰¹é‡ä¸Šä¼ å›¾ç‰‡"""
        tasks = [self.upload_image(url, task_id) for url in image_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [
            result if not isinstance(result, Exception) else {"error": str(result)}
            for result in results
        ]
```

### æ•°æ®ç»Ÿè®¡æœåŠ¡

#### ğŸ“Š **æœˆåº¦å»é‡ç»Ÿè®¡**

æ ¹æ®ç”²æ–¹è¦æ±‚ï¼Œå®ç°æŒ‰å…³é”®è¯çš„æœˆåº¦å»é‡æ•°æ®ç»Ÿè®¡ï¼š

```python
# services/statistics_service.py
from datetime import datetime, timedelta
from typing import Dict, List
import calendar

class StatisticsService:
    """æ•°æ®ç»Ÿè®¡æœåŠ¡"""

    def __init__(self, db_client):
        self.db = db_client.xget
        self.logger = logging.getLogger(__name__)

    async def get_monthly_stats_by_keyword(
        self,
        keyword: str,
        year: int,
        month: int
    ) -> Dict[str, int]:
        """æŒ‰å…³é”®è¯è·å–æœˆåº¦å»é‡ç»Ÿè®¡"""
        try:
            # è®¡ç®—æœˆä»½æ—¶é—´èŒƒå›´
            start_date = datetime(year, month, 1)
            if month == 12:
                end_date = datetime(year + 1, 1, 1)
            else:
                end_date = datetime(year, month + 1, 1)

            # èšåˆæŸ¥è¯¢ - æŒ‰å¸–å­IDå»é‡
            pipeline = [
                {
                    "$match": {
                        "metadata.search_keyword": keyword,
                        "metadata.collected_at": {
                            "$gte": start_date,
                            "$lt": end_date
                        }
                    }
                },
                {
                    "$group": {
                        "_id": "$tweet_id",  # æŒ‰å¸–å­IDå»é‡
                        "first_collected": {"$min": "$metadata.collected_at"},
                        "keyword": {"$first": "$metadata.search_keyword"}
                    }
                },
                {
                    "$group": {
                        "_id": None,
                        "unique_posts": {"$sum": 1},
                        "keywords": {"$addToSet": "$keyword"}
                    }
                }
            ]

            result = await self.db.tweets.aggregate(pipeline).to_list(length=1)

            if result:
                return {
                    "keyword": keyword,
                    "year": year,
                    "month": month,
                    "unique_posts_count": result[0]["unique_posts"],
                    "period": f"{year}-{month:02d}",
                    "calculated_at": datetime.utcnow().isoformat()
                }
            else:
                return {
                    "keyword": keyword,
                    "year": year,
                    "month": month,
                    "unique_posts_count": 0,
                    "period": f"{year}-{month:02d}",
                    "calculated_at": datetime.utcnow().isoformat()
                }

        except Exception as e:
            self.logger.error(f"æœˆåº¦ç»Ÿè®¡è®¡ç®—å¤±è´¥: {keyword}, {year}-{month}, é”™è¯¯: {str(e)}")
            raise

    async def get_all_keywords_monthly_stats(
        self,
        year: int,
        month: int
    ) -> List[Dict]:
        """è·å–æ‰€æœ‰å…³é”®è¯çš„æœˆåº¦ç»Ÿè®¡"""
        try:
            # è·å–è¯¥æœˆä»½æ‰€æœ‰å…³é”®è¯
            start_date = datetime(year, month, 1)
            if month == 12:
                end_date = datetime(year + 1, 1, 1)
            else:
                end_date = datetime(year, month + 1, 1)

            keywords = await self.db.tweets.distinct(
                "metadata.search_keyword",
                {
                    "metadata.collected_at": {
                        "$gte": start_date,
                        "$lt": end_date
                    }
                }
            )

            # ä¸ºæ¯ä¸ªå…³é”®è¯è®¡ç®—ç»Ÿè®¡
            stats = []
            for keyword in keywords:
                keyword_stats = await self.get_monthly_stats_by_keyword(keyword, year, month)
                stats.append(keyword_stats)

            return stats

        except Exception as e:
            self.logger.error(f"å…¨é‡æœˆåº¦ç»Ÿè®¡å¤±è´¥: {year}-{month}, é”™è¯¯: {str(e)}")
            raise

    async def get_task_completion_stats(self, task_id: str) -> Dict:
        """è·å–ä»»åŠ¡å®Œæˆç»Ÿè®¡"""
        try:
            task = await self.db.collection_tasks.find_one({"task_id": task_id})
            if not task:
                return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}

            # ç»Ÿè®¡è¯¥ä»»åŠ¡é‡‡é›†çš„æ•°æ®
            collected_count = await self.db.tweets.count_documents({
                "metadata.task_id": task_id
            })

            # å»é‡ç»Ÿè®¡
            unique_posts = await self.db.tweets.distinct(
                "tweet_id",
                {"metadata.task_id": task_id}
            )

            return {
                "task_id": task_id,
                "total_collected": collected_count,
                "unique_posts": len(unique_posts),
                "duplicate_rate": (collected_count - len(unique_posts)) / collected_count if collected_count > 0 else 0,
                "task_status": task["status"],
                "keyword": task["parameters"].get("keyword"),
                "created_at": task["timing"]["created_at"].isoformat(),
                "completed_at": task["timing"].get("completed_at", {}).isoformat() if task["timing"].get("completed_at") else None
            }

        except Exception as e:
            self.logger.error(f"ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")
            raise
```

#### ğŸ“ˆ **ç»Ÿè®¡APIæ¥å£**

```python
# api/routes/statistics.py
from fastapi import APIRouter, Query, Depends
from typing import List, Optional
from datetime import datetime
from ..services import StatisticsService

router = APIRouter(prefix="/api/v1/statistics", tags=["æ•°æ®ç»Ÿè®¡"])

@router.get("/monthly/{keyword}")
async def get_keyword_monthly_stats(
    keyword: str,
    year: int = Query(..., description="å¹´ä»½"),
    month: int = Query(..., ge=1, le=12, description="æœˆä»½"),
    token: str = Depends(verify_token)
):
    """è·å–å…³é”®è¯æœˆåº¦å»é‡ç»Ÿè®¡"""
    try:
        stats_service = StatisticsService(db_client)
        stats = await stats_service.get_monthly_stats_by_keyword(keyword, year, month)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")

@router.get("/monthly/all")
async def get_all_monthly_stats(
    year: int = Query(..., description="å¹´ä»½"),
    month: int = Query(..., ge=1, le=12, description="æœˆä»½"),
    token: str = Depends(verify_token)
):
    """è·å–æ‰€æœ‰å…³é”®è¯æœˆåº¦ç»Ÿè®¡"""
    try:
        stats_service = StatisticsService(db_client)
        stats = await stats_service.get_all_keywords_monthly_stats(year, month)
        return {
            "period": f"{year}-{month:02d}",
            "total_keywords": len(stats),
            "statistics": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}")
async def get_task_stats(
    task_id: str,
    token: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡å®Œæˆç»Ÿè®¡"""
    try:
        stats_service = StatisticsService(db_client)
        stats = await stats_service.get_task_completion_stats(task_id)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}")
```

## APIæœåŠ¡è®¾è®¡

### RESTful APIæ¥å£

åŸºäºFastAPIæ„å»ºçš„é«˜æ€§èƒ½APIæœåŠ¡ï¼Œæä¾›å®Œæ•´çš„æ•°æ®è®¿é—®å’Œç®¡ç†åŠŸèƒ½ã€‚

#### ğŸš€ **APIæ¶æ„è®¾è®¡**

```python
# api/main.py - FastAPIä¸»åº”ç”¨
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
from datetime import datetime, timedelta

app = FastAPI(
    title="XGet API",
    description="X(Twitter)æ•°æ®é‡‡é›†ç³»ç»ŸAPI",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å®‰å…¨è®¤è¯
security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éªŒè¯APIè®¿é—®ä»¤ç‰Œ"""
    # è¿™é‡Œå®ç°å…·ä½“çš„tokenéªŒè¯é€»è¾‘
    if not credentials.token or credentials.token != "your-api-token":
        raise HTTPException(status_code=401, detail="Invalid authentication token")
    return credentials.token
```

#### ğŸ“Š **æ•°æ®æŸ¥è¯¢API**

```python
# api/routes/data.py - æ•°æ®æŸ¥è¯¢æ¥å£
from fastapi import APIRouter, Query, Depends
from typing import List, Optional
from datetime import datetime
from ..models import TweetResponse, UserResponse, SearchRequest
from ..services import DataService

router = APIRouter(prefix="/api/v1/data", tags=["æ•°æ®æŸ¥è¯¢"])

@router.get("/tweets/search", response_model=List[TweetResponse])
async def search_tweets(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    limit: int = Query(100, ge=1, le=1000, description="è¿”å›æ•°é‡é™åˆ¶"),
    start_date: Optional[datetime] = Query(None, description="å¼€å§‹æ—¶é—´"),
    end_date: Optional[datetime] = Query(None, description="ç»“æŸæ—¶é—´"),
    username: Optional[str] = Query(None, description="æŒ‡å®šç”¨æˆ·å"),
    min_likes: Optional[int] = Query(None, description="æœ€å°ç‚¹èµæ•°"),
    has_media: Optional[bool] = Query(None, description="æ˜¯å¦åŒ…å«åª’ä½“"),
    token: str = Depends(verify_token)
):
    """
    æœç´¢æ¨æ–‡æ•°æ®

    æ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶ï¼š
    - å…³é”®è¯æœç´¢
    - æ—¶é—´èŒƒå›´è¿‡æ»¤
    - ç”¨æˆ·è¿‡æ»¤
    - äº’åŠ¨æ•°è¿‡æ»¤
    - åª’ä½“ç±»å‹è¿‡æ»¤
    """
    try:
        data_service = DataService()
        tweets = await data_service.search_tweets(
            keyword=keyword,
            limit=limit,
            start_date=start_date,
            end_date=end_date,
            username=username,
            min_likes=min_likes,
            has_media=has_media
        )
        return tweets
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")

@router.get("/tweets/{tweet_id}", response_model=TweetResponse)
async def get_tweet_by_id(
    tweet_id: str,
    token: str = Depends(verify_token)
):
    """æ ¹æ®æ¨æ–‡IDè·å–è¯¦ç»†ä¿¡æ¯"""
    try:
        data_service = DataService()
        tweet = await data_service.get_tweet_by_id(tweet_id)
        if not tweet:
            raise HTTPException(status_code=404, detail="æ¨æ–‡ä¸å­˜åœ¨")
        return tweet
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨æ–‡å¤±è´¥: {str(e)}")

@router.get("/users/{username}", response_model=UserResponse)
async def get_user_profile(
    username: str,
    token: str = Depends(verify_token)
):
    """è·å–ç”¨æˆ·èµ„æ–™"""
    try:
        data_service = DataService()
        user = await data_service.get_user_by_username(username)
        if not user:
            raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
        return user
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç”¨æˆ·å¤±è´¥: {str(e)}")

@router.get("/users/{username}/tweets", response_model=List[TweetResponse])
async def get_user_tweets(
    username: str,
    limit: int = Query(100, ge=1, le=1000),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    token: str = Depends(verify_token)
):
    """è·å–ç”¨æˆ·çš„æ¨æ–‡åˆ—è¡¨"""
    try:
        data_service = DataService()
        tweets = await data_service.get_user_tweets(
            username=username,
            limit=limit,
            start_date=start_date,
            end_date=end_date
        )
        return tweets
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç”¨æˆ·æ¨æ–‡å¤±è´¥: {str(e)}")

@router.get("/analytics/trending", response_model=Dict[str, Any])
async def get_trending_topics(
    hours: int = Query(24, ge=1, le=168, description="æ—¶é—´èŒƒå›´(å°æ—¶)"),
    limit: int = Query(20, ge=1, le=100, description="è¿”å›æ•°é‡"),
    token: str = Depends(verify_token)
):
    """è·å–çƒ­é—¨è¯é¢˜å’Œè¶‹åŠ¿"""
    try:
        data_service = DataService()
        trending = await data_service.get_trending_topics(hours=hours, limit=limit)
        return trending
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¶‹åŠ¿å¤±è´¥: {str(e)}")
```

#### ğŸ”§ **ä»»åŠ¡ç®¡ç†API**

```python
# api/routes/tasks.py - ä»»åŠ¡ç®¡ç†æ¥å£
from fastapi import APIRouter, BackgroundTasks, Depends
from typing import List, Optional
from ..models import TaskRequest, TaskResponse, TaskStatus
from ..services import TaskService

router = APIRouter(prefix="/api/v1/tasks", tags=["ä»»åŠ¡ç®¡ç†"])

@router.post("/search", response_model=TaskResponse)
async def create_search_task(
    request: TaskRequest,
    background_tasks: BackgroundTasks,
    token: str = Depends(verify_token)
):
    """
    åˆ›å»ºæœç´¢ä»»åŠ¡

    æ”¯æŒçš„ä»»åŠ¡ç±»å‹ï¼š
    - keyword_search: å…³é”®è¯æœç´¢
    - user_timeline: ç”¨æˆ·æ—¶é—´çº¿
    - user_profile: ç”¨æˆ·èµ„æ–™é‡‡é›†
    """
    try:
        task_service = TaskService()
        task = await task_service.create_search_task(request)

        # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
        background_tasks.add_task(task_service.execute_task, task.task_id)

        return task
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/", response_model=List[TaskResponse])
async def list_tasks(
    status: Optional[TaskStatus] = Query(None, description="ä»»åŠ¡çŠ¶æ€è¿‡æ»¤"),
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    token: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    try:
        task_service = TaskService()
        tasks = await task_service.list_tasks(
            status=status,
            limit=limit,
            offset=offset
        )
        return tasks
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/{task_id}", response_model=TaskResponse)
async def get_task_detail(
    task_id: str,
    token: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡è¯¦æƒ…"""
    try:
        task_service = TaskService()
        task = await task_service.get_task_by_id(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        return task
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.post("/{task_id}/cancel")
async def cancel_task(
    task_id: str,
    token: str = Depends(verify_token)
):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        task_service = TaskService()
        result = await task_service.cancel_task(task_id)
        if not result:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ")
        return {"message": "ä»»åŠ¡å·²å–æ¶ˆ", "task_id": task_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/{task_id}/progress")
async def get_task_progress(
    task_id: str,
    token: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡è¿›åº¦"""
    try:
        task_service = TaskService()
        progress = await task_service.get_task_progress(task_id)
        if not progress:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        return progress
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
```

#### âš™ï¸ **ç³»ç»Ÿç®¡ç†API**

```python
# api/routes/admin.py - ç³»ç»Ÿç®¡ç†æ¥å£
from fastapi import APIRouter, Depends, HTTPException
from typing import List, Dict, Any
from ..models import AccountResponse, ProxyResponse, SystemStats
from ..services import AdminService

router = APIRouter(prefix="/api/v1/admin", tags=["ç³»ç»Ÿç®¡ç†"])

@router.get("/accounts", response_model=List[AccountResponse])
async def list_accounts(
    status: Optional[str] = Query(None, description="è´¦å·çŠ¶æ€è¿‡æ»¤"),
    token: str = Depends(verify_token)
):
    """è·å–è´¦å·åˆ—è¡¨"""
    try:
        admin_service = AdminService()
        accounts = await admin_service.list_accounts(status=status)
        return accounts
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è´¦å·åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/accounts/{account_id}/test")
async def test_account(
    account_id: str,
    token: str = Depends(verify_token)
):
    """æµ‹è¯•è´¦å·å¯ç”¨æ€§"""
    try:
        admin_service = AdminService()
        result = await admin_service.test_account(account_id)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•è´¦å·å¤±è´¥: {str(e)}")

@router.post("/accounts/{account_id}/refresh")
async def refresh_account_cookies(
    account_id: str,
    token: str = Depends(verify_token)
):
    """åˆ·æ–°è´¦å·cookies"""
    try:
        admin_service = AdminService()
        result = await admin_service.refresh_account_cookies(account_id)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°cookieså¤±è´¥: {str(e)}")

@router.get("/proxies", response_model=List[ProxyResponse])
async def list_proxies(
    status: Optional[str] = Query(None, description="ä»£ç†çŠ¶æ€è¿‡æ»¤"),
    token: str = Depends(verify_token)
):
    """è·å–ä»£ç†åˆ—è¡¨"""
    try:
        admin_service = AdminService()
        proxies = await admin_service.list_proxies(status=status)
        return proxies
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»£ç†åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/proxies/{proxy_id}/test")
async def test_proxy(
    proxy_id: str,
    token: str = Depends(verify_token)
):
    """æµ‹è¯•ä»£ç†å¯ç”¨æ€§"""
    try:
        admin_service = AdminService()
        result = await admin_service.test_proxy(proxy_id)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•ä»£ç†å¤±è´¥: {str(e)}")

@router.get("/stats", response_model=SystemStats)
async def get_system_stats(
    token: str = Depends(verify_token)
):
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        admin_service = AdminService()
        stats = await admin_service.get_system_stats()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/health")
async def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ï¼ˆæ— éœ€è®¤è¯ï¼‰"""
    try:
        admin_service = AdminService()
        health = await admin_service.health_check()
        return health
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿä¸å¥åº·: {str(e)}")
```

#### ğŸ“‹ **æ•°æ®æ¨¡å‹å®šä¹‰**

```python
# api/models.py - APIæ•°æ®æ¨¡å‹
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class TaskType(str, Enum):
    KEYWORD_SEARCH = "keyword_search"
    USER_TIMELINE = "user_timeline"
    USER_PROFILE = "user_profile"

class MediaItem(BaseModel):
    type: str = Field(..., description="åª’ä½“ç±»å‹")
    url: str = Field(..., description="åª’ä½“URL")
    width: Optional[int] = Field(None, description="å®½åº¦")
    height: Optional[int] = Field(None, description="é«˜åº¦")

class UserInfo(BaseModel):
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    username: str = Field(..., description="ç”¨æˆ·å")
    display_name: str = Field(..., description="æ˜¾ç¤ºåç§°")
    verified: bool = Field(False, description="æ˜¯å¦è®¤è¯")
    followers_count: int = Field(0, description="ç²‰ä¸æ•°")

class TweetResponse(BaseModel):
    tweet_id: str = Field(..., description="æ¨æ–‡ID")
    text: str = Field(..., description="æ¨æ–‡å†…å®¹")
    user: UserInfo = Field(..., description="ç”¨æˆ·ä¿¡æ¯")
    like_count: int = Field(0, description="ç‚¹èµæ•°")
    retweet_count: int = Field(0, description="è½¬å‘æ•°")
    reply_count: int = Field(0, description="å›å¤æ•°")
    quote_count: int = Field(0, description="å¼•ç”¨æ•°")
    view_count: int = Field(0, description="æŸ¥çœ‹æ•°")
    hashtags: List[str] = Field(default_factory=list, description="è¯é¢˜æ ‡ç­¾")
    mentions: List[str] = Field(default_factory=list, description="æåŠç”¨æˆ·")
    urls: List[str] = Field(default_factory=list, description="é“¾æ¥")
    media: List[MediaItem] = Field(default_factory=list, description="åª’ä½“å†…å®¹")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    collected_at: datetime = Field(..., description="é‡‡é›†æ—¶é—´")

class UserResponse(BaseModel):
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    username: str = Field(..., description="ç”¨æˆ·å")
    display_name: str = Field(..., description="æ˜¾ç¤ºåç§°")
    description: Optional[str] = Field(None, description="ç”¨æˆ·ç®€ä»‹")
    verified: bool = Field(False, description="æ˜¯å¦è®¤è¯")
    protected: bool = Field(False, description="æ˜¯å¦å—ä¿æŠ¤")
    followers_count: int = Field(0, description="ç²‰ä¸æ•°")
    following_count: int = Field(0, description="å…³æ³¨æ•°")
    tweet_count: int = Field(0, description="æ¨æ–‡æ•°")
    location: Optional[str] = Field(None, description="ä½ç½®")
    website: Optional[str] = Field(None, description="ç½‘ç«™")
    profile_image_url: Optional[str] = Field(None, description="å¤´åƒURL")
    created_at: Optional[datetime] = Field(None, description="è´¦å·åˆ›å»ºæ—¶é—´")
    collected_at: datetime = Field(..., description="é‡‡é›†æ—¶é—´")

class TaskRequest(BaseModel):
    type: TaskType = Field(..., description="ä»»åŠ¡ç±»å‹")
    keyword: Optional[str] = Field(None, description="æœç´¢å…³é”®è¯")
    username: Optional[str] = Field(None, description="ç”¨æˆ·å")
    count: int = Field(100, ge=1, le=10000, description="é‡‡é›†æ•°é‡")
    priority: str = Field("normal", description="ä»»åŠ¡ä¼˜å…ˆçº§")
    start_date: Optional[datetime] = Field(None, description="å¼€å§‹æ—¶é—´")
    end_date: Optional[datetime] = Field(None, description="ç»“æŸæ—¶é—´")

class TaskProgress(BaseModel):
    total: int = Field(..., description="æ€»æ•°")
    collected: int = Field(..., description="å·²é‡‡é›†")
    failed: int = Field(..., description="å¤±è´¥æ•°")
    percentage: float = Field(..., description="å®Œæˆç™¾åˆ†æ¯”")

class TaskResponse(BaseModel):
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    type: TaskType = Field(..., description="ä»»åŠ¡ç±»å‹")
    status: TaskStatus = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    parameters: Dict[str, Any] = Field(..., description="ä»»åŠ¡å‚æ•°")
    progress: Optional[TaskProgress] = Field(None, description="ä»»åŠ¡è¿›åº¦")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    started_at: Optional[datetime] = Field(None, description="å¼€å§‹æ—¶é—´")
    completed_at: Optional[datetime] = Field(None, description="å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")

class AccountResponse(BaseModel):
    account_id: str = Field(..., description="è´¦å·ID")
    username: str = Field(..., description="ç”¨æˆ·å")
    status: str = Field(..., description="è´¦å·çŠ¶æ€")
    health_score: float = Field(..., description="å¥åº·åˆ†æ•°")
    success_rate: float = Field(..., description="æˆåŠŸç‡")
    daily_used: int = Field(..., description="ä»Šæ—¥ä½¿ç”¨æ¬¡æ•°")
    daily_limit: int = Field(..., description="æ¯æ—¥é™åˆ¶")
    last_used: Optional[datetime] = Field(None, description="æœ€åä½¿ç”¨æ—¶é—´")

class ProxyResponse(BaseModel):
    proxy_id: str = Field(..., description="ä»£ç†ID")
    host: str = Field(..., description="ä¸»æœºåœ°å€")
    port: int = Field(..., description="ç«¯å£")
    type: str = Field(..., description="ä»£ç†ç±»å‹")
    location: str = Field(..., description="ä½ç½®")
    status: str = Field(..., description="çŠ¶æ€")
    latency_ms: int = Field(..., description="å»¶è¿Ÿ(æ¯«ç§’)")
    success_rate: float = Field(..., description="æˆåŠŸç‡")

class SystemStats(BaseModel):
    total_tweets: int = Field(..., description="æ€»æ¨æ–‡æ•°")
    total_users: int = Field(..., description="æ€»ç”¨æˆ·æ•°")
    active_tasks: int = Field(..., description="æ´»è·ƒä»»åŠ¡æ•°")
    healthy_accounts: int = Field(..., description="å¥åº·è´¦å·æ•°")
    available_proxies: int = Field(..., description="å¯ç”¨ä»£ç†æ•°")
    today_collected: int = Field(..., description="ä»Šæ—¥é‡‡é›†æ•°")
    system_uptime: str = Field(..., description="ç³»ç»Ÿè¿è¡Œæ—¶é—´")
    last_updated: datetime = Field(..., description="æœ€åæ›´æ–°æ—¶é—´")
```

#### ğŸ”§ **æœåŠ¡å±‚å®ç°**

```python
# api/services/data_service.py - æ•°æ®æœåŠ¡
from motor.motor_asyncio import AsyncIOMotorClient
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import logging
from ..models import TweetResponse, UserResponse

class DataService:
    """æ•°æ®æŸ¥è¯¢æœåŠ¡"""

    def __init__(self):
        self.client = AsyncIOMotorClient(MONGODB_URI)
        self.db = self.client.xget
        self.logger = logging.getLogger(__name__)

    async def search_tweets(
        self,
        keyword: str,
        limit: int = 100,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        username: Optional[str] = None,
        min_likes: Optional[int] = None,
        has_media: Optional[bool] = None
    ) -> List[TweetResponse]:
        """æœç´¢æ¨æ–‡"""
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {}

            # å…³é”®è¯æœç´¢
            if keyword:
                query["$or"] = [
                    {"text": {"$regex": keyword, "$options": "i"}},
                    {"content.hashtags": {"$regex": keyword, "$options": "i"}}
                ]

            # æ—¶é—´èŒƒå›´
            if start_date or end_date:
                date_query = {}
                if start_date:
                    date_query["$gte"] = start_date
                if end_date:
                    date_query["$lte"] = end_date
                query["metadata.created_at"] = date_query

            # ç”¨æˆ·è¿‡æ»¤
            if username:
                query["user.username"] = username

            # äº’åŠ¨æ•°è¿‡æ»¤
            if min_likes:
                query["metrics.like_count"] = {"$gte": min_likes}

            # åª’ä½“è¿‡æ»¤
            if has_media is not None:
                if has_media:
                    query["content.media"] = {"$exists": True, "$ne": []}
                else:
                    query["$or"] = [
                        {"content.media": {"$exists": False}},
                        {"content.media": []}
                    ]

            # æ‰§è¡ŒæŸ¥è¯¢
            cursor = self.db.tweets.find(query).sort("metadata.created_at", -1).limit(limit)
            tweets = await cursor.to_list(length=limit)

            # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
            return [self._tweet_to_response(tweet) for tweet in tweets]

        except Exception as e:
            self.logger.error(f"æœç´¢æ¨æ–‡å¤±è´¥: {str(e)}")
            raise

    async def get_tweet_by_id(self, tweet_id: str) -> Optional[TweetResponse]:
        """æ ¹æ®IDè·å–æ¨æ–‡"""
        try:
            tweet = await self.db.tweets.find_one({"tweet_id": tweet_id})
            return self._tweet_to_response(tweet) if tweet else None
        except Exception as e:
            self.logger.error(f"è·å–æ¨æ–‡å¤±è´¥: {str(e)}")
            raise

    async def get_user_by_username(self, username: str) -> Optional[UserResponse]:
        """æ ¹æ®ç”¨æˆ·åè·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            user = await self.db.users.find_one({"username": username})
            return self._user_to_response(user) if user else None
        except Exception as e:
            self.logger.error(f"è·å–ç”¨æˆ·å¤±è´¥: {str(e)}")
            raise

    async def get_trending_topics(self, hours: int = 24, limit: int = 20) -> Dict[str, Any]:
        """è·å–çƒ­é—¨è¯é¢˜"""
        try:
            start_time = datetime.utcnow() - timedelta(hours=hours)

            # èšåˆæŸ¥è¯¢çƒ­é—¨è¯é¢˜
            pipeline = [
                {"$match": {"metadata.created_at": {"$gte": start_time}}},
                {"$unwind": "$content.hashtags"},
                {"$group": {
                    "_id": "$content.hashtags",
                    "count": {"$sum": 1},
                    "total_likes": {"$sum": "$metrics.like_count"},
                    "total_retweets": {"$sum": "$metrics.retweet_count"}
                }},
                {"$sort": {"count": -1}},
                {"$limit": limit}
            ]

            trending = await self.db.tweets.aggregate(pipeline).to_list(length=limit)

            return {
                "time_range_hours": hours,
                "trending_topics": [
                    {
                        "hashtag": item["_id"],
                        "tweet_count": item["count"],
                        "total_likes": item["total_likes"],
                        "total_retweets": item["total_retweets"]
                    }
                    for item in trending
                ]
            }

        except Exception as e:
            self.logger.error(f"è·å–çƒ­é—¨è¯é¢˜å¤±è´¥: {str(e)}")
            raise

    def _tweet_to_response(self, tweet: Dict) -> TweetResponse:
        """è½¬æ¢æ¨æ–‡æ•°æ®ä¸ºå“åº”æ¨¡å‹"""
        return TweetResponse(
            tweet_id=tweet["tweet_id"],
            text=tweet["text"],
            user=UserInfo(
                user_id=tweet["user"]["user_id"],
                username=tweet["user"]["username"],
                display_name=tweet["user"]["display_name"],
                verified=tweet["user"].get("verified", False),
                followers_count=tweet["user"].get("followers_count", 0)
            ),
            like_count=tweet["metrics"].get("like_count", 0),
            retweet_count=tweet["metrics"].get("retweet_count", 0),
            reply_count=tweet["metrics"].get("reply_count", 0),
            quote_count=tweet["metrics"].get("quote_count", 0),
            view_count=tweet["metrics"].get("view_count", 0),
            hashtags=tweet["content"].get("hashtags", []),
            mentions=tweet["content"].get("mentions", []),
            urls=[url["expanded_url"] for url in tweet["content"].get("urls", [])],
            media=[
                MediaItem(
                    type=media["type"],
                    url=media["url"],
                    width=media.get("width"),
                    height=media.get("height")
                )
                for media in tweet["content"].get("media", [])
            ],
            created_at=tweet["metadata"]["created_at"],
            collected_at=tweet["metadata"]["collected_at"]
        )

    def _user_to_response(self, user: Dict) -> UserResponse:
        """è½¬æ¢ç”¨æˆ·æ•°æ®ä¸ºå“åº”æ¨¡å‹"""
        return UserResponse(
            user_id=user["user_id"],
            username=user["username"],
            display_name=user["display_name"],
            description=user.get("description"),
            verified=user["profile"].get("verified", False),
            protected=user["profile"].get("protected", False),
            followers_count=user["metrics"].get("followers_count", 0),
            following_count=user["metrics"].get("following_count", 0),
            tweet_count=user["metrics"].get("tweet_count", 0),
            location=user["profile"].get("location"),
            website=user["profile"].get("website"),
            profile_image_url=user["profile"].get("profile_image_url"),
            created_at=user["metadata"].get("created_at"),
            collected_at=user["metadata"]["collected_at"]
        )
```

#### ğŸ¯ **APIä½¿ç”¨ç¤ºä¾‹**

```bash
# 1. æœç´¢æ¨æ–‡
curl -X GET "http://localhost:8000/api/v1/data/tweets/search?keyword=python&limit=50" \
  -H "Authorization: Bearer your-api-token"

# 2. è·å–ç”¨æˆ·èµ„æ–™
curl -X GET "http://localhost:8000/api/v1/data/users/elonmusk" \
  -H "Authorization: Bearer your-api-token"

# 3. åˆ›å»ºæœç´¢ä»»åŠ¡
curl -X POST "http://localhost:8000/api/v1/tasks/search" \
  -H "Authorization: Bearer your-api-token" \
  -H "Content-Type: application/json" \
  -d '{
    "type": "keyword_search",
    "keyword": "artificial intelligence",
    "count": 1000,
    "priority": "high"
  }'

# 4. è·å–ç³»ç»Ÿç»Ÿè®¡
curl -X GET "http://localhost:8000/api/v1/admin/stats" \
  -H "Authorization: Bearer your-api-token"

# 5. å¥åº·æ£€æŸ¥
curl -X GET "http://localhost:8000/api/v1/admin/health"
```

#### ğŸ“Š **APIå“åº”ç¤ºä¾‹**

```json
{
  "tweet_id": "1234567890123456789",
  "text": "Python is amazing for data science! #python #datascience",
  "user": {
    "user_id": "987654321",
    "username": "data_scientist",
    "display_name": "Data Scientist",
    "verified": false,
    "followers_count": 5000
  },
  "like_count": 150,
  "retweet_count": 45,
  "reply_count": 12,
  "quote_count": 8,
  "view_count": 2500,
  "hashtags": ["python", "datascience"],
  "mentions": [],
  "urls": ["https://example.com/article"],
  "media": [],
  "created_at": "2024-01-01T12:00:00Z",
  "collected_at": "2024-01-01T12:05:00Z"
}
```

## åç»­æ‰©å±•éœ€æ±‚

### ğŸš€ **ç”²æ–¹åç»­éœ€æ±‚è§„åˆ’**

æ ¹æ®ç”²æ–¹éœ€æ±‚æ–‡æ¡£ï¼Œç³»ç»Ÿéœ€è¦æ”¯æŒå¤šå¹³å°æ‰©å±•ï¼š

#### ğŸ“˜ **Facebook æ‰©å±•éœ€æ±‚**

```python
# Facebookç”¨æˆ·æ•°æ®ç»“æ„
{
  "platform": "facebook",
  "user_id": "facebook_user_123",
  "user_details": {
    "name": "ç”¨æˆ·åç§°",
    "avatar": "å¤´åƒURL",
    "description": "ç”¨æˆ·ç®€ä»‹",
    "gender": "æ€§åˆ«",
    "friends_count": 1500,  # å¥½å‹æ•°
    "location": {
      "country": "ä¸­å›½",
      "city": "åŒ—äº¬"
    }
  },
  "posts": {
    "post_id": "facebook_post_456",
    "content": "å¸–æ–‡å†…å®¹",
    "like_list": [  # ç‚¹èµåˆ—è¡¨
      {"user_id": "user1", "name": "ç”¨æˆ·1"},
      {"user_id": "user2", "name": "ç”¨æˆ·2"}
    ],
    "share_list": [  # è½¬å‘åˆ—è¡¨
      {"user_id": "user3", "name": "ç”¨æˆ·3"},
      {"user_id": "user4", "name": "ç”¨æˆ·4"}
    ]
  }
}
```

#### ğŸ¦ **Twitter æ‰©å±•éœ€æ±‚**

```python
# Twitteræ‰©å±•æ•°æ®ç»“æ„
{
  "platform": "twitter",
  "extended_features": {
    "search_terms": ["æœç´¢è¯1", "æœç´¢è¯2"],  # æœç´¢è¯åŠŸèƒ½
    "retweet_lists": {
      "direct_retweets": [  # ç›´æ¥è½¬æ¨åˆ—è¡¨
        {"user_id": "user1", "retweet_time": "2024-01-01T12:00:00Z"}
      ],
      "quote_retweets": [   # å¼•ç”¨è½¬æ¨åˆ—è¡¨
        {
          "user_id": "user2",
          "quote_content": "å¼•ç”¨å†…å®¹",
          "quote_time": "2024-01-01T12:30:00Z"
        }
      ]
    }
  }
}
```

#### ğŸ“¸ **Instagram æ‰©å±•éœ€æ±‚**

```python
# Instagramæ•°æ®ç»“æ„
{
  "platform": "instagram",
  "user_id": "instagram_user_789",
  "user_details": {
    "username": "ç”¨æˆ·å",
    "display_name": "æ˜¾ç¤ºåç§°",
    "avatar": "å¤´åƒURL",
    "bio": "ä¸ªäººç®€ä»‹",
    "location": {
      "country": "ç¾å›½",
      "city": "çº½çº¦"
    }
  },
  "posts": {
    "post_id": "instagram_post_101",
    "content": "å¸–æ–‡å†…å®¹",
    "media_type": "image",  # image/video/carousel
    "like_list": [  # ç‚¹èµåˆ—è¡¨
      {"user_id": "user5", "username": "user5_name"}
    ]
  },
  "search_terms": ["æœç´¢è¯A", "æœç´¢è¯B"]  # æœç´¢è¯åŠŸèƒ½
}
```

### ğŸ“Š **å®æ–½ä¼˜å…ˆçº§**

#### **ç¬¬ä¸€é˜¶æ®µ (å½“å‰)ï¼šTwitteræ ¸å¿ƒåŠŸèƒ½**
- âœ… å…³é”®è¯æœç´¢é‡‡é›†
- âœ… åŒæ¨¡å¼ä»»åŠ¡ç³»ç»Ÿ
- âœ… å®Œæ•´æ•°æ®è§£æ
- âœ… OSSèµ„æºç®¡ç†
- âœ… APIæ¥å£ä½“ç³»

#### **ç¬¬äºŒé˜¶æ®µï¼šTwitteræ‰©å±•åŠŸèƒ½**
- ğŸ”„ æœç´¢è¯ç®¡ç†
- ğŸ”„ è½¬æ¨åˆ—è¡¨é‡‡é›†
- ğŸ”„ å¼•ç”¨è½¬æ¨åˆ†æ
- ğŸ”„ é«˜çº§æ•°æ®åˆ†æ

#### **ç¬¬ä¸‰é˜¶æ®µï¼šFacebooké›†æˆ**
- ğŸ“‹ ç”¨æˆ·è¯¦æƒ…é‡‡é›†
- ğŸ“‹ å¥½å‹å…³ç³»åˆ†æ
- ğŸ“‹ å¸–æ–‡äº’åŠ¨åˆ—è¡¨
- ğŸ“‹ åœ°ç†ä½ç½®æ•°æ®

#### **ç¬¬å››é˜¶æ®µï¼šInstagramé›†æˆ**
- ğŸ“‹ è§†è§‰å†…å®¹é‡‡é›†
- ğŸ“‹ ç”¨æˆ·åœ°ç†æ•°æ®
- ğŸ“‹ äº’åŠ¨åˆ—è¡¨åˆ†æ
- ğŸ“‹ æœç´¢è¯åŠŸèƒ½

## æ€»ç»“

### âœ… **å®Œå…¨ç¬¦åˆç”²æ–¹éœ€æ±‚**

1. **ä¸šåŠ¡éœ€æ±‚100%è¦†ç›–** - æ‰€æœ‰ç”²æ–¹è¦æ±‚çš„åŠŸèƒ½éƒ½å·²è®¾è®¡
2. **æ•°æ®å­—æ®µå®Œæ•´å¯¹åº”** - ä¸¥æ ¼æŒ‰ç…§ç”²æ–¹20ä¸ªå­—æ®µè¦æ±‚
3. **APIæ¥å£æ ‡å‡†åŒ–** - æä¾›ç”²ä¹™åŒæ–¹å®Œæ•´çš„æ¥å£è§„èŒƒ
4. **OSSèµ„æºç®¡ç†** - å®Œæ•´çš„å›¾ç‰‡ä¸‹è½½å’Œå­˜å‚¨æ–¹æ¡ˆ
5. **æ•°æ®ç»Ÿè®¡å»é‡** - æŒ‰å…³é”®è¯æœˆåº¦å»é‡ç»Ÿè®¡

### ğŸ¯ **æŠ€æœ¯å®ç°ä¼˜åŠ¿**

1. **ç”Ÿäº§å°±ç»ª** - åŒ…å«ç”Ÿäº§ç¯å¢ƒå¿…éœ€ç»„ä»¶
2. **å¯æ‰©å±•æ€§** - æ”¯æŒå¤šå¹³å°åç»­æ‰©å±•
3. **ç¨³å®šæ€§** - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œç›‘æ§
4. **æ ‡å‡†åŒ–** - ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’ŒAPIè§„èŒƒ

### ğŸ’¡ **Apifyç»éªŒé›†æˆä¼˜åŠ¿**

åŸºäºApify Twitter Scraperçš„æˆåŠŸç»éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

#### **ğŸ¯ æŸ¥è¯¢ä¼˜åŒ–**
- **é«˜çº§æœç´¢è¯­æ³•** - æ”¯æŒå¤æ‚çš„TwitteræŸ¥è¯¢è¯­æ³•
- **æ™ºèƒ½åˆ†æ®µæŸ¥è¯¢** - è‡ªåŠ¨ä¼˜åŒ–å¤§é‡æ•°æ®é‡‡é›†
- **æˆæœ¬æ§åˆ¶** - é€æ˜çš„äº‹ä»¶é©±åŠ¨å®šä»·æ¨¡å‹

#### **ğŸ“Š æ•°æ®è´¨é‡**
- **å®Œæ•´å­—æ®µæ˜ å°„** - ä¸¥æ ¼æŒ‰ç…§ç”²æ–¹20ä¸ªå­—æ®µè¦æ±‚
- **åª’ä½“å¤„ç†** - æ”¯æŒå›¾ç‰‡ã€è§†é¢‘çš„å®Œæ•´ä¿¡æ¯
- **åŸå§‹æ•°æ®ä¿ç•™** - æ»¡è¶³ç”²æ–¹åŸå§‹æ•°æ®è¦æ±‚

#### **âš¡ æ€§èƒ½ä¼˜åŒ–**
- **æ‰¹é‡å¤„ç†** - æ”¯æŒå¤§è§„æ¨¡æ•°æ®é‡‡é›†
- **æ™ºèƒ½é‡è¯•** - è‡ªåŠ¨å¤„ç†å¤±è´¥å’Œé™æµ
- **èµ„æºç®¡ç†** - é«˜æ•ˆçš„è´¦å·å’Œä»£ç†è½®æ¢

#### **ğŸ’° æˆæœ¬æ•ˆç›Š**
- **é€æ˜å®šä»·** - å‚è€ƒApifyçš„äº‹ä»¶é©±åŠ¨æ¨¡å‹
- **æŸ¥è¯¢ä¼˜åŒ–** - å‡å°‘ä¸å¿…è¦çš„APIè°ƒç”¨
- **æ‰¹é‡æŠ˜æ‰£** - å¤§é‡é‡‡é›†æ—¶çš„æˆæœ¬ä¼˜åŠ¿

### ğŸš€ **å®æ–½å»ºè®®**

1. **ç«‹å³å¼€å§‹Twitteræ ¸å¿ƒåŠŸèƒ½** - æŠ€æœ¯é£é™©å¯æ§ï¼Œå¯ä»¥ç«‹å³å®æ–½
2. **åˆ†é˜¶æ®µäº¤ä»˜** - æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯ç”¨çš„åŠŸèƒ½
3. **ä¸¥æ ¼æŒ‰éœ€æ±‚å®æ–½** - ç¡®ä¿æ¯ä¸ªåŠŸèƒ½éƒ½ç¬¦åˆç”²æ–¹è¦æ±‚
4. **é¢„ç•™æ‰©å±•æ¥å£** - ä¸ºåç»­å¤šå¹³å°æ‰©å±•åšå¥½å‡†å¤‡
5. **å€Ÿé‰´Apifyæœ€ä½³å®è·µ** - é‡‡ç”¨ç»è¿‡éªŒè¯çš„æ¶æ„å’Œä¼˜åŒ–ç­–ç•¥

### ğŸ“ˆ **é¢„æœŸæ•ˆæœ**

åŸºäºApify Twitter Scraperçš„æˆåŠŸæ¡ˆä¾‹ï¼Œæˆ‘ä»¬çš„XGetç³»ç»Ÿé¢„æœŸèƒ½å¤Ÿå®ç°ï¼š

- **é‡‡é›†æ•ˆç‡**: æ¯ç§’å¤„ç†49-64æ¡æ¨æ–‡ (å‚è€ƒApifyæ€§èƒ½)
- **æˆæœ¬æ§åˆ¶**: é€æ˜çš„$0.0004/æ¡æ¨æ–‡å®šä»·æ¨¡å‹
- **ç¨³å®šæ€§**: >99%çš„è¿è¡ŒæˆåŠŸç‡
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ— é™é‡æ•°æ®é‡‡é›†
- **åˆè§„æ€§**: å®Œå…¨ç¬¦åˆç”²æ–¹ä¸šåŠ¡éœ€æ±‚

è¿™ä¸ªæ–¹æ¡ˆå®Œå…¨åŸºäºç”²æ–¹éœ€æ±‚æ–‡æ¡£è®¾è®¡ï¼Œå¹¶èåˆäº†Apify Twitter Scraperçš„æˆåŠŸç»éªŒï¼Œç¡®ä¿äº†ä¸šåŠ¡éœ€æ±‚çš„100%è¦†ç›–å’ŒæŠ€æœ¯å®ç°çš„å¯è¡Œæ€§ã€‚
