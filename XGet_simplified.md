# XGet å¹³è¡¡å®æ–½æ–¹æ¡ˆ - ç”Ÿäº§å°±ç»ªç‰ˆæœ¬

## é¡¹ç›®æ¦‚è¿°

åŸºäºå¯¹åŸæ–¹æ¡ˆçš„åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªå¹³è¡¡äº†åŠŸèƒ½å®Œæ•´æ€§å’Œå®æ–½å¤æ‚åº¦çš„X(Twitter)æ•°æ®é‡‡é›†ç³»ç»Ÿæ–¹æ¡ˆã€‚åœ¨ä¿æŒæ ¸å¿ƒåŠŸèƒ½å®Œæ•´çš„åŒæ—¶ï¼Œé¿å…è¿‡åº¦è®¾è®¡ï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## æ ¸å¿ƒåŸåˆ™

1. **ç”Ÿäº§å°±ç»ªä¼˜å…ˆ** - åŒ…å«ç”Ÿäº§ç¯å¢ƒå¿…éœ€çš„å…³é”®ç»„ä»¶
2. **æŠ€æœ¯æ ˆå¹³è¡¡** - ä½¿ç”¨æˆç†Ÿç¨³å®šçš„æŠ€æœ¯ç»„åˆï¼Œä½†ä¿ç•™å¿…è¦çš„å¤æ‚æ€§
3. **æ¨¡å—åŒ–æ¶æ„** - å•ä½“åº”ç”¨ä½†æ¨¡å—æ¸…æ™°ï¼Œæ”¯æŒåç»­å¾®æœåŠ¡æ‹†åˆ†
4. **æ¸è¿›å¼æ‰©å±•** - æ”¯æŒåŠŸèƒ½å’Œæ¶æ„çš„å¹³æ»‘å‡çº§

## æŠ€æœ¯æ ˆé€‰æ‹©

### æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶
- **ç¼–ç¨‹è¯­è¨€**: Python 3.12.11 (å·²éªŒè¯) / Python 3.9+ (æœ€ä½è¦æ±‚)
- **çˆ¬å–æ¡†æ¶**: twscrape 0.17.0 (å·²éªŒè¯) + httpx 0.28.1 (ç½‘ç»œå±‚)
- **æµè§ˆå™¨è‡ªåŠ¨åŒ–**: Playwright 1.53.0 (å·²éªŒè¯ï¼Œcookiesç®¡ç† + ç‰¹æ®Šåœºæ™¯)
- **ä»»åŠ¡é˜Ÿåˆ—**: Celery + Redis
- **æ•°æ®å­˜å‚¨**: MongoDB (æ–‡æ¡£å­˜å‚¨) + Redis (ç¼“å­˜/ä¼šè¯)
- **APIæ¡†æ¶**: FastAPI + Uvicorn
- **é…ç½®ç®¡ç†**: Pydantic Settings + ç¯å¢ƒå˜é‡
- **æ—¥å¿—ç³»ç»Ÿ**: Structured Logging (JSONæ ¼å¼)
- **ç›‘æ§**: Prometheus + Grafana (å¯é€‰)
- **éƒ¨ç½²**: Docker Compose (å¼€å‘) + Docker Swarm/K8s (ç”Ÿäº§)

### âœ… æŠ€æœ¯éªŒè¯çŠ¶æ€
- **ğŸ‰ æ ¸å¿ƒæŠ€æœ¯æ ˆ**: 100% éªŒè¯é€šè¿‡
- **ğŸª Cookiesæ–¹æ¡ˆ**: å·²è§£å†³ç™»å½•éš¾é¢˜
- **ğŸ“Š æ•°æ®é‡‡é›†**: å·²éªŒè¯å¯è·å–çœŸå®æ•°æ®
- **ğŸ­ æµè§ˆå™¨è‡ªåŠ¨åŒ–**: å…¨åŠŸèƒ½éªŒè¯é€šè¿‡
- **ğŸš€ å¼€å‘å°±ç»ª**: å¯ç«‹å³å¼€å§‹é¡¹ç›®å¼€å‘

### å¿…è¦çš„ç”Ÿäº§ç»„ä»¶
- **ä»£ç†IPç®¡ç†** - ç”Ÿäº§ç¯å¢ƒå¿…éœ€
- **è´¦å·æ± ç®¡ç†** - Cookieè½®æ¢å’Œå¥åº·æ£€æŸ¥
- **é”™è¯¯å¤„ç†å’Œé‡è¯•** - æé«˜ç³»ç»Ÿç¨³å®šæ€§
- **æ•°æ®éªŒè¯** - Pydanticæ¨¡å‹éªŒè¯
- **åŸºç¡€ç›‘æ§** - ç³»ç»Ÿå¥åº·çŠ¶æ€ç›‘æ§
- **é…ç½®ç®¡ç†** - ç¯å¢ƒéš”ç¦»å’Œé…ç½®çƒ­æ›´æ–°

## ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        XGet å¹³è¡¡æ¶æ„                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Web API       â”‚   ä»»åŠ¡è°ƒåº¦       â”‚   æ•°æ®å­˜å‚¨       â”‚   åŸºç¡€è®¾æ–½     â”‚
â”‚   (FastAPI)     â”‚   (Celery)      â”‚   (MongoDB)     â”‚   (ç›‘æ§/æ—¥å¿—)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é‡‡é›†å¼•æ“      â”‚    â”‚  èµ„æºç®¡ç†      â”‚    â”‚  æ•°æ®å¤„ç†      â”‚
â”‚  twscrape     â”‚    â”‚  è´¦å·/ä»£ç†æ±     â”‚    â”‚  éªŒè¯/å­˜å‚¨     â”‚
â”‚  Playwright   â”‚    â”‚  å¥åº·æ£€æŸ¥      â”‚    â”‚  Redisç¼“å­˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

1. **é‡‡é›†å¼•æ“å±‚** - è´Ÿè´£å®é™…çš„æ•°æ®æŠ“å–
2. **èµ„æºç®¡ç†å±‚** - ç®¡ç†è´¦å·æ± ã€ä»£ç†IPç­‰èµ„æº
3. **æ•°æ®å¤„ç†å±‚** - æ•°æ®éªŒè¯ã€å­˜å‚¨ã€ç¼“å­˜
4. **APIæœåŠ¡å±‚** - å¯¹å¤–æä¾›æ¥å£æœåŠ¡
5. **ä»»åŠ¡è°ƒåº¦å±‚** - åˆ†å¸ƒå¼ä»»åŠ¡ç®¡ç†
6. **åŸºç¡€è®¾æ–½å±‚** - ç›‘æ§ã€æ—¥å¿—ã€é…ç½®ç®¡ç†

## ğŸ¯ æŠ€æœ¯åˆ†å±‚æ¶æ„è¯¦è§£

åŸºäºæŠ€æœ¯éªŒè¯ç»“æœï¼ŒXGeté¡¹ç›®é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œæ¯å±‚è´Ÿè´£ç‰¹å®šåŠŸèƒ½ï¼Œå®ç°é«˜æ•ˆåä½œï¼š

### ğŸ“‹ **æŠ€æœ¯åˆ†å±‚æ¦‚è§ˆ**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        XGet æŠ€æœ¯åˆ†å±‚æ¶æ„                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸª è®¤è¯å±‚      â”‚   ğŸ“Š æ•°æ®é‡‡é›†å±‚   â”‚   ğŸ”§ ç®¡ç†å±‚       â”‚   ğŸš€ åº”ç”¨å±‚     â”‚
â”‚   (Playwright)  â”‚   (twscrape)    â”‚   (Python)      â”‚   (FastAPI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸª **ç¬¬ä¸€å±‚ï¼šè®¤è¯å±‚ï¼ˆPlaywrightï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šè‡ªåŠ¨åŒ–cookiesè·å–å’Œç™»å½•ç®¡ç†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **è‡ªåŠ¨ç™»å½•è·å–cookies** - è§£æ”¾äººå·¥æ“ä½œ
- **æ‰¹é‡è´¦å·ç®¡ç†** - æ”¯æŒå¤šè´¦å·è‡ªåŠ¨åŒ–
- **cookiesè‡ªåŠ¨åˆ·æ–°** - è¿‡æœŸæ—¶è‡ªåŠ¨æ›´æ–°
- **å¤„ç†ç™»å½•éªŒè¯** - éªŒè¯ç ã€é‚®ç®±éªŒè¯ç­‰

#### ğŸ”§ **æŠ€æœ¯å®ç°**ï¼š
```python
# è‡ªåŠ¨åŒ–cookiesè·å–
async def auto_extract_cookies(account_info):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        # 1. è‡ªåŠ¨ç™»å½•
        await auto_login(page, account_info)

        # 2. æå–cookies
        cookies = await page.context.cookies()

        # 3. å¯¼å…¥åˆ°twscrape
        await import_to_twscrape(cookies)

        return cookies
```

#### ğŸ¯ **åº”ç”¨åœºæ™¯**ï¼š
- **æ‰¹é‡è´¦å·åˆå§‹åŒ–** - æ–°é¡¹ç›®å¯åŠ¨æ—¶
- **å®šæœŸç»´æŠ¤** - cookiesè¿‡æœŸè‡ªåŠ¨åˆ·æ–°
- **æ•…éšœæ¢å¤** - è´¦å·è¢«é™åˆ¶æ—¶é‡æ–°è·å–
- **æ‰©å®¹æ”¯æŒ** - æ–°å¢è´¦å·æ—¶è‡ªåŠ¨é…ç½®

### ğŸ“Š **ç¬¬äºŒå±‚ï¼šæ•°æ®é‡‡é›†å±‚ï¼ˆtwscrapeï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šé«˜æ•ˆçš„Twitteræ•°æ®é‡‡é›†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **æ¨æ–‡æœç´¢** - å…³é”®è¯ã€è¯é¢˜ã€ç”¨æˆ·æœç´¢
- **ç”¨æˆ·ä¿¡æ¯è·å–** - å®Œæ•´çš„ç”¨æˆ·èµ„æ–™
- **é€Ÿç‡é™åˆ¶ç®¡ç†** - å†…ç½®æ™ºèƒ½é™åˆ¶
- **å¤šè´¦å·è½®æ¢** - è‡ªåŠ¨è´¦å·åˆ‡æ¢

#### ğŸ”§ **æŠ€æœ¯ä¼˜åŠ¿**ï¼š
```python
# é«˜æ•ˆæ•°æ®é‡‡é›†
async def efficient_data_collection():
    api = API()  # ä½¿ç”¨Playwrightç»´æŠ¤çš„cookies

    # é«˜æ€§èƒ½æœç´¢
    tweets = []
    async for tweet in api.search("python", limit=1000):
        tweets.append(process_tweet(tweet))

    return tweets
```

#### ğŸ“ˆ **æ€§èƒ½ç‰¹ç‚¹**ï¼š
- **âš¡ é«˜æ•ˆç‡** - APIè°ƒç”¨æ¯”æµè§ˆå™¨å¿«10å€
- **ğŸ›¡ï¸ ç¨³å®šæ€§** - ä¸“é—¨ä¼˜åŒ–çš„åæ£€æµ‹
- **ğŸ”„ æ™ºèƒ½è½®æ¢** - è‡ªåŠ¨ç®¡ç†è´¦å·å’Œè¯·æ±‚
- **ğŸ“Š ç»“æ„åŒ–æ•°æ®** - ç›´æ¥è¿”å›Pythonå¯¹è±¡

### ğŸ”§ **ç¬¬ä¸‰å±‚ï¼šç®¡ç†å±‚ï¼ˆPythonè„šæœ¬ï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šåè°ƒä¸¤ä¸ªå·¥å…·ï¼Œæä¾›ç»Ÿä¸€ç®¡ç†

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **å·¥å…·åè°ƒ** - Playwright + twscrapeåä½œ
- **é”™è¯¯å¤„ç†** - ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•
- **æ•°æ®å¤„ç†** - æ¸…æ´—ã€éªŒè¯ã€å­˜å‚¨
- **ç›‘æ§æ—¥å¿—** - ç³»ç»ŸçŠ¶æ€ç›‘æ§

#### ğŸ”§ **æ¶æ„è®¾è®¡**ï¼š
```python
class XGetManager:
    """XGetç»Ÿä¸€ç®¡ç†å™¨"""

    def __init__(self):
        self.playwright_manager = PlaywrightManager()
        self.twscrape_manager = TwscrapeManager()
        self.data_manager = DataManager()

    async def collect_data(self, keyword: str):
        # 1. ç¡®ä¿cookiesæœ‰æ•ˆ
        await self.playwright_manager.ensure_cookies_valid()

        # 2. æ‰§è¡Œæ•°æ®é‡‡é›†
        tweets = await self.twscrape_manager.search(keyword)

        # 3. æ•°æ®å¤„ç†å’Œå­˜å‚¨
        processed_data = await self.data_manager.process(tweets)

        return processed_data
```

#### ğŸ¯ **æ ¸å¿ƒä»·å€¼**ï¼š
- **ğŸ”— æ— ç¼é›†æˆ** - ä¸¤ä¸ªå·¥å…·å®Œç¾é…åˆ
- **ğŸ›¡ï¸ å®¹é”™èƒ½åŠ›** - è‡ªåŠ¨å¤„ç†å„ç§å¼‚å¸¸
- **ğŸ“Š æ•°æ®è´¨é‡** - ç»Ÿä¸€çš„æ•°æ®éªŒè¯
- **ğŸ” å¯è§‚æµ‹æ€§** - å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—

### ğŸš€ **ç¬¬å››å±‚ï¼šåº”ç”¨å±‚ï¼ˆFastAPIï¼‰**

**æ ¸å¿ƒèŒè´£**ï¼šå¯¹å¤–æä¾›æœåŠ¡æ¥å£

#### âœ… **ä¸»è¦åŠŸèƒ½**ï¼š
- **RESTful API** - æ ‡å‡†çš„HTTPæ¥å£
- **ä»»åŠ¡ç®¡ç†** - å¼‚æ­¥ä»»åŠ¡è°ƒåº¦
- **æ•°æ®æŸ¥è¯¢** - çµæ´»çš„æ•°æ®æ£€ç´¢
- **ç³»ç»Ÿç›‘æ§** - å¥åº·çŠ¶æ€æ£€æŸ¥

## ğŸ’¡ **åˆ†å±‚åä½œæµç¨‹**

### ğŸ”„ **å…¸å‹å·¥ä½œæµç¨‹**ï¼š

```text
1. ğŸª Playwrightå±‚ï¼š
   â”œâ”€â”€ æ£€æŸ¥cookiesæœ‰æ•ˆæ€§
   â”œâ”€â”€ è‡ªåŠ¨åˆ·æ–°è¿‡æœŸcookies
   â””â”€â”€ ç»´æŠ¤è´¦å·ç™»å½•çŠ¶æ€

2. ğŸ“Š twscrapeå±‚ï¼š
   â”œâ”€â”€ ä½¿ç”¨æœ‰æ•ˆcookies
   â”œâ”€â”€ æ‰§è¡Œé«˜æ•ˆæ•°æ®é‡‡é›†
   â””â”€â”€ è¿”å›ç»“æ„åŒ–æ•°æ®

3. ğŸ”§ ç®¡ç†å±‚ï¼š
   â”œâ”€â”€ åè°ƒä¸Šè¿°ä¸¤å±‚
   â”œâ”€â”€ å¤„ç†é”™è¯¯å’Œé‡è¯•
   â””â”€â”€ æ•°æ®æ¸…æ´—å’Œå­˜å‚¨

4. ğŸš€ åº”ç”¨å±‚ï¼š
   â”œâ”€â”€ æ¥æ”¶ç”¨æˆ·è¯·æ±‚
   â”œâ”€â”€ è°ƒç”¨ç®¡ç†å±‚æœåŠ¡
   â””â”€â”€ è¿”å›å¤„ç†ç»“æœ
```

### ğŸ¯ **åˆ†å·¥ä¼˜åŠ¿**ï¼š

| å±‚çº§ | ä¸“é•¿ | ä¼˜åŠ¿ |
|------|------|------|
| **Playwright** | æµè§ˆå™¨è‡ªåŠ¨åŒ– | ğŸª è§£å†³ç™»å½•éš¾é¢˜ |
| **twscrape** | APIé«˜æ•ˆè°ƒç”¨ | âš¡ 10å€æ€§èƒ½æå‡ |
| **Pythonç®¡ç†** | ç³»ç»Ÿåè°ƒ | ğŸ”§ ç»Ÿä¸€æ§åˆ¶ |
| **FastAPI** | æœåŠ¡æ¥å£ | ğŸš€ æ ‡å‡†åŒ–æœåŠ¡ |

## ğŸ‰ **æ¶æ„ä¼˜åŠ¿æ€»ç»“**

### âœ… **æŠ€æœ¯ä¼˜åŠ¿**ï¼š
1. **ğŸ”‘ è§£å†³æ ¸å¿ƒéš¾é¢˜** - Playwrightè§£å†³ç™»å½•é—®é¢˜
2. **âš¡ ä¿æŒé«˜æ€§èƒ½** - twscrapeæä¾›é«˜æ•ˆé‡‡é›†
3. **ğŸ›¡ï¸ æé«˜ç¨³å®šæ€§** - åˆ†å±‚è®¾è®¡é™ä½è€¦åˆ
4. **ğŸ”§ ä¾¿äºç»´æŠ¤** - èŒè´£æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•

### âœ… **ä¸šåŠ¡ä¼˜åŠ¿**ï¼š
1. **ğŸ“ˆ å¯æ‰©å±•æ€§** - æ¯å±‚ç‹¬ç«‹æ‰©å±•
2. **ğŸ”„ å¯æ›¿æ¢æ€§** - å•å±‚æ›¿æ¢ä¸å½±å“æ•´ä½“
3. **ğŸ¯ ä¸“ä¸šåŒ–** - æ¯å±‚ä¸“æ³¨æ ¸å¿ƒèƒ½åŠ›
4. **ğŸ’° æˆæœ¬æ•ˆç›Š** - æœ€å¤§åŒ–åˆ©ç”¨å„å·¥å…·ä¼˜åŠ¿

## æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. æ•°æ®é‡‡é›†æ¨¡å—

```python
# core/scraper.py
import asyncio
import random
from twscrape import API
from typing import List, Dict, Optional
from datetime import datetime
import logging
from .proxy_manager import ProxyManager
from .account_manager import AccountManager

class ProductionTwitterScraper:
    """ç”Ÿäº§çº§Twitteré‡‡é›†å™¨"""

    def __init__(self, account_manager: AccountManager, proxy_manager: ProxyManager):
        self.api = API()
        self.account_manager = account_manager
        self.proxy_manager = proxy_manager
        self.logger = logging.getLogger(__name__)

    async def search_tweets(self, keyword: str, count: int = 100) -> List[Dict]:
        """æœç´¢æ¨æ–‡ - å¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•"""
        tweets = []
        retry_count = 0
        max_retries = 3

        while retry_count < max_retries:
            try:
                # è·å–å¯ç”¨è´¦å·å’Œä»£ç†
                account = await self.account_manager.get_available_account()
                proxy = await self.proxy_manager.get_proxy()

                # é…ç½®API
                if proxy:
                    self.api.set_proxy(proxy['url'])

                async for tweet in self.api.search(keyword, limit=count):
                    tweet_data = {
                        'id': tweet.id,
                        'text': tweet.rawContent,
                        'user_id': tweet.user.id,
                        'username': tweet.user.username,
                        'created_at': tweet.date.isoformat(),
                        'retweet_count': tweet.retweetCount,
                        'like_count': tweet.likeCount,
                        'reply_count': tweet.replyCount,
                        'quote_count': tweet.quoteCount,
                        'view_count': getattr(tweet, 'viewCount', 0),
                        'media': [{'url': m.url, 'type': m.type} for m in tweet.media] if tweet.media else [],
                        'hashtags': [tag.text for tag in tweet.hashtags] if tweet.hashtags else [],
                        'urls': [url.expandedUrl for url in tweet.urls] if tweet.urls else [],
                        'collected_at': datetime.utcnow().isoformat(),
                        'source_account': account['id'],
                        'source_proxy': proxy['id'] if proxy else None
                    }
                    tweets.append(tweet_data)

                # æˆåŠŸåæ›´æ–°è´¦å·çŠ¶æ€
                await self.account_manager.update_account_success(account['id'])
                break

            except Exception as e:
                retry_count += 1
                self.logger.warning(f"Search failed (attempt {retry_count}): {str(e)}")

                if retry_count < max_retries:
                    # æ ‡è®°è´¦å·å¯èƒ½æœ‰é—®é¢˜
                    if 'account' in locals():
                        await self.account_manager.mark_account_error(account['id'], str(e))

                    # ç­‰å¾…åé‡è¯•
                    await asyncio.sleep(random.uniform(5, 15))
                else:
                    raise

        return tweets

    async def get_user_profile(self, username: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·èµ„æ–™ - å®Œæ•´å®ç°"""
        try:
            user = await self.api.user_by_login(username)
            if not user:
                return None

            return {
                'user_id': user.id,
                'username': user.username,
                'display_name': user.displayname,
                'description': user.description,
                'followers_count': user.followersCount,
                'following_count': user.followingCount,
                'tweet_count': user.statusesCount,
                'verified': user.verified,
                'created_at': user.created.isoformat() if user.created else None,
                'location': user.location,
                'profile_image_url': user.profileImageUrl,
                'profile_banner_url': user.profileBannerUrl,
                'collected_at': datetime.utcnow().isoformat()
            }
        except Exception as e:
            self.logger.error(f"Failed to get user profile for {username}: {str(e)}")
            raise
```

### 2. è´¦å·ç®¡ç†æ¨¡å—

```python
# core/account_manager.py
import asyncio
import json
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import redis.asyncio as redis

class AccountManager:
    """è´¦å·æ± ç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.health_threshold = 0.7

    async def get_available_account(self) -> Optional[Dict]:
        """è·å–å¯ç”¨è´¦å·"""
        try:
            # è·å–å¥åº·è´¦å·åˆ—è¡¨
            healthy_accounts = await self.redis.smembers('accounts:healthy')
            if not healthy_accounts:
                raise Exception("No healthy accounts available")

            # é€‰æ‹©ä½¿ç”¨é¢‘ç‡æœ€ä½çš„è´¦å·
            best_account = None
            min_usage = float('inf')

            for account_id in healthy_accounts:
                usage_count = await self.redis.get(f'account:{account_id.decode()}:usage_today')
                usage = int(usage_count) if usage_count else 0

                if usage < min_usage:
                    min_usage = usage
                    best_account = account_id.decode()

            if best_account:
                # å¢åŠ ä½¿ç”¨è®¡æ•°
                await self.redis.incr(f'account:{best_account}:usage_today')
                await self.redis.expire(f'account:{best_account}:usage_today', 86400)

                # è·å–è´¦å·è¯¦æƒ…
                account_data = await self.redis.hgetall(f'account:{best_account}')
                return {
                    'id': best_account,
                    'username': account_data.get(b'username', b'').decode(),
                    'cookies': json.loads(account_data.get(b'cookies', '{}'))
                }

            return None

        except Exception as e:
            logging.error(f"Failed to get available account: {str(e)}")
            return None

    async def update_account_success(self, account_id: str):
        """æ›´æ–°è´¦å·æˆåŠŸä½¿ç”¨è®°å½•"""
        await self.redis.hincrby(f'account:{account_id}', 'success_count', 1)
        await self.redis.hset(f'account:{account_id}', 'last_success', datetime.utcnow().isoformat())

    async def mark_account_error(self, account_id: str, error: str):
        """æ ‡è®°è´¦å·é”™è¯¯"""
        await self.redis.hincrby(f'account:{account_id}', 'error_count', 1)
        await self.redis.hset(f'account:{account_id}', 'last_error', error)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœè´¦å·
        error_count = await self.redis.hget(f'account:{account_id}', 'error_count')
        if error_count and int(error_count) > 5:
            await self.redis.srem('accounts:healthy', account_id)
            await self.redis.sadd('accounts:suspended', account_id)
```

### 3. ä»£ç†ç®¡ç†æ¨¡å—

```python
# core/proxy_manager.py
import aiohttp
import random
from typing import Dict, List, Optional

class ProxyManager:
    """ä»£ç†IPç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client

    async def get_proxy(self) -> Optional[Dict]:
        """è·å–å¯ç”¨ä»£ç†"""
        try:
            healthy_proxies = await self.redis.smembers('proxies:healthy')
            if not healthy_proxies:
                return None

            proxy_id = random.choice(list(healthy_proxies)).decode()
            proxy_config = await self.redis.hget(f'proxy:{proxy_id}', 'config')

            if proxy_config:
                return json.loads(proxy_config)
            return None

        except Exception as e:
            logging.error(f"Failed to get proxy: {str(e)}")
            return None

    async def check_proxy_health(self, proxy: Dict) -> bool:
        """æ£€æŸ¥ä»£ç†å¥åº·çŠ¶æ€"""
        try:
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(
                    'https://httpbin.org/ip',
                    proxy=proxy['url']
                ) as response:
                    return response.status == 200
        except:
            return False
```

### 4. æ•°æ®å­˜å‚¨æ¨¡å—

```python
# core/storage.py
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel, ValidationError
from typing import Dict, List, Optional
import logging
from datetime import datetime

class TweetModel(BaseModel):
    """æ¨æ–‡æ•°æ®æ¨¡å‹"""
    id: str
    text: str
    user_id: str
    username: str
    created_at: str
    retweet_count: int = 0
    like_count: int = 0
    reply_count: int = 0
    quote_count: int = 0
    view_count: int = 0
    media: List[Dict] = []
    hashtags: List[str] = []
    urls: List[str] = []
    collected_at: str
    source_account: Optional[str] = None
    source_proxy: Optional[str] = None

class ProductionDataManager:
    """ç”Ÿäº§çº§æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, mongodb_uri: str, redis_client):
        self.client = AsyncIOMotorClient(mongodb_uri)
        self.db = self.client.xget
        self.redis = redis_client
        self.logger = logging.getLogger(__name__)

    async def save_tweets(self, tweets: List[Dict]) -> Dict[str, int]:
        """æ‰¹é‡ä¿å­˜æ¨æ–‡ - å¸¦éªŒè¯å’Œç»Ÿè®¡"""
        stats = {'saved': 0, 'failed': 0, 'duplicates': 0}

        for tweet_data in tweets:
            try:
                # æ•°æ®éªŒè¯
                tweet = TweetModel(**tweet_data)

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = await self.db.tweets.find_one({'id': tweet.id})
                if existing:
                    # æ›´æ–°äº’åŠ¨æ•°æ®
                    await self.db.tweets.update_one(
                        {'id': tweet.id},
                        {'$set': {
                            'retweet_count': tweet.retweet_count,
                            'like_count': tweet.like_count,
                            'reply_count': tweet.reply_count,
                            'quote_count': tweet.quote_count,
                            'view_count': tweet.view_count,
                            'updated_at': datetime.utcnow()
                        }}
                    )
                    stats['duplicates'] += 1
                else:
                    # æ’å…¥æ–°æ¨æ–‡
                    await self.db.tweets.insert_one(tweet.dict())
                    stats['saved'] += 1

            except ValidationError as e:
                self.logger.error(f"Tweet validation failed: {e}")
                stats['failed'] += 1
            except Exception as e:
                self.logger.error(f"Failed to save tweet: {e}")
                stats['failed'] += 1

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        await self._update_collection_stats('tweets', stats)
        return stats

    async def _update_collection_stats(self, collection: str, stats: Dict):
        """æ›´æ–°é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        date_key = datetime.utcnow().strftime('%Y-%m-%d')
        for key, value in stats.items():
            await self.redis.hincrby(f'stats:{collection}:{date_key}', key, value)
        await self.redis.expire(f'stats:{collection}:{date_key}', 86400 * 30)
```

### 5. ä»»åŠ¡è°ƒåº¦æ¨¡å—

```python
# core/tasks.py
from celery import Celery
from celery.exceptions import Retry
import asyncio
import logging
from datetime import datetime
from .scraper import ProductionTwitterScraper
from .storage import ProductionDataManager
from .account_manager import AccountManager
from .proxy_manager import ProxyManager

app = Celery('xget')
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_routes={
        'core.tasks.search_task': {'queue': 'search'},
        'core.tasks.user_profile_task': {'queue': 'profile'},
        'core.tasks.health_check_task': {'queue': 'maintenance'},
    }
)

@app.task(bind=True, max_retries=3, default_retry_delay=60)
def search_task(self, keyword: str, count: int = 100, priority: str = 'normal'):
    """æœç´¢ä»»åŠ¡ - å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†"""
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)
        scraper = ProductionTwitterScraper(account_manager, proxy_manager)
        data_manager = ProductionDataManager(MONGODB_URI, redis_client)

        # æ‰§è¡Œæœç´¢
        tweets = asyncio.run(scraper.search_tweets(keyword, count))

        # ä¿å­˜æ•°æ®
        stats = asyncio.run(data_manager.save_tweets(tweets))

        # è®°å½•ä»»åŠ¡å®Œæˆ
        task_result = {
            "status": "success",
            "keyword": keyword,
            "requested_count": count,
            "collected_count": len(tweets),
            "save_stats": stats,
            "completed_at": datetime.utcnow().isoformat()
        }

        logging.info(f"Search task completed: {keyword}, collected {len(tweets)} tweets")
        return task_result

    except Exception as e:
        logging.error(f"Search task failed: {keyword}, error: {str(e)}")

        # é‡è¯•é€»è¾‘
        if self.request.retries < self.max_retries:
            logging.info(f"Retrying search task for {keyword} (attempt {self.request.retries + 1})")
            raise self.retry(countdown=60 * (2 ** self.request.retries))

        return {
            "status": "failed",
            "keyword": keyword,
            "error": str(e),
            "retries": self.request.retries,
            "failed_at": datetime.utcnow().isoformat()
        }

@app.task(bind=True, max_retries=2)
def user_profile_task(self, username: str):
    """ç”¨æˆ·èµ„æ–™é‡‡é›†ä»»åŠ¡"""
    try:
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)
        scraper = ProductionTwitterScraper(account_manager, proxy_manager)
        data_manager = ProductionDataManager(MONGODB_URI, redis_client)

        # è·å–ç”¨æˆ·èµ„æ–™
        user_data = asyncio.run(scraper.get_user_profile(username))
        if not user_data:
            return {"status": "not_found", "username": username}

        # ä¿å­˜ç”¨æˆ·æ•°æ®
        await data_manager.save_user(user_data)

        return {
            "status": "success",
            "username": username,
            "user_id": user_data['user_id'],
            "completed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logging.error(f"User profile task failed: {username}, error: {str(e)}")

        if self.request.retries < self.max_retries:
            raise self.retry(countdown=30)

        return {
            "status": "failed",
            "username": username,
            "error": str(e),
            "failed_at": datetime.utcnow().isoformat()
        }

@app.task
def health_check_task():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        account_manager = AccountManager(redis_client)
        proxy_manager = ProxyManager(redis_client)

        # æ£€æŸ¥è´¦å·å¥åº·çŠ¶æ€
        asyncio.run(account_manager.check_all_accounts_health())

        # æ£€æŸ¥ä»£ç†å¥åº·çŠ¶æ€
        asyncio.run(proxy_manager.batch_health_check())

        return {
            "status": "completed",
            "checked_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logging.error(f"Health check failed: {str(e)}")
        return {"status": "failed", "error": str(e)}

# å®šæ—¶ä»»åŠ¡é…ç½®
app.conf.beat_schedule = {
    'health-check': {
        'task': 'core.tasks.health_check_task',
        'schedule': 300.0,  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    },
}
```

## éƒ¨ç½²é…ç½®

### ç”Ÿäº§çº§ Docker Compose é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  mongodb:
    image: mongo:6
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD}
      - MONGO_INITDB_DATABASE=xget
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    restart: unless-stopped
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3

  xget-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENVIRONMENT=${ENVIRONMENT:-production}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  celery-worker-search:
    build: .
    command: celery -A core.tasks worker --loglevel=info --queues=search --concurrency=4
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      replicas: 2

  celery-worker-profile:
    build: .
    command: celery -A core.tasks worker --loglevel=info --queues=profile --concurrency=2
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped

  celery-beat:
    build: .
    command: celery -A core.tasks beat --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379
      - MONGODB_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017/xget
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    restart: unless-stopped

  # å¯é€‰ï¼šç›‘æ§ç»„ä»¶
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis_data:
  mongodb_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

### ç¯å¢ƒé…ç½®æ–‡ä»¶

```bash
# .env.production
MONGO_USER=xget_user
MONGO_PASSWORD=your_secure_password
LOG_LEVEL=INFO
ENVIRONMENT=production
GRAFANA_PASSWORD=your_grafana_password

# ä»£ç†é…ç½®
PROXY_API_KEY=your_proxy_api_key
PROXY_ENDPOINT=https://your-proxy-provider.com/api

# Twitterè´¦å·åŠ å¯†å¯†é’¥
ACCOUNT_ENCRYPTION_KEY=your_32_char_encryption_key

# ç›‘æ§é…ç½®
PROMETHEUS_ENABLED=true
GRAFANA_ENABLED=true
```

### Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨
RUN playwright install chromium

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash xget
RUN chown -R xget:xget /app
USER xget

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## å®æ–½è®¡åˆ’

### ğŸ‰ æŠ€æœ¯éªŒè¯é˜¶æ®µ (å·²å®Œæˆ)

- [x] **Python 3.12 ç¯å¢ƒæ­å»º** - å®Œæˆ
- [x] **twscrape åŠŸèƒ½éªŒè¯** - 100% é€šè¿‡
- [x] **Playwright åŠŸèƒ½éªŒè¯** - 100% é€šè¿‡
- [x] **Cookies è‡ªåŠ¨åŒ–æ–¹æ¡ˆ** - æ ¸å¿ƒçªç ´å®Œæˆ
- [x] **Twitter æ•°æ®é‡‡é›†æµ‹è¯•** - è·å–çœŸå®æ•°æ®æˆåŠŸ
- [x] **æŠ€æœ¯åˆ†å±‚æ¶æ„è®¾è®¡** - å®Œæˆ
- [x] **å¼€å‘ç¯å¢ƒé…ç½®** - å®Œæˆ
- [x] **é¡¹ç›®æ–‡æ¡£å’Œå·¥å…·** - å®Œæˆ

### ç¬¬ä¸€é˜¶æ®µ (2-3å‘¨): æ ¸å¿ƒåŠŸèƒ½å¼€å‘

- [x] é¡¹ç›®ç»“æ„æ­å»ºå’ŒæŠ€æœ¯éªŒè¯
- [ ] è´¦å·ç®¡ç†æ¨¡å—å¼€å‘ (åŸºäºPlaywrightè‡ªåŠ¨åŒ–)
- [ ] ä»£ç†ç®¡ç†æ¨¡å—å¼€å‘
- [ ] åŸºç¡€çˆ¬å–æ¨¡å—å¼€å‘ (twscrapeé›†æˆï¼Œå·²éªŒè¯å¯è¡Œ)
- [ ] æ•°æ®å­˜å‚¨æ¨¡å—å¼€å‘ (MongoDB + æ•°æ®éªŒè¯)
- [ ] ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ (Celery + é˜Ÿåˆ—åˆ†ç¦»)
- [ ] Dockerç¯å¢ƒé…ç½®

### ç¬¬äºŒé˜¶æ®µ (1-2å‘¨): APIå’Œç›‘æ§

- [ ] FastAPIæ¥å£å¼€å‘
- [ ] ä»»åŠ¡ç®¡ç†API (æäº¤ã€æŸ¥è¯¢ã€å–æ¶ˆ)
- [ ] æ•°æ®æŸ¥è¯¢API (æ”¯æŒå¤æ‚æŸ¥è¯¢)
- [ ] ç³»ç»Ÿç›‘æ§API (å¥åº·çŠ¶æ€ã€ç»Ÿè®¡ä¿¡æ¯)
- [ ] åŸºç¡€Webç®¡ç†ç•Œé¢
- [ ] Prometheus + Grafanaé›†æˆ

### ç¬¬ä¸‰é˜¶æ®µ (1-2å‘¨): ç”Ÿäº§ä¼˜åŒ–

- [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶å®Œå–„
- [ ] æ€§èƒ½ä¼˜åŒ– (æ‰¹é‡å¤„ç†ã€è¿æ¥æ± )
- [ ] å®‰å…¨åŠ å›º (è¾“å…¥éªŒè¯ã€è®¿é—®æ§åˆ¶)
- [ ] æ—¥å¿—ç³»ç»Ÿå®Œå–„ (ç»“æ„åŒ–æ—¥å¿—)
- [ ] å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½è°ƒä¼˜
- [ ] éƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œ

### ç¬¬å››é˜¶æ®µ (1å‘¨): æµ‹è¯•å’Œå‘å¸ƒ

- [ ] é›†æˆæµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] ç›‘æ§å‘Šè­¦é…ç½®
- [ ] ç”¨æˆ·åŸ¹è®­å’Œæ–‡æ¡£

## æŠ€æœ¯å€ºåŠ¡ç®¡ç†

### å¿…é¡»å®ç°çš„æ ¸å¿ƒåŠŸèƒ½

1. **è´¦å·æ± ç®¡ç†** - ä¸å¯çœç•¥ï¼Œç›´æ¥å½±å“ç³»ç»Ÿç¨³å®šæ€§
2. **ä»£ç†IPè½®æ¢** - ç”Ÿäº§ç¯å¢ƒå¿…éœ€
3. **æ•°æ®éªŒè¯** - ç¡®ä¿æ•°æ®è´¨é‡
4. **é”™è¯¯å¤„ç†** - æé«˜ç³»ç»Ÿé²æ£’æ€§
5. **åŸºç¡€ç›‘æ§** - ç”Ÿäº§è¿ç»´å¿…éœ€

### å¯ä»¥åæœŸæ·»åŠ çš„åŠŸèƒ½

1. **å¤æ‚çš„MLé¢„æµ‹** - éæ ¸å¿ƒåŠŸèƒ½
2. **ä¼ä¸šçº§æƒé™ç³»ç»Ÿ** - å¯ç”¨ç®€å•è®¤è¯æ›¿ä»£
3. **æ•°æ®å…³ç³»å›¾è°±** - å¯åœ¨MongoDBä¸­ç®€å•å­˜å‚¨
4. **é«˜çº§åˆ†æåŠŸèƒ½** - ä¸šåŠ¡ä»·å€¼éªŒè¯åæ·»åŠ 

## é£é™©è¯„ä¼°ä¸åº”å¯¹

### æŠ€æœ¯é£é™© âš ï¸

**é£é™©**: Twitteråçˆ¬è™«æœºåˆ¶å‡çº§
**åº”å¯¹**:
- å¤šæ ·åŒ–çš„çˆ¬å–ç­–ç•¥ (twscrape + Playwright)
- çµæ´»çš„è´¦å·å’Œä»£ç†è½®æ¢
- å¿«é€Ÿé€‚åº”æœºåˆ¶

**é£é™©**: è´¦å·å°ç¦ç‡è¿‡é«˜
**åº”å¯¹**:
- è´¦å·å¥åº·ç›‘æ§
- ä½¿ç”¨é¢‘ç‡æ§åˆ¶
- å¤šè´¦å·æ± å¤‡ä»½

### ä¸šåŠ¡é£é™© âš ï¸

**é£é™©**: æ•°æ®é‡è¶…å‡ºé¢„æœŸ
**åº”å¯¹**:
- æ°´å¹³æ‰©å±•è®¾è®¡
- æ•°æ®åˆ†ç‰‡ç­–ç•¥
- æ€§èƒ½ç›‘æ§å’Œé¢„è­¦

### è¿ç»´é£é™© âœ…

**é£é™©**: ç³»ç»Ÿç»´æŠ¤å¤æ‚
**åº”å¯¹**:
- å®Œå–„çš„ç›‘æ§ä½“ç³»
- è‡ªåŠ¨åŒ–éƒ¨ç½²
- è¯¦ç»†çš„è¿ç»´æ–‡æ¡£

## æˆæœ¬ä¼°ç®—

### å¼€å‘æˆæœ¬

- **äººåŠ›**: 2-3åå¼€å‘äººå‘˜ï¼Œ6-8å‘¨
- **æŠ€æœ¯æ ˆ**: å…¨éƒ¨ä½¿ç”¨å¼€æºæŠ€æœ¯ï¼Œæ— æˆæƒè´¹ç”¨
- **æ€»ä½“**: ç›¸æ¯”åŸæ–¹æ¡ˆå‡å°‘60%çš„å¼€å‘æ—¶é—´

### è¿ç»´æˆæœ¬

- **æœåŠ¡å™¨**: ä¸­ç­‰é…ç½®å³å¯æ»¡è¶³åˆæœŸéœ€æ±‚
- **ä»£ç†IP**: æ ¹æ®é‡‡é›†é‡æŒ‰éœ€è´­ä¹°
- **ç›‘æ§**: ä½¿ç”¨å¼€æºæ–¹æ¡ˆï¼Œæˆæœ¬å¯æ§

## æ€»ç»“

è¿™ä¸ªå¹³è¡¡æ–¹æ¡ˆç›¸æ¯”åŸæ–¹æ¡ˆçš„ä¼˜åŠ¿ï¼š

### âœ… ä¿ç•™çš„å…³é”®åŠŸèƒ½

1. **ç”Ÿäº§å°±ç»ª** - åŒ…å«ç”Ÿäº§ç¯å¢ƒå¿…éœ€ç»„ä»¶
2. **å¯æ‰©å±•æ€§** - æ”¯æŒåç»­åŠŸèƒ½æ‰©å±•
3. **ç¨³å®šæ€§** - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œç›‘æ§
4. **å¯ç»´æŠ¤æ€§** - æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†

### ğŸ‰ æŠ€æœ¯éªŒè¯æˆæœ

1. **100% æŠ€æœ¯å¯è¡Œæ€§ç¡®è®¤** - æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯å·²éªŒè¯
2. **å…³é”®æŠ€æœ¯çªç ´** - Playwright cookiesè‡ªåŠ¨åŒ–æ–¹æ¡ˆ
3. **çœŸå®æ•°æ®éªŒè¯** - æˆåŠŸè·å–TwitterçœŸå®æ•°æ®
4. **åˆ†å±‚æ¶æ„éªŒè¯** - æŠ€æœ¯åˆ†å·¥æ˜ç¡®ï¼Œåä½œé«˜æ•ˆ
5. **å¼€å‘ç¯å¢ƒå°±ç»ª** - å¯ç«‹å³å¼€å§‹æ­£å¼å¼€å‘

### âœ… ç®€åŒ–çš„éƒ¨åˆ†

1. **æŠ€æœ¯æ ˆ** - å‡å°‘éå¿…éœ€æŠ€æœ¯ç»„ä»¶
2. **æ¶æ„å¤æ‚åº¦** - é¿å…è¿‡æ—©çš„å¾®æœåŠ¡æ‹†åˆ†
3. **ä¼ä¸šçº§åŠŸèƒ½** - æ¨è¿Ÿåˆ°ä¸šåŠ¡éªŒè¯å
4. **å¼€å‘å‘¨æœŸ** - 6-8å‘¨å®Œæˆç”Ÿäº§ç‰ˆæœ¬

### ğŸ¯ å»ºè®®

è¿™ä¸ªå¹³è¡¡æ–¹æ¡ˆæ—¢ä¿è¯äº†ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œåˆé¿å…äº†è¿‡åº¦è®¾è®¡çš„å¤æ‚æ€§ã€‚å»ºè®®ï¼š

1. **ç«‹å³å¼€å§‹** - æŠ€æœ¯é£é™©å¯æ§ï¼Œå¯ä»¥ç«‹å³å¼€å§‹å®æ–½
2. **åˆ†é˜¶æ®µäº¤ä»˜** - æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯ç”¨çš„åŠŸèƒ½
3. **æŒç»­ä¼˜åŒ–** - åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­æ ¹æ®å®é™…éœ€æ±‚ä¼˜åŒ–
4. **ä¸šåŠ¡é©±åŠ¨** - æ ¹æ®ä¸šåŠ¡ä»·å€¼å†³å®šåç»­åŠŸèƒ½ä¼˜å…ˆçº§
