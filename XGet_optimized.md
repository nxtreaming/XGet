# XGet ä¼˜åŒ–å®æ–½æ–¹æ¡ˆ - å®ç”¨ç‰ˆ

## é¡¹ç›®æ¦‚è¿°

åŸºäºæŠ€æœ¯éªŒè¯ç»“æœï¼Œè¿™æ˜¯ä¸€ä¸ª**å®ç”¨ä¼˜å…ˆã€é¿å…è¿‡åº¦è®¾è®¡**çš„X(Twitter)æ•°æ®é‡‡é›†ç³»ç»Ÿæ–¹æ¡ˆã€‚ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½çš„å¿«é€Ÿå®ç°å’Œç¨³å®šè¿è¡Œã€‚

## æ ¸å¿ƒåŸåˆ™

1. **ç®€å•æœ‰æ•ˆ** - ä¼˜å…ˆé€‰æ‹©æœ€ç®€å•å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ
2. **å¿«é€Ÿè¿­ä»£** - å…ˆå®ç°åŸºç¡€åŠŸèƒ½ï¼Œåç»­æŒ‰éœ€ä¼˜åŒ–
3. **å®ç”¨å¯¼å‘** - æ¯ä¸ªç»„ä»¶éƒ½æœ‰æ˜ç¡®çš„ä¸šåŠ¡ä»·å€¼
4. **æ˜“äºç»´æŠ¤** - ä»£ç ç»“æ„æ¸…æ™°ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹

## ğŸ¯ ç®€åŒ–æŠ€æœ¯æ ˆ

### æ ¸å¿ƒç»„ä»¶ï¼ˆMVPç‰ˆæœ¬ï¼‰
```
ğŸ“¦ XGeté¡¹ç›®ç»“æ„
â”œâ”€â”€ ğŸ Python 3.12 (å·²éªŒè¯)
â”œâ”€â”€ ğŸ”§ twscrape (æ•°æ®é‡‡é›†)
â”œâ”€â”€ ğŸ­ Playwright (cookiesç®¡ç†)
â”œâ”€â”€ ğŸ“ SQLite (æ•°æ®å­˜å‚¨)
â”œâ”€â”€ âš¡ FastAPI (APIæ¥å£)
â””â”€â”€ ğŸ“ Python logging (æ—¥å¿—)
```

### é¿å…çš„å¤æ‚ç»„ä»¶
- âŒ Redis/MongoDB (åˆæœŸä¸éœ€è¦)
- âŒ Celeryä»»åŠ¡é˜Ÿåˆ— (åŒæ­¥å¤„ç†è¶³å¤Ÿ)
- âŒ å¤æ‚ç›‘æ§ç³»ç»Ÿ (åŸºç¡€æ—¥å¿—å³å¯)
- âŒ å¾®æœåŠ¡æ¶æ„ (å•ä½“åº”ç”¨æ›´ç®€å•)
- âŒ å®¹å™¨åŒ–éƒ¨ç½² (ç›´æ¥è¿è¡Œæ›´å¿«)

## ğŸ—ï¸ ç®€åŒ–æ¶æ„è®¾è®¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              XGet ç®€åŒ–æ¶æ„                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸŒ APIå±‚      â”‚   ğŸ”§ ä¸šåŠ¡å±‚      â”‚  ğŸ’¾ æ•°æ®å±‚â”‚
â”‚   FastAPI       â”‚   é‡‡é›†ç®¡ç†       â”‚  SQLite  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
   ğŸª Playwright    ğŸ“Š twscrape    ğŸ“ Logging
   (è·å–cookies)    (æ•°æ®é‡‡é›†)     (è®°å½•æ—¥å¿—)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
xget/
â”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ config.py            # é…ç½®ç®¡ç†
â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹
â”œâ”€â”€ scraper.py           # é‡‡é›†æ ¸å¿ƒ
â”œâ”€â”€ auth_manager.py      # è®¤è¯ç®¡ç†
â”œâ”€â”€ api.py               # APIæ¥å£
â”œâ”€â”€ database.py          # æ•°æ®åº“æ“ä½œ
â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”œâ”€â”€ requirements.txt     # ä¾èµ–åŒ…
â”œâ”€â”€ config.yaml          # é…ç½®æ–‡ä»¶
â””â”€â”€ logs/                # æ—¥å¿—ç›®å½•
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. é…ç½®ç®¡ç† (config.py)
```python
from pydantic import BaseSettings
from typing import List

class Settings(BaseSettings):
    # åŸºç¡€é…ç½®
    debug: bool = False
    log_level: str = "INFO"
    
    # æ•°æ®åº“é…ç½®
    database_url: str = "sqlite:///xget.db"
    
    # APIé…ç½®
    api_host: str = "127.0.0.1"
    api_port: int = 8000
    
    # Twitterè´¦å·é…ç½®
    twitter_accounts: List[dict] = []
    
    # é‡‡é›†é…ç½®
    default_tweet_limit: int = 100
    request_delay: float = 1.0
    
    class Config:
        env_file = ".env"

settings = Settings()
```

### 2. æ•°æ®æ¨¡å‹ (models.py)
```python
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, List

class Tweet(BaseModel):
    id: str
    text: str
    user_id: str
    username: str
    created_at: datetime
    retweet_count: int = 0
    like_count: int = 0
    reply_count: int = 0
    collected_at: datetime

class User(BaseModel):
    user_id: str
    username: str
    display_name: str
    description: Optional[str] = None
    followers_count: int = 0
    following_count: int = 0
    tweet_count: int = 0
    collected_at: datetime

class SearchTask(BaseModel):
    id: Optional[int] = None
    keyword: str
    limit: int = 100
    status: str = "pending"  # pending, running, completed, failed
    created_at: datetime
    completed_at: Optional[datetime] = None
    result_count: int = 0
    error_message: Optional[str] = None
```

### 3. ç®€åŒ–é‡‡é›†å™¨ (scraper.py)
```python
import asyncio
import logging
from twscrape import API
from typing import List, Optional
from datetime import datetime

class SimpleTwitterScraper:
    """ç®€åŒ–ç‰ˆTwitteré‡‡é›†å™¨"""
    
    def __init__(self):
        self.api = API()
        self.logger = logging.getLogger(__name__)
    
    async def search_tweets(self, keyword: str, limit: int = 100) -> List[dict]:
        """æœç´¢æ¨æ–‡"""
        tweets = []
        try:
            async for tweet in self.api.search(keyword, limit=limit):
                tweet_data = {
                    'id': tweet.id,
                    'text': tweet.rawContent,
                    'user_id': tweet.user.id,
                    'username': tweet.user.username,
                    'created_at': tweet.date,
                    'retweet_count': tweet.retweetCount,
                    'like_count': tweet.likeCount,
                    'reply_count': tweet.replyCount,
                    'collected_at': datetime.utcnow()
                }
                tweets.append(tweet_data)
                
                # ç®€å•çš„å»¶è¿Ÿæ§åˆ¶
                await asyncio.sleep(0.5)
                
        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            raise
            
        return tweets
    
    async def get_user_info(self, username: str) -> Optional[dict]:
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            user = await self.api.user_by_login(username)
            if not user:
                return None
                
            return {
                'user_id': user.id,
                'username': user.username,
                'display_name': user.displayname,
                'description': user.description,
                'followers_count': user.followersCount,
                'following_count': user.followingCount,
                'tweet_count': user.statusesCount,
                'collected_at': datetime.utcnow()
            }
        except Exception as e:
            self.logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
            return None
```

### 4. ç®€åŒ–API (api.py)
```python
from fastapi import FastAPI, HTTPException
from typing import List
import asyncio

app = FastAPI(title="XGet API", version="1.0.0")

@app.get("/")
async def root():
    return {"message": "XGet API is running"}

@app.post("/search")
async def search_tweets(keyword: str, limit: int = 100):
    """æœç´¢æ¨æ–‡"""
    try:
        scraper = SimpleTwitterScraper()
        tweets = await scraper.search_tweets(keyword, limit)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        db = get_database()
        saved_count = db.save_tweets(tweets)
        
        return {
            "status": "success",
            "keyword": keyword,
            "found": len(tweets),
            "saved": saved_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/tweets")
async def get_tweets(limit: int = 50):
    """è·å–å·²é‡‡é›†çš„æ¨æ–‡"""
    db = get_database()
    tweets = db.get_tweets(limit)
    return {"tweets": tweets}

@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    db = get_database()
    stats = db.get_stats()
    return stats
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv_py312
source venv_py312/bin/activate  # Linux/Mac
# æˆ– venv_py312\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install twscrape playwright fastapi uvicorn pydantic sqlalchemy
playwright install chromium
```

### 2. é…ç½®è´¦å·
```bash
# æ·»åŠ Twitterè´¦å·
twscrape add_accounts accounts.txt
twscrape login_accounts
```

### 3. è¿è¡Œç¨‹åº
```bash
# å¯åŠ¨APIæœåŠ¡
python main.py

# æˆ–ç›´æ¥ä½¿ç”¨
uvicorn api:app --host 0.0.0.0 --port 8000
```

## ğŸ“ˆ åç»­ä¼˜åŒ–è·¯å¾„

### é˜¶æ®µ1ï¼šåŸºç¡€åŠŸèƒ½ (å½“å‰)
- âœ… åŸºç¡€æ•°æ®é‡‡é›†
- âœ… ç®€å•APIæ¥å£
- âœ… SQLiteå­˜å‚¨

### é˜¶æ®µ2ï¼šåŠŸèƒ½å¢å¼º
- ğŸ”„ æ·»åŠ ä»»åŠ¡é˜Ÿåˆ— (å¦‚éœ€è¦)
- ğŸ”„ æ”¹ç”¨PostgreSQL (æ•°æ®é‡å¤§æ—¶)
- ğŸ”„ æ·»åŠ åŸºç¡€ç›‘æ§

### é˜¶æ®µ3ï¼šç”Ÿäº§ä¼˜åŒ–
- ğŸ”„ å®¹å™¨åŒ–éƒ¨ç½²
- ğŸ”„ è´Ÿè½½å‡è¡¡
- ğŸ”„ é«˜å¯ç”¨æ¶æ„

## ğŸ’¡ å®æ–½å»ºè®®

1. **å…ˆè·‘èµ·æ¥** - ç”¨æœ€ç®€å•çš„æ–¹å¼å®ç°æ ¸å¿ƒåŠŸèƒ½
2. **é€æ­¥ä¼˜åŒ–** - æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µå†³å®šä¼˜åŒ–æ–¹å‘
3. **ç›‘æ§å…³é”®æŒ‡æ ‡** - é‡‡é›†æˆåŠŸç‡ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡
4. **ä¿æŒç®€å•** - ä¸è¦ä¸ºäº†æŠ€æœ¯è€ŒæŠ€æœ¯

è¿™ä¸ªæ–¹æ¡ˆçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å…ˆè®©ç³»ç»Ÿå·¥ä½œï¼Œå†è®©ç³»ç»Ÿå·¥ä½œå¾—æ›´å¥½**ã€‚

## ğŸ”§ å…³é”®å®ç°ç»†èŠ‚

### 5. æ•°æ®åº“æ“ä½œ (database.py)
```python
import sqlite3
from typing import List, Dict, Optional
from datetime import datetime
import json

class SimpleDatabase:
    """ç®€åŒ–æ•°æ®åº“æ“ä½œ"""

    def __init__(self, db_path: str = "xget.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ¨æ–‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tweets (
                id TEXT PRIMARY KEY,
                text TEXT NOT NULL,
                user_id TEXT,
                username TEXT,
                created_at TEXT,
                retweet_count INTEGER DEFAULT 0,
                like_count INTEGER DEFAULT 0,
                reply_count INTEGER DEFAULT 0,
                collected_at TEXT,
                raw_data TEXT
            )
        ''')

        # ç”¨æˆ·è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                user_id TEXT PRIMARY KEY,
                username TEXT,
                display_name TEXT,
                description TEXT,
                followers_count INTEGER DEFAULT 0,
                following_count INTEGER DEFAULT 0,
                tweet_count INTEGER DEFAULT 0,
                collected_at TEXT
            )
        ''')

        # æœç´¢ä»»åŠ¡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT NOT NULL,
                limit_count INTEGER DEFAULT 100,
                status TEXT DEFAULT 'pending',
                created_at TEXT,
                completed_at TEXT,
                result_count INTEGER DEFAULT 0,
                error_message TEXT
            )
        ''')

        conn.commit()
        conn.close()

    def save_tweets(self, tweets: List[Dict]) -> int:
        """ä¿å­˜æ¨æ–‡æ•°æ®"""
        if not tweets:
            return 0

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        saved_count = 0
        for tweet in tweets:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO tweets
                    (id, text, user_id, username, created_at, retweet_count,
                     like_count, reply_count, collected_at, raw_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    tweet['id'],
                    tweet['text'],
                    tweet['user_id'],
                    tweet['username'],
                    tweet['created_at'].isoformat() if isinstance(tweet['created_at'], datetime) else tweet['created_at'],
                    tweet.get('retweet_count', 0),
                    tweet.get('like_count', 0),
                    tweet.get('reply_count', 0),
                    tweet['collected_at'].isoformat() if isinstance(tweet['collected_at'], datetime) else tweet['collected_at'],
                    json.dumps(tweet)
                ))
                saved_count += 1
            except Exception as e:
                print(f"ä¿å­˜æ¨æ–‡å¤±è´¥: {e}")
                continue

        conn.commit()
        conn.close()
        return saved_count

    def get_tweets(self, limit: int = 50) -> List[Dict]:
        """è·å–æ¨æ–‡æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT id, text, username, created_at, retweet_count, like_count, reply_count
            FROM tweets
            ORDER BY collected_at DESC
            LIMIT ?
        ''', (limit,))

        tweets = []
        for row in cursor.fetchall():
            tweets.append({
                'id': row[0],
                'text': row[1],
                'username': row[2],
                'created_at': row[3],
                'retweet_count': row[4],
                'like_count': row[5],
                'reply_count': row[6]
            })

        conn.close()
        return tweets

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ¨æ–‡æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM tweets')
        tweet_count = cursor.fetchone()[0]

        # ç”¨æˆ·æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM users')
        user_count = cursor.fetchone()[0]

        # ä»Šæ—¥é‡‡é›†æ•°
        cursor.execute('''
            SELECT COUNT(*) FROM tweets
            WHERE date(collected_at) = date('now')
        ''')
        today_count = cursor.fetchone()[0]

        conn.close()

        return {
            'total_tweets': tweet_count,
            'total_users': user_count,
            'today_tweets': today_count
        }

# å…¨å±€æ•°æ®åº“å®ä¾‹
_db_instance = None

def get_database() -> SimpleDatabase:
    """è·å–æ•°æ®åº“å®ä¾‹"""
    global _db_instance
    if _db_instance is None:
        _db_instance = SimpleDatabase()
    return _db_instance
```

### 6. è®¤è¯ç®¡ç† (auth_manager.py)
```python
import asyncio
import logging
from playwright.async_api import async_playwright
from typing import Dict, List, Optional
import json
import os

class SimpleAuthManager:
    """ç®€åŒ–è®¤è¯ç®¡ç†"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.cookies_dir = "cookies"
        os.makedirs(self.cookies_dir, exist_ok=True)

    async def login_and_save_cookies(self, username: str, password: str, email: str) -> bool:
        """ç™»å½•å¹¶ä¿å­˜cookies"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=False)
                page = await browser.new_page()

                # è®¿é—®Twitterç™»å½•é¡µ
                await page.goto("https://twitter.com/login")
                await page.wait_for_timeout(3000)

                # è¾“å…¥ç”¨æˆ·å/é‚®ç®±
                await page.fill('input[name="text"]', username)
                await page.click('div[role="button"]:has-text("Next")')
                await page.wait_for_timeout(2000)

                # å¦‚æœéœ€è¦è¾“å…¥é‚®ç®±éªŒè¯
                try:
                    email_input = page.locator('input[name="text"]')
                    if await email_input.is_visible():
                        await email_input.fill(email)
                        await page.click('div[role="button"]:has-text("Next")')
                        await page.wait_for_timeout(2000)
                except:
                    pass

                # è¾“å…¥å¯†ç 
                await page.fill('input[name="password"]', password)
                await page.click('div[data-testid="LoginForm_Login_Button"]')

                # ç­‰å¾…ç™»å½•å®Œæˆ
                await page.wait_for_url("https://twitter.com/home", timeout=30000)

                # ä¿å­˜cookies
                cookies = await page.context.cookies()
                cookies_file = os.path.join(self.cookies_dir, f"{username}.json")
                with open(cookies_file, 'w') as f:
                    json.dump(cookies, f, indent=2)

                await browser.close()

                self.logger.info(f"è´¦å· {username} ç™»å½•æˆåŠŸï¼Œcookieså·²ä¿å­˜")
                return True

        except Exception as e:
            self.logger.error(f"è´¦å· {username} ç™»å½•å¤±è´¥: {e}")
            return False

    def load_cookies(self, username: str) -> Optional[List[Dict]]:
        """åŠ è½½cookies"""
        cookies_file = os.path.join(self.cookies_dir, f"{username}.json")
        if os.path.exists(cookies_file):
            with open(cookies_file, 'r') as f:
                return json.load(f)
        return None

    def list_available_accounts(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨è´¦å·"""
        accounts = []
        for file in os.listdir(self.cookies_dir):
            if file.endswith('.json'):
                accounts.append(file[:-5])  # ç§»é™¤.jsonåç¼€
        return accounts
```

### 7. ä¸»ç¨‹åº (main.py)
```python
import asyncio
import logging
import uvicorn
from config import settings
from api import app
from database import get_database

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/xget.log'),
        logging.StreamHandler()
    ]
)

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # åˆå§‹åŒ–æ•°æ®åº“
    db = get_database()
    logging.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    # å¯åŠ¨APIæœåŠ¡
    logging.info(f"å¯åŠ¨APIæœåŠ¡: http://{settings.api_host}:{settings.api_port}")
    uvicorn.run(
        app,
        host=settings.api_host,
        port=settings.api_port,
        log_level=settings.log_level.lower()
    )

if __name__ == "__main__":
    main()
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### å‘½ä»¤è¡Œä½¿ç”¨
```bash
# 1. å¯åŠ¨æœåŠ¡
python main.py

# 2. æœç´¢æ¨æ–‡
curl -X POST "http://localhost:8000/search?keyword=python&limit=50"

# 3. æŸ¥çœ‹ç»“æœ
curl "http://localhost:8000/tweets?limit=10"

# 4. æŸ¥çœ‹ç»Ÿè®¡
curl "http://localhost:8000/stats"
```

### Pythonè„šæœ¬ä½¿ç”¨
```python
import asyncio
from scraper import SimpleTwitterScraper
from database import get_database

async def example_usage():
    # åˆå§‹åŒ–
    scraper = SimpleTwitterScraper()
    db = get_database()

    # æœç´¢æ¨æ–‡
    tweets = await scraper.search_tweets("python", limit=20)
    print(f"æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")

    # ä¿å­˜åˆ°æ•°æ®åº“
    saved = db.save_tweets(tweets)
    print(f"ä¿å­˜äº† {saved} æ¡æ¨æ–‡")

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = db.get_stats()
    print(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    asyncio.run(example_usage())
```

## ğŸ“¦ ä¾èµ–æ–‡ä»¶

### requirements.txt
```
twscrape==0.17.0
playwright==1.53.0
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
httpx==0.28.1
aiofiles==23.2.1
python-multipart==0.0.6
```

### config.yaml (å¯é€‰é…ç½®æ–‡ä»¶)
```yaml
# XGet é…ç½®æ–‡ä»¶
app:
  name: "XGet"
  version: "1.0.0"
  debug: false

api:
  host: "127.0.0.1"
  port: 8000

database:
  path: "data/xget.db"

logging:
  level: "INFO"
  file: "logs/xget.log"

scraper:
  default_limit: 100
  request_delay: 1.0
  max_retries: 3

accounts:
  # åœ¨è¿™é‡Œé…ç½®Twitterè´¦å·ä¿¡æ¯
  # - username: "your_username"
  #   password: "your_password"
  #   email: "your_email@gmail.com"
```

### .env (ç¯å¢ƒå˜é‡æ–‡ä»¶)
```bash
# å¼€å‘ç¯å¢ƒé…ç½®
DEBUG=true
LOG_LEVEL=DEBUG
API_HOST=127.0.0.1
API_PORT=8000

# æ•°æ®åº“é…ç½®
DATABASE_URL=sqlite:///data/xget.db

# Twitteré…ç½® (å¯é€‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡twscrapeå‘½ä»¤è¡Œæ·»åŠ )
# TWITTER_USERNAME=your_username
# TWITTER_PASSWORD=your_password
# TWITTER_EMAIL=your_email@gmail.com
```

## ğŸš€ å¿«é€Ÿéƒ¨ç½²æŒ‡å—

### 1. æœ¬åœ°å¼€å‘éƒ¨ç½²
```bash
# å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir xget && cd xget

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv_py312
source venv_py312/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
playwright install chromium

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p logs data cookies

# é…ç½®è´¦å· (æ‰‹åŠ¨æ–¹å¼)
twscrape add_accounts accounts.txt
twscrape login_accounts

# å¯åŠ¨æœåŠ¡
python main.py
```

### 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² (ç®€åŒ–ç‰ˆ)
```bash
# ä½¿ç”¨systemdæœåŠ¡ (Linux)
sudo tee /etc/systemd/system/xget.service > /dev/null <<EOF
[Unit]
Description=XGet Twitter Scraper
After=network.target

[Service]
Type=simple
User=xget
WorkingDirectory=/opt/xget
Environment=PATH=/opt/xget/venv_py312/bin
ExecStart=/opt/xget/venv_py312/bin/python main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# å¯åŠ¨æœåŠ¡
sudo systemctl enable xget
sudo systemctl start xget
sudo systemctl status xget
```

### 3. Dockeréƒ¨ç½² (å¯é€‰)
```dockerfile
# Dockerfile
FROM python:3.12-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwright
RUN playwright install chromium
RUN playwright install-deps chromium

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p logs data cookies

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  xget:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./cookies:/app/cookies
    environment:
      - DEBUG=false
      - LOG_LEVEL=INFO
    restart: unless-stopped
```

## ğŸ”§ å®ç”¨å·¥å…·è„šæœ¬

### è´¦å·ç®¡ç†è„šæœ¬ (tools/account_manager.py)
```python
#!/usr/bin/env python3
"""è´¦å·ç®¡ç†å·¥å…·"""

import asyncio
import sys
from auth_manager import SimpleAuthManager

async def add_account():
    """æ·»åŠ æ–°è´¦å·"""
    username = input("è¯·è¾“å…¥ç”¨æˆ·å: ")
    password = input("è¯·è¾“å…¥å¯†ç : ")
    email = input("è¯·è¾“å…¥é‚®ç®±: ")

    auth_manager = SimpleAuthManager()
    success = await auth_manager.login_and_save_cookies(username, password, email)

    if success:
        print(f"âœ… è´¦å· {username} æ·»åŠ æˆåŠŸ")

        # åŒæ—¶æ·»åŠ åˆ°twscrape
        import subprocess
        result = subprocess.run([
            'twscrape', 'add_account', username, password, email, password
        ], capture_output=True, text=True)

        if result.returncode == 0:
            print("âœ… å·²åŒæ­¥åˆ°twscrape")
        else:
            print(f"âš ï¸ twscrapeåŒæ­¥å¤±è´¥: {result.stderr}")
    else:
        print(f"âŒ è´¦å· {username} æ·»åŠ å¤±è´¥")

def list_accounts():
    """åˆ—å‡ºæ‰€æœ‰è´¦å·"""
    auth_manager = SimpleAuthManager()
    accounts = auth_manager.list_available_accounts()

    if accounts:
        print("ğŸ“‹ å¯ç”¨è´¦å·åˆ—è¡¨:")
        for i, account in enumerate(accounts, 1):
            print(f"  {i}. {account}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨è´¦å·")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python account_manager.py add    # æ·»åŠ è´¦å·")
        print("  python account_manager.py list   # åˆ—å‡ºè´¦å·")
        sys.exit(1)

    command = sys.argv[1]
    if command == "add":
        asyncio.run(add_account())
    elif command == "list":
        list_accounts()
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")
```

### æ•°æ®å¯¼å‡ºè„šæœ¬ (tools/export_data.py)
```python
#!/usr/bin/env python3
"""æ•°æ®å¯¼å‡ºå·¥å…·"""

import csv
import json
import sys
from database import get_database

def export_to_csv(filename: str, limit: int = None):
    """å¯¼å‡ºåˆ°CSVæ–‡ä»¶"""
    db = get_database()
    tweets = db.get_tweets(limit or 10000)

    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['id', 'text', 'username', 'created_at', 'retweet_count', 'like_count', 'reply_count']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for tweet in tweets:
            writer.writerow(tweet)

    print(f"âœ… å·²å¯¼å‡º {len(tweets)} æ¡æ¨æ–‡åˆ° {filename}")

def export_to_json(filename: str, limit: int = None):
    """å¯¼å‡ºåˆ°JSONæ–‡ä»¶"""
    db = get_database()
    tweets = db.get_tweets(limit or 10000)

    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(tweets, jsonfile, ensure_ascii=False, indent=2)

    print(f"âœ… å·²å¯¼å‡º {len(tweets)} æ¡æ¨æ–‡åˆ° {filename}")

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python export_data.py csv output.csv [limit]")
        print("  python export_data.py json output.json [limit]")
        sys.exit(1)

    format_type = sys.argv[1]
    filename = sys.argv[2]
    limit = int(sys.argv[3]) if len(sys.argv) > 3 else None

    if format_type == "csv":
        export_to_csv(filename, limit)
    elif format_type == "json":
        export_to_json(filename, limit)
    else:
        print(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
```

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### ç®€å•çš„å¥åº·æ£€æŸ¥è„šæœ¬ (tools/health_check.py)
```python
#!/usr/bin/env python3
"""å¥åº·æ£€æŸ¥å·¥å…·"""

import requests
import sys
from datetime import datetime

def check_api_health(base_url: str = "http://localhost:8000"):
    """æ£€æŸ¥APIå¥åº·çŠ¶æ€"""
    try:
        # æ£€æŸ¥æ ¹è·¯å¾„
        response = requests.get(f"{base_url}/", timeout=5)
        if response.status_code == 200:
            print("âœ… APIæœåŠ¡æ­£å¸¸")
        else:
            print(f"âŒ APIæœåŠ¡å¼‚å¸¸: {response.status_code}")
            return False

        # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
        response = requests.get(f"{base_url}/stats", timeout=5)
        if response.status_code == 200:
            stats = response.json()
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {stats}")
        else:
            print("âš ï¸ æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")

        return True

    except requests.exceptions.RequestException as e:
        print(f"âŒ APIè¿æ¥å¤±è´¥: {e}")
        return False

def check_database_health():
    """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
    try:
        from database import get_database
        db = get_database()
        stats = db.get_stats()
        print(f"âœ… æ•°æ®åº“æ­£å¸¸: {stats}")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“å¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    print(f"ğŸ” XGetå¥åº·æ£€æŸ¥ - {datetime.now()}")
    print("-" * 40)

    api_ok = check_api_health()
    db_ok = check_database_health()

    if api_ok and db_ok:
        print("\nâœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        sys.exit(0)
    else:
        print("\nâŒ ç³»ç»Ÿå­˜åœ¨é—®é¢˜")
        sys.exit(1)
```

## ğŸ¯ æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜

### è´¦å·ç®¡ç†æœ€ä½³å®è·µ
```python
# ç®€å•çš„è´¦å·è½®æ¢ç­–ç•¥
class SimpleAccountRotator:
    def __init__(self):
        self.accounts = []
        self.current_index = 0
        self.load_accounts()

    def load_accounts(self):
        """åŠ è½½å¯ç”¨è´¦å·"""
        auth_manager = SimpleAuthManager()
        self.accounts = auth_manager.list_available_accounts()

    def get_next_account(self):
        """è·å–ä¸‹ä¸€ä¸ªè´¦å·"""
        if not self.accounts:
            return None

        account = self.accounts[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.accounts)
        return account
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
```python
import time
import random
from functools import wraps

def retry_on_error(max_retries=3, delay_range=(1, 5)):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e

                    delay = random.uniform(*delay_range)
                    print(f"é‡è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay:.1f}s: {e}")
                    await asyncio.sleep(delay)

        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
class RobustScraper(SimpleTwitterScraper):
    @retry_on_error(max_retries=3)
    async def search_tweets_robust(self, keyword: str, limit: int = 100):
        """å¸¦é‡è¯•çš„æœç´¢"""
        return await self.search_tweets(keyword, limit)
```

### æ•°æ®éªŒè¯å’Œæ¸…æ´—
```python
from pydantic import BaseModel, validator
import re

class CleanTweet(BaseModel):
    id: str
    text: str
    username: str
    created_at: str

    @validator('text')
    def clean_text(cls, v):
        """æ¸…ç†æ¨æ–‡æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        v = re.sub(r'\s+', ' ', v).strip()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é€‰ï¼‰
        # v = re.sub(r'[^\w\s#@]', '', v)
        return v

    @validator('username')
    def validate_username(cls, v):
        """éªŒè¯ç”¨æˆ·åæ ¼å¼"""
        if not v or not v.replace('_', '').isalnum():
            raise ValueError('Invalid username format')
        return v.lower()

def clean_tweet_data(raw_tweets):
    """æ¸…æ´—æ¨æ–‡æ•°æ®"""
    cleaned_tweets = []
    for tweet in raw_tweets:
        try:
            clean_tweet = CleanTweet(**tweet)
            cleaned_tweets.append(clean_tweet.dict())
        except Exception as e:
            print(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}, åŸå§‹æ•°æ®: {tweet}")
            continue
    return cleaned_tweets
```

## ğŸ”§ å®ç”¨å·¥å…·å’Œè„šæœ¬

### æ‰¹é‡æœç´¢è„šæœ¬
```python
# tools/batch_search.py
import asyncio
import csv
from scraper import SimpleTwitterScraper
from database import get_database

async def batch_search_from_file(keywords_file: str):
    """ä»æ–‡ä»¶æ‰¹é‡æœç´¢å…³é”®è¯"""
    scraper = SimpleTwitterScraper()
    db = get_database()

    # è¯»å–å…³é”®è¯
    with open(keywords_file, 'r', encoding='utf-8') as f:
        keywords = [line.strip() for line in f if line.strip()]

    print(f"å¼€å§‹æ‰¹é‡æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯...")

    for i, keyword in enumerate(keywords, 1):
        try:
            print(f"[{i}/{len(keywords)}] æœç´¢: {keyword}")
            tweets = await scraper.search_tweets(keyword, limit=50)
            saved = db.save_tweets(tweets)
            print(f"  âœ… æ‰¾åˆ° {len(tweets)} æ¡ï¼Œä¿å­˜ {saved} æ¡")

            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)

        except Exception as e:
            print(f"  âŒ æœç´¢å¤±è´¥: {e}")
            continue

    print("æ‰¹é‡æœç´¢å®Œæˆï¼")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python batch_search.py keywords.txt")
        sys.exit(1)

    asyncio.run(batch_search_from_file(sys.argv[1]))
```

### æ•°æ®ç»Ÿè®¡è„šæœ¬
```python
# tools/data_stats.py
import sqlite3
from collections import Counter
import matplotlib.pyplot as plt
import pandas as pd

def analyze_tweet_data(db_path="xget.db"):
    """åˆ†ææ¨æ–‡æ•°æ®"""
    conn = sqlite3.connect(db_path)

    # åŸºç¡€ç»Ÿè®¡
    df = pd.read_sql_query("""
        SELECT username, created_at, retweet_count, like_count,
               length(text) as text_length
        FROM tweets
    """, conn)

    print("ğŸ“Š æ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 40)
    print(f"æ€»æ¨æ–‡æ•°: {len(df)}")
    print(f"ç”¨æˆ·æ•°: {df['username'].nunique()}")
    print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {df['text_length'].mean():.1f}")
    print(f"å¹³å‡è½¬å‘æ•°: {df['retweet_count'].mean():.1f}")
    print(f"å¹³å‡ç‚¹èµæ•°: {df['like_count'].mean():.1f}")

    # çƒ­é—¨ç”¨æˆ·
    print("\nğŸ”¥ æœ€æ´»è·ƒç”¨æˆ· (Top 10):")
    top_users = df['username'].value_counts().head(10)
    for user, count in top_users.items():
        print(f"  {user}: {count} æ¡æ¨æ–‡")

    # æ—¶é—´åˆ†å¸ƒ
    df['hour'] = pd.to_datetime(df['created_at']).dt.hour
    print("\nâ° å‘æ¨æ—¶é—´åˆ†å¸ƒ:")
    hour_dist = df['hour'].value_counts().sort_index()
    for hour, count in hour_dist.items():
        print(f"  {hour:02d}:00 - {count} æ¡")

    conn.close()

if __name__ == "__main__":
    analyze_tweet_data()
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### ç®€å•çš„è¿›ç¨‹ç®¡ç†è„šæœ¬
```bash
#!/bin/bash
# scripts/xget_service.sh

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
PID_FILE="$PROJECT_DIR/xget.pid"
LOG_FILE="$PROJECT_DIR/logs/xget.log"

start() {
    if [ -f "$PID_FILE" ]; then
        echo "XGet å·²åœ¨è¿è¡Œ (PID: $(cat $PID_FILE))"
        return 1
    fi

    echo "å¯åŠ¨ XGet æœåŠ¡..."
    cd "$PROJECT_DIR"
    source venv_py312/bin/activate
    nohup python main.py > "$LOG_FILE" 2>&1 &
    echo $! > "$PID_FILE"
    echo "XGet æœåŠ¡å·²å¯åŠ¨ (PID: $!)"
}

stop() {
    if [ ! -f "$PID_FILE" ]; then
        echo "XGet æœåŠ¡æœªè¿è¡Œ"
        return 1
    fi

    PID=$(cat "$PID_FILE")
    echo "åœæ­¢ XGet æœåŠ¡ (PID: $PID)..."
    kill "$PID"
    rm -f "$PID_FILE"
    echo "XGet æœåŠ¡å·²åœæ­¢"
}

status() {
    if [ -f "$PID_FILE" ]; then
        PID=$(cat "$PID_FILE")
        if ps -p "$PID" > /dev/null; then
            echo "XGet æœåŠ¡è¿è¡Œä¸­ (PID: $PID)"
        else
            echo "XGet æœåŠ¡å¼‚å¸¸ (PIDæ–‡ä»¶å­˜åœ¨ä½†è¿›ç¨‹ä¸å­˜åœ¨)"
            rm -f "$PID_FILE"
        fi
    else
        echo "XGet æœåŠ¡æœªè¿è¡Œ"
    fi
}

case "$1" in
    start)   start ;;
    stop)    stop ;;
    restart) stop; sleep 2; start ;;
    status)  status ;;
    *)       echo "ä½¿ç”¨æ–¹æ³•: $0 {start|stop|restart|status}" ;;
esac
```

### æ—¥å¿—è½®è½¬é…ç½®
```bash
# /etc/logrotate.d/xget
/opt/xget/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 xget xget
    postrotate
        # é‡å¯æœåŠ¡ä»¥é‡æ–°æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        /opt/xget/scripts/xget_service.sh restart > /dev/null 2>&1 || true
    endscript
}
```

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### ç®€å•çš„ç›‘æ§è„šæœ¬
```python
# tools/monitor.py
import time
import requests
import smtplib
from email.mime.text import MIMEText
from datetime import datetime

class SimpleMonitor:
    def __init__(self, api_url="http://localhost:8000", check_interval=300):
        self.api_url = api_url
        self.check_interval = check_interval
        self.last_alert_time = 0
        self.alert_cooldown = 3600  # 1å°æ—¶å†…ä¸é‡å¤å‘Šè­¦

    def check_health(self):
        """å¥åº·æ£€æŸ¥"""
        try:
            response = requests.get(f"{self.api_url}/stats", timeout=10)
            if response.status_code == 200:
                stats = response.json()
                return True, stats
            else:
                return False, f"HTTP {response.status_code}"
        except Exception as e:
            return False, str(e)

    def send_alert(self, message):
        """å‘é€å‘Šè­¦ï¼ˆå¯ä»¥æ‰©å±•ä¸ºé‚®ä»¶ã€å¾®ä¿¡ç­‰ï¼‰"""
        current_time = time.time()
        if current_time - self.last_alert_time < self.alert_cooldown:
            return  # å†·å´æœŸå†…ä¸å‘é€

        print(f"ğŸš¨ ALERT: {message}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ é‚®ä»¶å‘é€é€»è¾‘
        self.last_alert_time = current_time

    def run(self):
        """è¿è¡Œç›‘æ§"""
        print(f"å¼€å§‹ç›‘æ§ XGet æœåŠ¡ ({self.api_url})")

        while True:
            try:
                is_healthy, result = self.check_health()

                if is_healthy:
                    print(f"âœ… {datetime.now()}: æœåŠ¡æ­£å¸¸ - {result}")
                else:
                    error_msg = f"æœåŠ¡å¼‚å¸¸: {result}"
                    print(f"âŒ {datetime.now()}: {error_msg}")
                    self.send_alert(error_msg)

                time.sleep(self.check_interval)

            except KeyboardInterrupt:
                print("\nç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(60)

if __name__ == "__main__":
    monitor = SimpleMonitor()
    monitor.run()
```

## ğŸ¯ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### Q1: è´¦å·ç™»å½•å¤±è´¥
**é—®é¢˜**: twscrape æˆ– Playwright ç™»å½• Twitter å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: é‡æ–°é…ç½®è´¦å·
twscrape delete_accounts
twscrape add_accounts accounts.txt
twscrape login_accounts

# æ–¹æ¡ˆ2: æ‰‹åŠ¨è·å– cookies
python tools/account_manager.py add

# æ–¹æ¡ˆ3: æ£€æŸ¥è´¦å·çŠ¶æ€
twscrape accounts
```

### Q2: æ•°æ®é‡‡é›†é€Ÿåº¦æ…¢æˆ–å¤±è´¥
**é—®é¢˜**: æœç´¢æ¨æ–‡æ—¶é€Ÿåº¦å¾ˆæ…¢æˆ–ç»å¸¸å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# è°ƒæ•´é‡‡é›†å‚æ•°
class OptimizedScraper(SimpleTwitterScraper):
    async def search_tweets_optimized(self, keyword: str, limit: int = 100):
        tweets = []
        batch_size = 20

        try:
            async for tweet in self.api.search(keyword, limit=limit):
                tweets.append(self.process_tweet(tweet))

                # æ¯æ‰¹æ¬¡åçŸ­æš‚ä¼‘æ¯
                if len(tweets) % batch_size == 0:
                    await asyncio.sleep(1)

        except Exception as e:
            self.logger.warning(f"æœç´¢ä¸­æ–­: {e}, å·²è·å– {len(tweets)} æ¡")

        return tweets
```

### Q3: æ•°æ®åº“æ–‡ä»¶è¿‡å¤§
**é—®é¢˜**: SQLite æ–‡ä»¶è¶Šæ¥è¶Šå¤§ï¼Œå½±å“æ€§èƒ½

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ•°æ®æ¸…ç†å’Œå‹ç¼©
def maintain_database():
    """æ•°æ®åº“ç»´æŠ¤"""
    conn = sqlite3.connect("xget.db")
    cursor = conn.cursor()

    # 1. åˆ é™¤30å¤©å‰çš„æ•°æ®
    cursor.execute('''
        DELETE FROM tweets
        WHERE date(collected_at) < date('now', '-30 days')
    ''')

    # 2. å‹ç¼©æ•°æ®åº“
    cursor.execute('VACUUM')

    # 3. é‡å»ºç´¢å¼•
    cursor.execute('REINDEX')

    conn.commit()
    conn.close()
    print("æ•°æ®åº“ç»´æŠ¤å®Œæˆ")

# å®šæœŸæ‰§è¡Œï¼ˆå¯ä»¥ç”¨ cron å®šæ—¶ä»»åŠ¡ï¼‰
# 0 2 * * 0 cd /opt/xget && python -c "from tools.maintenance import maintain_database; maintain_database()"
```

### Q4: API æœåŠ¡æ— å“åº”
**é—®é¢˜**: FastAPI æœåŠ¡å¯åŠ¨åæ— æ³•è®¿é—®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8000

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status
sudo ufw allow 8000

# æ£€æŸ¥æœåŠ¡æ—¥å¿—
tail -f logs/xget.log

# é‡å¯æœåŠ¡
./scripts/xget_service.sh restart
```

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: ç¯å¢ƒå‡†å¤‡
```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir xget && cd xget

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv_py312
source venv_py312/bin/activate

# å®‰è£…ä¾èµ–
pip install twscrape playwright fastapi uvicorn pydantic
playwright install chromium
```

### æ­¥éª¤2: åˆ›å»ºæ ¸å¿ƒæ–‡ä»¶
```bash
# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p logs data cookies tools scripts

# å¤åˆ¶ä¸Šé¢çš„ä»£ç åˆ°å¯¹åº”æ–‡ä»¶
# main.py, config.py, models.py, scraper.py,
# auth_manager.py, api.py, database.py
```

### æ­¥éª¤3: é…ç½®è´¦å·
```bash
# æ–¹æ³•1: ä½¿ç”¨ twscrape (æ¨è)
echo "username:password:email:email" > accounts.txt
twscrape add_accounts accounts.txt
twscrape login_accounts

# æ–¹æ³•2: æ‰‹åŠ¨è·å– cookies
python tools/account_manager.py add
```

### æ­¥éª¤4: å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨ API æœåŠ¡
python main.py

# æˆ–è€…åå°è¿è¡Œ
nohup python main.py > logs/xget.log 2>&1 &
```

### æ­¥éª¤5: æµ‹è¯•åŠŸèƒ½
```bash
# æµ‹è¯• API
curl "http://localhost:8000/"

# æœç´¢æ¨æ–‡
curl -X POST "http://localhost:8000/search?keyword=python&limit=10"

# æŸ¥çœ‹ç»“æœ
curl "http://localhost:8000/tweets?limit=5"

# æŸ¥çœ‹ç»Ÿè®¡
curl "http://localhost:8000/stats"
```

## ğŸ“‹ é¡¹ç›®æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥
- [ ] Python 3.12+ ç¯å¢ƒå·²å®‰è£…
- [ ] æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…
- [ ] Twitter è´¦å·å·²é…ç½®å¹¶æµ‹è¯•
- [ ] æ•°æ®åº“æ–‡ä»¶å¯æ­£å¸¸åˆ›å»º
- [ ] API æœåŠ¡å¯æ­£å¸¸å¯åŠ¨
- [ ] æ—¥å¿—ç›®å½•æœ‰å†™å…¥æƒé™

### è¿è¡Œä¸­ç›‘æ§
- [ ] API æœåŠ¡å“åº”æ­£å¸¸
- [ ] æ•°æ®é‡‡é›†åŠŸèƒ½æ­£å¸¸
- [ ] æ•°æ®åº“æ­£å¸¸å†™å…¥
- [ ] æ—¥å¿—æ–‡ä»¶æ­£å¸¸è®°å½•
- [ ] ç£ç›˜ç©ºé—´å……è¶³
- [ ] è´¦å·çŠ¶æ€æ­£å¸¸

### å®šæœŸç»´æŠ¤
- [ ] æ¯å‘¨æ£€æŸ¥è´¦å·çŠ¶æ€
- [ ] æ¯æœˆæ¸…ç†æ—§æ•°æ®
- [ ] æ¯æœˆå¤‡ä»½æ•°æ®åº“
- [ ] æ¯å­£åº¦æ›´æ–°ä¾èµ–åŒ…
- [ ] å®šæœŸæŸ¥çœ‹é”™è¯¯æ—¥å¿—

## ğŸ‰ æ€»ç»“

è¿™ä¸ª **XGet ä¼˜åŒ–å®æ–½æ–¹æ¡ˆ** çš„æ ¸å¿ƒç‰¹ç‚¹ï¼š

### âœ… ä¼˜åŠ¿
1. **ç®€å•å®ç”¨** - é¿å…è¿‡åº¦è®¾è®¡ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½
2. **å¿«é€Ÿä¸Šæ‰‹** - æœ€å°‘ä¾èµ–ï¼Œæœ€ç®€é…ç½®ï¼Œ5åˆ†é’Ÿå¯è¿è¡Œ
3. **æ˜“äºç»´æŠ¤** - æ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œå®Œå–„çš„å·¥å…·è„šæœ¬
4. **æ¸è¿›å‡çº§** - æ”¯æŒä»ç®€å•åˆ°å¤æ‚çš„å¹³æ»‘æ¼”è¿›
5. **ç”Ÿäº§å¯ç”¨** - åŒ…å«å¿…è¦çš„é”™è¯¯å¤„ç†ã€æ—¥å¿—ã€ç›‘æ§

### ğŸ¯ é€‚ç”¨åœºæ™¯
- **ä¸ªäººé¡¹ç›®** - å¿«é€Ÿæ­å»º Twitter æ•°æ®é‡‡é›†ç³»ç»Ÿ
- **å°å›¢é˜Ÿ** - ç®€å•çš„ç¤¾äº¤åª’ä½“æ•°æ®åˆ†æ
- **åŸå‹éªŒè¯** - å¿«é€ŸéªŒè¯æ•°æ®é‡‡é›†å¯è¡Œæ€§
- **å­¦ä¹ ç ”ç©¶** - äº†è§£ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›†æŠ€æœ¯

### ğŸ”„ å‡çº§è·¯å¾„
1. **ç¬¬ä¸€é˜¶æ®µ**: ä½¿ç”¨å½“å‰æ–¹æ¡ˆï¼ŒéªŒè¯æ ¸å¿ƒåŠŸèƒ½
2. **ç¬¬äºŒé˜¶æ®µ**: æ ¹æ®æ•°æ®é‡å’Œå¹¶å‘éœ€æ±‚ï¼Œå‡çº§æ•°æ®åº“å’Œç¼“å­˜
3. **ç¬¬ä¸‰é˜¶æ®µ**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œæ·»åŠ æ›´å¤šé«˜çº§åŠŸèƒ½

### ğŸ’¡ æ ¸å¿ƒç†å¿µ
**å…ˆè®©ç³»ç»Ÿå·¥ä½œï¼Œå†è®©ç³»ç»Ÿå·¥ä½œå¾—æ›´å¥½ï¼**

ä¸è¦ä¸€å¼€å§‹å°±è¿½æ±‚å®Œç¾çš„æ¶æ„ï¼Œè€Œæ˜¯ï¼š
1. å¿«é€Ÿå®ç°æ ¸å¿ƒåŠŸèƒ½
2. åœ¨ä½¿ç”¨ä¸­å‘ç°é—®é¢˜
3. é’ˆå¯¹æ€§åœ°ä¼˜åŒ–æ”¹è¿›
4. é€æ­¥æ¼”è¿›åˆ°ç†æƒ³çŠ¶æ€

è¿™æ ·æ—¢èƒ½å¿«é€Ÿçœ‹åˆ°æ•ˆæœï¼Œåˆèƒ½é¿å…è¿‡åº¦è®¾è®¡çš„é™·é˜±ã€‚ğŸš€

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è´¦å·ç®¡ç†å»ºè®®
- **å¤šè´¦å·è½®æ¢**: å‡†å¤‡3-5ä¸ªæµ‹è¯•è´¦å·ï¼Œé¿å…å•è´¦å·è¿‡åº¦ä½¿ç”¨
- **å®šæœŸæ£€æŸ¥**: æ¯å‘¨æ£€æŸ¥è´¦å·çŠ¶æ€ï¼ŒåŠæ—¶å¤„ç†å¼‚å¸¸
- **å¤‡ä»½cookies**: å®šæœŸå¤‡ä»½cookiesæ–‡ä»¶ï¼Œé¿å…é‡å¤ç™»å½•

### 2. æ•°æ®é‡‡é›†å»ºè®®
- **åˆç†é™é€Ÿ**: æ¯æ¬¡è¯·æ±‚é—´éš”1-2ç§’ï¼Œé¿å…è§¦å‘é™åˆ¶
- **å°æ‰¹é‡é‡‡é›†**: æ¯æ¬¡é‡‡é›†100-500æ¡æ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§å¤§é‡è¯·æ±‚
- **é”™è¯¯é‡è¯•**: è®¾ç½®åˆç†çš„é‡è¯•æœºåˆ¶ï¼Œä½†ä¸è¦æ— é™é‡è¯•

### 3. æ•°æ®å­˜å‚¨å»ºè®®
- **å®šæœŸå¤‡ä»½**: æ¯å¤©å¤‡ä»½SQLiteæ•°æ®åº“æ–‡ä»¶
- **æ•°æ®æ¸…ç†**: å®šæœŸæ¸…ç†è¿‡æœŸæˆ–æ— ç”¨æ•°æ®
- **ç´¢å¼•ä¼˜åŒ–**: ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•

### 4. ç›‘æ§å’Œç»´æŠ¤
- **æ—¥å¿—ç›‘æ§**: å®šæœŸæŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼Œå…³æ³¨é”™è¯¯ä¿¡æ¯
- **æ€§èƒ½ç›‘æ§**: ç›‘æ§APIå“åº”æ—¶é—´å’Œæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
- **èµ„æºç›‘æ§**: ç›‘æ§ç£ç›˜ç©ºé—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ

## â“ å¸¸è§é—®é¢˜è§£å†³

### Q1: twscrapeç™»å½•å¤±è´¥
```bash
# è§£å†³æ–¹æ¡ˆ1: é‡æ–°æ·»åŠ è´¦å·
twscrape delete_accounts
twscrape add_accounts accounts.txt
twscrape login_accounts

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨Playwrightæ‰‹åŠ¨è·å–cookies
python tools/account_manager.py add
```

### Q2: æ•°æ®é‡‡é›†é€Ÿåº¦æ…¢
```python
# ä¼˜åŒ–å»ºè®®: è°ƒæ•´å¹¶å‘å’Œå»¶è¿Ÿ
class OptimizedScraper(SimpleTwitterScraper):
    async def search_tweets_fast(self, keyword: str, limit: int = 100):
        """ä¼˜åŒ–ç‰ˆæœç´¢"""
        tweets = []
        batch_size = 20  # æ‰¹é‡å¤„ç†

        async for tweet in self.api.search(keyword, limit=limit):
            tweets.append(self.process_tweet(tweet))

            # æ¯20æ¡å¤„ç†ä¸€æ¬¡
            if len(tweets) % batch_size == 0:
                await asyncio.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ

        return tweets
```

### Q3: æ•°æ®åº“æ–‡ä»¶è¿‡å¤§
```python
# æ•°æ®æ¸…ç†è„šæœ¬
def cleanup_old_data(days: int = 30):
    """æ¸…ç†Nå¤©å‰çš„æ•°æ®"""
    conn = sqlite3.connect("xget.db")
    cursor = conn.cursor()

    cursor.execute('''
        DELETE FROM tweets
        WHERE date(collected_at) < date('now', '-{} days')
    '''.format(days))

    deleted = cursor.rowcount
    conn.commit()
    conn.close()

    print(f"æ¸…ç†äº† {deleted} æ¡æ—§æ•°æ®")
```

### Q4: APIæœåŠ¡æ— å“åº”
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
python tools/health_check.py

# é‡å¯æœåŠ¡
pkill -f "python main.py"
python main.py &

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/xget.log
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åº“ä¼˜åŒ–
```sql
-- æ·»åŠ ç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_tweets_username ON tweets(username);
CREATE INDEX idx_tweets_created_at ON tweets(created_at);
CREATE INDEX idx_tweets_collected_at ON tweets(collected_at);
```

### 2. å†…å­˜ä¼˜åŒ–
```python
# ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜å ç”¨
async def search_tweets_generator(self, keyword: str, limit: int = 100):
    """ç”Ÿæˆå™¨ç‰ˆæœ¬ï¼ŒèŠ‚çœå†…å­˜"""
    async for tweet in self.api.search(keyword, limit=limit):
        yield self.process_tweet(tweet)
        await asyncio.sleep(0.5)
```

### 3. å¹¶å‘ä¼˜åŒ–
```python
# é€‚åº¦å¹¶å‘å¤„ç†
import asyncio
from asyncio import Semaphore

class ConcurrentScraper:
    def __init__(self, max_concurrent: int = 3):
        self.semaphore = Semaphore(max_concurrent)

    async def search_multiple_keywords(self, keywords: List[str]):
        """å¹¶å‘æœç´¢å¤šä¸ªå…³é”®è¯"""
        tasks = []
        for keyword in keywords:
            task = self.search_with_semaphore(keyword)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results

    async def search_with_semaphore(self, keyword: str):
        async with self.semaphore:
            return await self.search_tweets(keyword)
```

## ğŸ”„ å‡çº§è·¯å¾„

### ä»SQLiteåˆ°PostgreSQL
```python
# æ•°æ®è¿ç§»è„šæœ¬
def migrate_to_postgresql():
    """è¿ç§»æ•°æ®åˆ°PostgreSQL"""
    # 1. å¯¼å‡ºSQLiteæ•°æ®
    sqlite_db = SimpleDatabase("xget.db")
    tweets = sqlite_db.get_tweets(limit=None)

    # 2. å¯¼å…¥åˆ°PostgreSQL
    # (éœ€è¦å…ˆå®‰è£… psycopg2-binary)
    import psycopg2
    conn = psycopg2.connect(
        host="localhost",
        database="xget",
        user="xget_user",
        password="password"
    )

    # 3. æ‰¹é‡æ’å…¥æ•°æ®
    # ... å®ç°æ•°æ®è¿ç§»é€»è¾‘
```

### æ·»åŠ Redisç¼“å­˜
```python
# ç¼“å­˜å±‚
import redis

class CachedScraper(SimpleTwitterScraper):
    def __init__(self):
        super().__init__()
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)

    async def search_tweets_cached(self, keyword: str, limit: int = 100):
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        cache_key = f"search:{keyword}:{limit}"

        # æ£€æŸ¥ç¼“å­˜
        cached = self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)

        # æ‰§è¡Œæœç´¢
        tweets = await self.search_tweets(keyword, limit)

        # ç¼“å­˜ç»“æœï¼ˆ1å°æ—¶è¿‡æœŸï¼‰
        self.redis_client.setex(
            cache_key,
            3600,
            json.dumps(tweets, default=str)
        )

        return tweets
```

## ğŸ‰ æ€»ç»“

è¿™ä¸ªä¼˜åŒ–æ–¹æ¡ˆçš„æ ¸å¿ƒç‰¹ç‚¹ï¼š

1. **ç®€å•å®ç”¨** - é¿å…è¿‡åº¦è®¾è®¡ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½
2. **å¿«é€Ÿä¸Šæ‰‹** - æœ€å°‘çš„ä¾èµ–ï¼Œæœ€ç®€çš„é…ç½®
3. **æ˜“äºç»´æŠ¤** - æ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œå®Œå–„çš„å·¥å…·è„šæœ¬
4. **æ¸è¿›å‡çº§** - æ”¯æŒä»ç®€å•åˆ°å¤æ‚çš„å¹³æ»‘æ¼”è¿›

**ç«‹å³å¼€å§‹çš„æ­¥éª¤**ï¼š
1. å¤åˆ¶ä»£ç æ–‡ä»¶
2. å®‰è£…ä¾èµ–åŒ…
3. é…ç½®Twitterè´¦å·
4. å¯åŠ¨æœåŠ¡æµ‹è¯•
5. æ ¹æ®éœ€è¦é€æ­¥ä¼˜åŒ–

è®°ä½ï¼š**å…ˆè®©å®ƒå·¥ä½œï¼Œå†è®©å®ƒå·¥ä½œå¾—æ›´å¥½ï¼** ğŸš€
