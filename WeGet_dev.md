# WeGet X(Twitter)æ•°æ®é‡‡é›†ç³»ç»Ÿ - ä¼ä¸šçº§æŠ€æœ¯å®æ–½æ–¹æ¡ˆ

## é¡¹ç›®æ¦‚è¿°

åŸºäºç°ä»£äº‘åŸç”Ÿæ¶æ„æ„å»ºçš„å¤§è§„æ¨¡X(Twitter)æ•°æ®é‡‡é›†ç³»ç»Ÿï¼Œæ”¯æŒå…³é”®è¯æœç´¢ã€ç”¨æˆ·ä¸»é¡µã€è´¦å·ä¿¡æ¯å’Œå•æ¡å¸–å­çš„å…¨æ–¹ä½æ•°æ®é‡‡é›†ã€‚ç³»ç»Ÿé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒ5000+ä»£ç†IPå’Œæ•°åƒè´¦å·çš„åˆ†å¸ƒå¼çˆ¬å–ï¼Œå…·å¤‡ä¼ä¸šçº§å¯è§‚æµ‹æ€§ã€å®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚

**é‡è¦è¯´æ˜**:
- æœ¬ç³»ç»Ÿé’ˆå¯¹X.com(åŸTwitter.com)è¿›è¡Œç½‘ç»œæŠ“å–ï¼Œé‡‡ç”¨æµè§ˆå™¨æ± åŒ–æŠ€æœ¯å’Œåˆ†å¸ƒå¼æ¶æ„
- é‡‡ç”¨çº¯ç½‘ç»œæŠ“å–æ–¹æ³•ï¼Œä¸ä½¿ç”¨ä»»ä½•å®˜æ–¹APIï¼Œé€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–å’Œè¯·æ±‚æ‹¦æˆªè·å–æ•°æ®
- ç³»ç»Ÿå…·å¤‡å®Œæ•´çš„æ•°æ®æ²»ç†ã€å®‰å…¨å®¡è®¡å’ŒGDPRåˆè§„èƒ½åŠ›

## æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. æœç´¢é‡‡é›†æ¨¡å—
- **åŠŸèƒ½**: æ ¹æ®å…³é”®è¯æœç´¢è´´æ–‡å’Œè¯„è®º
- **æŠ€æœ¯æ–¹æ¡ˆ**: æµè§ˆå™¨è‡ªåŠ¨åŒ–è®¿é—®Xæœç´¢é¡µé¢ï¼Œç»“åˆç½‘ç»œè¯·æ±‚æ‹¦æˆªè·å–æ•°æ®
- **å®ç°è¦ç‚¹**:
  - æ”¯æŒå¤šç§æœç´¢ç±»å‹ï¼šLatestã€Topã€Peopleã€Photosã€Videos
  - æ™ºèƒ½æ»šåŠ¨åŠ è½½è·å–å®Œæ•´æœç´¢ç»“æœ
  - å®æ—¶é‡‡é›†æœç´¢ç»“æœä¸­çš„äº’åŠ¨æ•°æ®
  - åŒé‡æ•°æ®æºï¼šé¡µé¢DOMè§£æ + åå°APIæ‹¦æˆª

### 2. ä¸»é¡µé‡‡é›†æ¨¡å—
- **åŠŸèƒ½**: é‡‡é›†æŒ‡å®šç”¨æˆ·ä¸»é¡µä¿¡æ¯ã€å¸–å­åˆ—è¡¨ã€äº’åŠ¨æ•°æ®
- **æŠ€æœ¯æ–¹æ¡ˆ**: æµè§ˆå™¨è®¿é—®ç”¨æˆ·ä¸»é¡µï¼Œæ‹¦æˆªåå°æ•°æ®è¯·æ±‚
- **å®ç°è¦ç‚¹**:
  - ç”¨æˆ·åŸºç¡€ä¿¡æ¯ï¼šå¤´åƒã€ç®€ä»‹ã€å…³æ³¨æ•°ã€ç²‰ä¸æ•°
  - å¸–å­åˆ—è¡¨ï¼šå†…å®¹ã€å‘å¸ƒæ—¶é—´ã€åª’ä½“æ–‡ä»¶
  - äº’åŠ¨æ•°æ®ï¼šç‚¹èµã€è½¬å‘ã€è¯„è®ºã€æµè§ˆé‡
  - è§†é¢‘æ•°æ®ï¼šæ’­æ”¾æ¬¡æ•°ã€æ—¶é•¿ã€ç¼©ç•¥å›¾
  - è‡ªåŠ¨æ»šåŠ¨åŠ è½½å†å²æ¨æ–‡

### 3. è´¦å·ä¿¡æ¯é‡‡é›†æ¨¡å—
- **åŠŸèƒ½**: æ·±åº¦é‡‡é›†æŒ‡å®šè´¦å·çš„è¯¦ç»†ä¿¡æ¯
- **æŠ€æœ¯æ–¹æ¡ˆ**: ç»¼åˆå¤šä¸ªé¡µé¢è®¿é—®ï¼Œæ‹¦æˆªåå°æ•°æ®è·å–å®Œæ•´ç”¨æˆ·ç”»åƒ
- **å®ç°è¦ç‚¹**:
  - åŸºç¡€ä¿¡æ¯ï¼šç”¨æˆ·åã€handleã€è®¤è¯çŠ¶æ€ã€æ³¨å†Œæ—¶é—´
  - ç¤¾äº¤å…³ç³»ï¼šå…³æ³¨åˆ—è¡¨ã€ç²‰ä¸åˆ—è¡¨ã€äº’åŠ¨å…³ç³»
  - æ´»è·ƒåº¦åˆ†æï¼šå‘å¸–é¢‘ç‡ã€äº’åŠ¨æ¨¡å¼
  - åª’ä½“å†…å®¹ï¼šå¤´åƒé«˜æ¸…ç‰ˆã€èƒŒæ™¯å›¾
  - å…³æ³¨è€…/å…³æ³¨åˆ—è¡¨çš„æ‰¹é‡é‡‡é›†

### 4. å•æ¡å¸–å­é‡‡é›†æ¨¡å—
- **åŠŸèƒ½**: é‡‡é›†æŒ‡å®šå¸–å­çš„å®Œæ•´æ•°æ®å’Œäº’åŠ¨ä¿¡æ¯
- **æŠ€æœ¯æ–¹æ¡ˆ**: è®¿é—®æ¨æ–‡è¯¦æƒ…é¡µé¢ï¼Œæ‹¦æˆªåå°æ•°æ®è¯·æ±‚
- **å®ç°è¦ç‚¹**:
  - å¸–å­å®Œæ•´å†…å®¹ï¼šæ–‡æœ¬ã€åª’ä½“ã€é“¾æ¥
  - äº’åŠ¨ç»Ÿè®¡ï¼šå®æ—¶ç‚¹èµã€è½¬å‘ã€è¯„è®ºã€æµè§ˆæ•°
  - è¯„è®ºæ ‘ï¼šå®Œæ•´è¯„è®ºé“¾å’Œå›å¤å…³ç³»
  - è§†é¢‘æ•°æ®ï¼šæ’­æ”¾ç»Ÿè®¡ã€è§‚çœ‹æ—¶é•¿åˆ†å¸ƒ
  - å¼•ç”¨æ¨æ–‡å’Œè½¬å‘é“¾çš„å®Œæ•´è¿½è¸ª

## æŠ€æœ¯æ¶æ„è®¾è®¡

### ğŸ¯ æŠ€æœ¯æ ¸å¿ƒæŠ€æœ¯æ ˆé€‰æ‹©ï¼ˆåŸºäº2025å¹´æœ€ä½³å®è·µï¼‰

**æ ¸å¿ƒæŠ€æœ¯æ ˆ** - Python å•æ ˆæ–¹æ¡ˆï¼š
- **æŠ“å–å¼•æ“**: `twscrape` (ä¸»åŠ›) + `httpx` (ç½‘ç»œå±‚)
- **åŠ¨æ€æ¸²æŸ“**: `Playwright` æ± åŒ–æœåŠ¡
- **å¼‚æ­¥æ¡†æ¶**: `asyncio` + `uvloop` (é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯)
- **ä»»åŠ¡è°ƒåº¦**: `Celery` + `Redis` (â‰¤åƒä¸‡æ¡/æ—¥)
- **APIæœåŠ¡**: `FastAPI` + `Uvicorn`
- **æ•°æ®å­˜å‚¨**: `MongoDB` (æ–‡æ¡£) + `Redis` (ç¼“å­˜/é˜Ÿåˆ—)

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç®¡ç†æ§åˆ¶å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ä»»åŠ¡è°ƒåº¦       â”‚   è´¦å·æ± ç®¡ç†     â”‚   ä»£ç†æ± ç®¡ç†             â”‚
â”‚   (Celery)      â”‚   (Redis)      â”‚   (Redis)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  twscrape     â”‚    â”‚  Playwright   â”‚    â”‚  FastAPI      â”‚
â”‚  çˆ¬è™«èŠ‚ç‚¹      â”‚    â”‚  æ¸²æŸ“æ±         â”‚    â”‚  APIæœåŠ¡      â”‚
â”‚  (httpx)      â”‚    â”‚  (Tokenåˆ·æ–°)   â”‚    â”‚  (æ•°æ®æ¥å£)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ•°æ®å­˜å‚¨å±‚     â”‚
                    â”‚ MongoDB + Redis â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## è¯¦ç»†å®æ–½è®¡åˆ’

### é˜¶æ®µä¸€ï¼šåŸºç¡€æ¶æ„æ­å»º (1-2å‘¨)

#### 1.1 ç¯å¢ƒå‡†å¤‡
```bash
# åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir -p weget/{core,modules,utils,config,data,logs,tests}
cd weget

# åˆ›å»º .gitignore æ–‡ä»¶
cat > .gitignore << 'EOF'
# è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ - è¯·å‹¿æ‰‹åŠ¨ç¼–è¾‘
docker-compose.dev.yml
docker-compose.prod.yml
generated/

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# ç¯å¢ƒå˜é‡å’Œå¯†é’¥
.env
.env.local
.env.dev
.env.prod
secrets/
*.key
*.pem

# æ•°æ®å’Œæ—¥å¿—
data/
logs/
*.log

# IDE
.vscode/
.idea/
*.swp
*.swo

# æµ‹è¯•
.coverage
.pytest_cache/
htmlcov/

# ä¸´æ—¶æ–‡ä»¶
*.tmp
*.bak
.DS_Store
Thumbs.db
EOF

# å®‰è£…ç®€åŒ–ä¾èµ–æ ˆ
pip install twscrape httpx playwright celery redis pymongo fastapi uvicorn uvloop
playwright install chromium

```

#### 1.2 Cookieæ± ç®¡ç†ç³»ç»Ÿ
```python
# core/cookie_manager.py
import redis.asyncio as redis
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import asyncio

logger = logging.getLogger(__name__)

class CookieManager:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self._account_cache = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    async def get_available_cookie(self, account_type='random') -> Tuple[str, Dict]:
        """è·å–å¯ç”¨Cookie - å®Œå…¨å¼‚æ­¥å®ç°"""
        try:
            # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è·å–å¯ç”¨è´¦å·
            available_accounts = await self.redis.smembers('accounts:available')
            if not available_accounts:
                raise Exception("No available accounts")

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶é€‰æ‹©è´¦å·
            account_list = list(available_accounts)
            account_id = account_list[0] if account_list else None

            if not account_id:
                raise Exception("No account ID available")

            # å¼‚æ­¥è·å–Cookieæ•°æ®
            cookie_data = await self.redis.hget(f'account:{account_id}', 'cookies')
            parsed_cookies = json.loads(cookie_data) if cookie_data else {}

            return account_id, parsed_cookies

        except Exception as e:
            logger.error(f"Failed to get available cookie: {str(e)}")
            raise

    async def update_cookie(self, account_id: str, cookie_data: Dict):
        """æ›´æ–°Cookieä¿¡æ¯ - ä½¿ç”¨å¼‚æ­¥Redisæ“ä½œ"""
        try:
            # ä½¿ç”¨å¼‚æ­¥ç®¡é“æé«˜æ€§èƒ½
            async with self.redis.pipeline() as pipe:
                await pipe.hset(f'account:{account_id}', 'cookies', json.dumps(cookie_data))
                await pipe.hset(f'account:{account_id}', 'last_updated', datetime.utcnow().isoformat())
                await pipe.execute()

            logger.debug(f"Updated cookies for account {account_id}")

        except Exception as e:
            logger.error(f"Failed to update cookie for account {account_id}: {str(e)}")
            raise

    async def mark_invalid(self, account_id: str, reason: str):
        """æ ‡è®°æ— æ•ˆè´¦å· - ä½¿ç”¨å¼‚æ­¥Redisæ“ä½œ"""
        try:
            # ä½¿ç”¨å¼‚æ­¥ç®¡é“ç¡®ä¿åŸå­æ€§
            async with self.redis.pipeline() as pipe:
                await pipe.srem('accounts:available', account_id)
                await pipe.sadd('accounts:banned', account_id)
                await pipe.hset(f'account:{account_id}', 'banned_reason', reason)
                await pipe.hset(f'account:{account_id}', 'banned_at', datetime.utcnow().isoformat())
                await pipe.execute()

            logger.warning(f"Marked account {account_id} as invalid: {reason}")

        except Exception as e:
            logger.error(f"Failed to mark account {account_id} as invalid: {str(e)}")
            raise

    async def _get_available_accounts(self) -> List[str]:
        """å¼‚æ­¥è·å–å¯ç”¨è´¦å·åˆ—è¡¨ - è§£å†³scan_iteré˜»å¡é—®é¢˜"""
        try:
            # ä½¿ç”¨å¼‚æ­¥scan_iteré¿å…é˜»å¡
            accounts = []
            async for key in self.redis.scan_iter(match='account:*:status', count=100):
                # æ£€æŸ¥è´¦å·çŠ¶æ€
                status = await self.redis.get(key)
                if status and status.decode() == 'available':
                    account_id = key.decode().split(':')[1]
                    accounts.append(account_id)

            return accounts

        except Exception as e:
            logger.error(f"Failed to get available accounts: {str(e)}")
            return []

    async def get_account_health_score(self, account_id: str) -> float:
        """è·å–è´¦å·å¥åº·åˆ†æ•° - å¼‚æ­¥å®ç°"""
        try:
            # å¼‚æ­¥è·å–è´¦å·ç»Ÿè®¡æ•°æ®
            account_data = await self.redis.hgetall(f'account:{account_id}')

            if not account_data:
                return 0.0

            # è®¡ç®—å¥åº·åˆ†æ•°
            total_requests = int(account_data.get('total_requests', 1))
            error_count = int(account_data.get('error_count', 0))
            last_used = account_data.get('last_used')

            # åŸºç¡€åˆ†æ•°
            error_rate = error_count / total_requests if total_requests > 0 else 0
            base_score = max(0, 1.0 - error_rate)

            # æ—¶é—´è¡°å‡
            if last_used:
                try:
                    last_used_time = datetime.fromisoformat(last_used.decode())
                    hours_since_use = (datetime.utcnow() - last_used_time).total_seconds() / 3600
                    time_factor = max(0.5, 1.0 - (hours_since_use / 24))  # 24å°æ—¶å†…çº¿æ€§è¡°å‡
                    base_score *= time_factor
                except ValueError:
                    pass

            return min(1.0, max(0.0, base_score))

        except Exception as e:
            logger.error(f"Failed to calculate health score for account {account_id}: {str(e)}")
            return 0.0

    async def refresh_account_pool(self):
        """åˆ·æ–°è´¦å·æ±  - å¼‚æ­¥æ‰¹é‡æ“ä½œ"""
        try:
            # è·å–æ‰€æœ‰è´¦å·
            all_accounts = await self._get_available_accounts()

            # æ‰¹é‡æ£€æŸ¥è´¦å·å¥åº·çŠ¶æ€
            healthy_accounts = []
            unhealthy_accounts = []

            # ä½¿ç”¨å¼‚æ­¥å¹¶å‘æ£€æŸ¥
            tasks = [self.get_account_health_score(account_id) for account_id in all_accounts]
            health_scores = await asyncio.gather(*tasks, return_exceptions=True)

            for account_id, score in zip(all_accounts, health_scores):
                if isinstance(score, Exception):
                    logger.warning(f"Failed to check health for account {account_id}: {score}")
                    continue

                if score > 0.7:  # å¥åº·é˜ˆå€¼
                    healthy_accounts.append(account_id)
                else:
                    unhealthy_accounts.append(account_id)

            # æ‰¹é‡æ›´æ–°Redis
            if healthy_accounts or unhealthy_accounts:
                async with self.redis.pipeline() as pipe:
                    # æ¸…ç©ºç°æœ‰é›†åˆ
                    await pipe.delete('accounts:available', 'accounts:unhealthy')

                    # æ·»åŠ å¥åº·è´¦å·
                    if healthy_accounts:
                        await pipe.sadd('accounts:available', *healthy_accounts)

                    # æ·»åŠ ä¸å¥åº·è´¦å·
                    if unhealthy_accounts:
                        await pipe.sadd('accounts:unhealthy', *unhealthy_accounts)

                    await pipe.execute()

            logger.info(f"Refreshed account pool: {len(healthy_accounts)} healthy, {len(unhealthy_accounts)} unhealthy")

        except Exception as e:
            logger.error(f"Failed to refresh account pool: {str(e)}")
            raise
```

#### 1.3 ä»£ç†IPæ± ç®¡ç†
```python
# core/proxy_manager.py
import redis.asyncio as redis
import aiohttp
import json
import logging
import asyncio
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)

class ProxyManager:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self._health_check_interval = 60  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        self._proxy_cache = {}

    async def get_proxy(self, account_id: Optional[str] = None) -> Optional[Dict]:
        """è·å–ä»£ç†IPï¼Œæ”¯æŒè´¦å·ç»‘å®š - å®Œå…¨å¼‚æ­¥å®ç°"""
        try:
            # å¼‚æ­¥è·å–å¥åº·çš„ä»£ç†åˆ—è¡¨
            healthy_proxies = await self.redis.smembers('proxies:healthy')
            if not healthy_proxies:
                raise Exception("No healthy proxies available")

            # å¦‚æœæŒ‡å®šè´¦å·ï¼Œå°è¯•è·å–ç»‘å®šçš„ä»£ç†
            if account_id:
                bound_proxy = await self.redis.get(f'account:{account_id}:proxy')
                if bound_proxy and bound_proxy in healthy_proxies:
                    proxy_config = await self.redis.hget(f'proxy:{bound_proxy.decode()}', 'config')
                    if proxy_config:
                        return json.loads(proxy_config)

            # éšæœºé€‰æ‹©ä¸€ä¸ªå¥åº·çš„ä»£ç†
            proxy_list = list(healthy_proxies)
            if not proxy_list:
                return None

            proxy_id = random.choice(proxy_list).decode()
            proxy_config = await self.redis.hget(f'proxy:{proxy_id}', 'config')

            if proxy_config:
                return json.loads(proxy_config)
            return None

        except Exception as e:
            logger.error(f"Failed to get proxy: {str(e)}")
            return None

    async def check_proxy_health(self, proxy: Dict) -> bool:
        """æ£€æµ‹ä»£ç†å¯ç”¨æ€§ - å¼‚æ­¥å¥åº·æ£€æŸ¥"""
        test_urls = ['https://httpbin.org/ip', 'https://x.com/robots.txt']

        try:
            timeout = aiohttp.ClientTimeout(total=10)
            connector = aiohttp.TCPConnector(limit=1)

            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                for url in test_urls:
                    try:
                        async with session.get(url, proxy=proxy['url']) as response:
                            if response.status != 200:
                                return False
                    except Exception as e:
                        logger.debug(f"Proxy health check failed for {proxy.get('id', 'unknown')}: {str(e)}")
                        return False

                return True

        except Exception as e:
            logger.warning(f"Proxy health check error: {str(e)}")
            return False

    async def rotate_proxy(self, current_proxy: Dict) -> Optional[Dict]:
        """è½®æ¢ä»£ç†IP - å¼‚æ­¥æ“ä½œ"""
        try:
            # æ ‡è®°å½“å‰ä»£ç†ä¸ºéœ€è¦è½®æ¢
            proxy_id = current_proxy.get('id')
            if proxy_id:
                async with self.redis.pipeline() as pipe:
                    await pipe.srem('proxies:healthy', proxy_id)
                    await pipe.sadd('proxies:rotating', proxy_id)
                    await pipe.setex(f'proxy:{proxy_id}:cooldown', 300, 'true')  # 5åˆ†é’Ÿå†·å´
                    await pipe.execute()

            # è·å–æ–°çš„ä»£ç†
            return await self.get_proxy()

        except Exception as e:
            logger.error(f"Failed to rotate proxy: {str(e)}")
            return None

    async def batch_health_check(self) -> Dict[str, int]:
        """æ‰¹é‡å¥åº·æ£€æŸ¥ - å¼‚æ­¥å¹¶å‘æ£€æŸ¥"""
        try:
            # è·å–æ‰€æœ‰ä»£ç†
            all_proxies = await self.redis.smembers('proxies:all')
            if not all_proxies:
                return {'healthy': 0, 'unhealthy': 0, 'total': 0}

            # å¹¶å‘å¥åº·æ£€æŸ¥
            proxy_configs = []
            for proxy_id in all_proxies:
                config = await self.redis.hget(f'proxy:{proxy_id.decode()}', 'config')
                if config:
                    proxy_data = json.loads(config)
                    proxy_data['id'] = proxy_id.decode()
                    proxy_configs.append(proxy_data)

            # é™åˆ¶å¹¶å‘æ•°é¿å…è¿‡è½½
            semaphore = asyncio.Semaphore(10)

            async def check_with_semaphore(proxy):
                async with semaphore:
                    return await self.check_proxy_health(proxy)

            # å¹¶å‘æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_results = await asyncio.gather(
                *[check_with_semaphore(proxy) for proxy in proxy_configs],
                return_exceptions=True
            )

            # åˆ†ç±»ç»“æœ
            healthy_proxies = []
            unhealthy_proxies = []

            for proxy, is_healthy in zip(proxy_configs, health_results):
                if isinstance(is_healthy, Exception):
                    logger.warning(f"Health check exception for proxy {proxy['id']}: {is_healthy}")
                    unhealthy_proxies.append(proxy['id'])
                elif is_healthy:
                    healthy_proxies.append(proxy['id'])
                else:
                    unhealthy_proxies.append(proxy['id'])

            # æ‰¹é‡æ›´æ–°RedisçŠ¶æ€
            if healthy_proxies or unhealthy_proxies:
                async with self.redis.pipeline() as pipe:
                    # æ¸…ç©ºç°æœ‰çŠ¶æ€
                    await pipe.delete('proxies:healthy', 'proxies:unhealthy')

                    # æ›´æ–°å¥åº·ä»£ç†
                    if healthy_proxies:
                        await pipe.sadd('proxies:healthy', *healthy_proxies)

                    # æ›´æ–°ä¸å¥åº·ä»£ç†
                    if unhealthy_proxies:
                        await pipe.sadd('proxies:unhealthy', *unhealthy_proxies)

                    # è®°å½•æ£€æŸ¥æ—¶é—´
                    await pipe.set('proxies:last_health_check', datetime.utcnow().isoformat())

                    await pipe.execute()

            stats = {
                'healthy': len(healthy_proxies),
                'unhealthy': len(unhealthy_proxies),
                'total': len(proxy_configs)
            }

            logger.info(f"Proxy health check completed: {stats}")
            return stats

        except Exception as e:
            logger.error(f"Batch health check failed: {str(e)}")
            return {'healthy': 0, 'unhealthy': 0, 'total': 0}

    async def start_health_monitor(self):
        """å¯åŠ¨ä»£ç†å¥åº·ç›‘æ§ - åå°å¼‚æ­¥ä»»åŠ¡"""
        logger.info("Starting proxy health monitor")

        while True:
            try:
                await self.batch_health_check()
                await asyncio.sleep(self._health_check_interval)

            except Exception as e:
                logger.error(f"Proxy health monitor error: {str(e)}")
                await asyncio.sleep(30)  # å‡ºé”™åç­‰å¾…30ç§’
```

### é˜¶æ®µäºŒï¼šæ ¸å¿ƒé‡‡é›†æ¨¡å—å¼€å‘ (2-3å‘¨)

#### 2.1 ç®€åŒ–æœç´¢é‡‡é›†æ¨¡å—ï¼ˆåŸºäºtwscrapeï¼‰

```python
# modules/search_scraper.py
import asyncio
import logging
from typing import Dict, List, Optional
from twscrape import API, gather
from twscrape.logger import set_log_level
import httpx

logger = logging.getLogger(__name__)

class TwitterSearchScraper:
    """åŸºäºtwscrapeçš„ç®€åŒ–æœç´¢é‡‡é›†å™¨"""

    def __init__(self, accounts_pool: List[Dict]):
        self.api = API()
        self.accounts_pool = accounts_pool
        self._setup_accounts()

    async def _setup_accounts(self):
        """è®¾ç½®è´¦å·æ± """
        for account in self.accounts_pool:
            await self.api.pool.add_account(
                username=account['username'],
                password=account['password'],
                email=account['email'],
                email_password=account['email_password']
            )

    async def search_tweets(self, keyword: str, count: int = 100) -> List[Dict]:
        """æœç´¢æ¨æ–‡ - ç›´æ¥ä½¿ç”¨twscrapeçš„GraphQLæ¥å£"""
        try:
            tweets = []
            async for tweet in self.api.search(keyword, limit=count):
                tweet_data = {
                    'id': tweet.id,
                    'text': tweet.rawContent,
                    'user': {
                        'id': tweet.user.id,
                        'username': tweet.user.username,
                        'displayname': tweet.user.displayname,
                        'followers_count': tweet.user.followersCount,
                        'verified': tweet.user.verified
                    },
                    'created_at': tweet.date.isoformat(),
                    'metrics': {
                        'retweet_count': tweet.retweetCount,
                        'like_count': tweet.likeCount,
                        'reply_count': tweet.replyCount,
                        'quote_count': tweet.quoteCount
                    },
                    'media': [{'url': m.url, 'type': m.type} for m in tweet.media] if tweet.media else [],
                    'urls': [url.expandedUrl for url in tweet.urls] if tweet.urls else []
                }
                tweets.append(tweet_data)

            logger.info(f"Collected {len(tweets)} tweets for keyword: {keyword}")
            return tweets

        except Exception as e:
            logger.error(f"Failed to search tweets for '{keyword}': {str(e)}")
            raise

    async def search_users(self, keyword: str, count: int = 20) -> List[Dict]:
        """æœç´¢ç”¨æˆ· - ä½¿ç”¨twscrapeç”¨æˆ·æœç´¢"""
        try:
            users = []
            async for user in self.api.search_users(keyword, limit=count):
                user_data = {
                    'id': user.id,
                    'username': user.username,
                    'displayname': user.displayname,
                    'description': user.description,
                    'followers_count': user.followersCount,
                    'following_count': user.followingCount,
                    'tweets_count': user.statusesCount,
                    'verified': user.verified,
                    'created_at': user.created.isoformat() if user.created else None,
                    'profile_image_url': user.profileImageUrl,
                    'profile_banner_url': user.profileBannerUrl
                }
                users.append(user_data)

            logger.info(f"Found {len(users)} users for keyword: {keyword}")
            return users

        except Exception as e:
            logger.error(f"Failed to search users for '{keyword}': {str(e)}")
            raise
```

#### 2.2 ç®€åŒ–ç”¨æˆ·ä¸»é¡µé‡‡é›†æ¨¡å—ï¼ˆåŸºäºtwscrapeï¼‰

```python
# modules/profile_scraper.py
import asyncio
import logging
from typing import Dict, List, Optional
from twscrape import API

logger = logging.getLogger(__name__)

class TwitterProfileScraper:
    """åŸºäºtwscrapeçš„ç®€åŒ–ç”¨æˆ·ä¸»é¡µé‡‡é›†å™¨"""

    def __init__(self, api: API):
        self.api = api

    async def get_user_info(self, username: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·åŸºç¡€ä¿¡æ¯ - ä½¿ç”¨twscrapeå†…ç½®æ–¹æ³•"""
        try:
            user = await self.api.user_by_login(username)
            if not user:
                logger.warning(f"User not found: {username}")
                return None

            return {
                'id': user.id,
                'username': user.username,
                'displayname': user.displayname,
                'description': user.description,
                'followers_count': user.followersCount,
                'following_count': user.followingCount,
                'tweets_count': user.statusesCount,
                'verified': user.verified,
                'created_at': user.created.isoformat() if user.created else None,
                'location': user.location,
                'profile_image_url': user.profileImageUrl,
                'profile_banner_url': user.profileBannerUrl,
                'url': user.url,
                'protected': user.protected
            }

        except Exception as e:
            logger.error(f"Failed to get user info for {username}: {str(e)}")
            raise

    async def get_user_tweets(self, user_id: str, count: int = 200) -> List[Dict]:
        """è·å–ç”¨æˆ·æ¨æ–‡åˆ—è¡¨ - ä½¿ç”¨twscrapeå†…ç½®æ–¹æ³•"""
        try:
            tweets = []
            async for tweet in self.api.user_tweets(int(user_id), limit=count):
                tweet_data = {
                    'id': tweet.id,
                    'text': tweet.rawContent,
                    'created_at': tweet.date.isoformat(),
                    'metrics': {
                        'retweet_count': tweet.retweetCount,
                        'like_count': tweet.likeCount,
                        'reply_count': tweet.replyCount,
                        'quote_count': tweet.quoteCount
                    },
                    'media': [{'url': m.url, 'type': m.type} for m in tweet.media] if tweet.media else [],
                    'urls': [url.expandedUrl for url in tweet.urls] if tweet.urls else [],
                    'hashtags': [tag.text for tag in tweet.hashtags] if tweet.hashtags else [],
                    'mentions': [mention.username for mention in tweet.mentions] if tweet.mentions else []
                }
                tweets.append(tweet_data)

            logger.info(f"Collected {len(tweets)} tweets for user {user_id}")
            return tweets

        except Exception as e:
            logger.error(f"Failed to get user tweets for {user_id}: {str(e)}")
            raise

    async def get_user_followers(self, user_id: str, count: int = 100) -> List[Dict]:
        """è·å–ç”¨æˆ·å…³æ³¨è€…åˆ—è¡¨"""
        try:
            followers = []
            async for user in self.api.user_followers(int(user_id), limit=count):
                follower_data = {
                    'id': user.id,
                    'username': user.username,
                    'displayname': user.displayname,
                    'followers_count': user.followersCount,
                    'verified': user.verified,
                    'profile_image_url': user.profileImageUrl
                }
                followers.append(follower_data)

            logger.info(f"Collected {len(followers)} followers for user {user_id}")
            return followers

        except Exception as e:
            logger.error(f"Failed to get followers for user {user_id}: {str(e)}")
            raise

    async def get_user_following(self, user_id: str, count: int = 100) -> List[Dict]:
        """è·å–ç”¨æˆ·å…³æ³¨åˆ—è¡¨"""
        try:
            following = []
            async for user in self.api.user_following(int(user_id), limit=count):
                following_data = {
                    'id': user.id,
                    'username': user.username,
                    'displayname': user.displayname,
                    'followers_count': user.followersCount,
                    'verified': user.verified,
                    'profile_image_url': user.profileImageUrl
                }
                following.append(following_data)

            logger.info(f"Collected {len(following)} following for user {user_id}")
            return following

        except Exception as e:
            logger.error(f"Failed to get following for user {user_id}: {str(e)}")
            raise
```

#### 2.3 è´¦å·æ·±åº¦ä¿¡æ¯é‡‡é›†
```python
# modules/account_scraper.py
from typing import Dict, List, Optional
from .base_scraper import TwitterBaseScraper
import logging

logger = logging.getLogger(__name__)

class TwitterAccountScraper(TwitterBaseScraper):
    """è´¦å·æ·±åº¦ä¿¡æ¯é‡‡é›†å™¨"""

    async def get_followers(self, user_id: str, count: int = 1000) -> List[Dict]:
        """è·å–ç²‰ä¸åˆ—è¡¨"""
        try:
            # å¯¼èˆªåˆ°å…³æ³¨è€…é¡µé¢
            success = await self.navigate_to_page('followers_page', username=user_id)
            if not success:
                raise RuntimeError(f"Failed to navigate to followers page for {user_id}")

            followers = []
            collected = 0

            while collected < count:
                # æå–å½“å‰é¡µé¢çš„å…³æ³¨è€…
                page_followers = await self.extract_page_data('followers_page')

                # å»é‡å¹¶æ·»åŠ æ–°å…³æ³¨è€…
                new_followers = [f for f in page_followers if f.get('user_id') not in {follower.get('user_id') for follower in followers}]
                followers.extend(new_followers)
                collected = len(followers)

                # æ»šåŠ¨åŠ è½½æ›´å¤š
                scroll_count = await self.scroll_and_load_more(max_scrolls=2)
                if scroll_count == 0 or not new_followers:
                    break

                await asyncio.sleep(2)

            return followers[:count]

        except Exception as e:
            logger.error(f"Failed to get followers for {user_id}: {str(e)}")
            raise

    async def get_following(self, user_id: str, count: int = 1000) -> List[Dict]:
        """è·å–å…³æ³¨åˆ—è¡¨"""
        try:
            # å¯¼èˆªåˆ°å…³æ³¨é¡µé¢
            success = await self.navigate_to_page('following_page', username=user_id)
            if not success:
                raise RuntimeError(f"Failed to navigate to following page for {user_id}")

            following = []
            collected = 0

            while collected < count:
                # æå–å½“å‰é¡µé¢çš„å…³æ³¨ç”¨æˆ·
                page_following = await self.extract_page_data('following_page')

                # å»é‡å¹¶æ·»åŠ æ–°å…³æ³¨ç”¨æˆ·
                new_following = [f for f in page_following if f.get('user_id') not in {user.get('user_id') for user in following}]
                following.extend(new_following)
                collected = len(following)

                # æ»šåŠ¨åŠ è½½æ›´å¤š
                scroll_count = await self.scroll_and_load_more(max_scrolls=2)
                if scroll_count == 0 or not new_following:
                    break

                await asyncio.sleep(2)

            return following[:count]

        except Exception as e:
            logger.error(f"Failed to get following for {user_id}: {str(e)}")
            raise

    async def analyze_activity(self, user_id: str) -> Dict:
        """åˆ†æç”¨æˆ·æ´»è·ƒåº¦"""
        try:
            # è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            user_info = await self.get_user_info(user_id)
            if not user_info:
                raise RuntimeError(f"Failed to get user info for activity analysis: {user_id}")

            # è·å–æœ€è¿‘æ¨æ–‡è¿›è¡Œæ´»è·ƒåº¦åˆ†æ
            recent_tweets = await self.get_user_tweets(user_id, count=100)

            # è®¡ç®—æ´»è·ƒåº¦æŒ‡æ ‡
            activity_metrics = {
                'user_id': user_id,
                'total_tweets': user_info.get('tweet_count', 0),
                'recent_tweets_count': len(recent_tweets),
                'avg_tweets_per_day': self._calculate_daily_tweet_rate(recent_tweets),
                'engagement_rate': self._calculate_engagement_rate(recent_tweets),
                'most_active_hours': self._analyze_posting_hours(recent_tweets),
                'hashtag_usage': self._analyze_hashtag_usage(recent_tweets),
                'mention_frequency': self._analyze_mention_frequency(recent_tweets),
                'media_usage_rate': self._calculate_media_usage_rate(recent_tweets)
            }

            return activity_metrics

        except Exception as e:
            logger.error(f"Failed to analyze activity for {user_id}: {str(e)}")
            raise

    def _calculate_daily_tweet_rate(self, tweets: List[Dict]) -> float:
        """è®¡ç®—æ—¥å‡æ¨æ–‡æ•°"""
        if not tweets:
            return 0.0

        try:
            from datetime import datetime, timedelta

            # è·å–æ—¶é—´èŒƒå›´
            dates = [datetime.fromisoformat(tweet.get('created_at', '').replace('Z', '+00:00')) for tweet in tweets if tweet.get('created_at')]
            if not dates:
                return 0.0

            date_range = (max(dates) - min(dates)).days
            if date_range == 0:
                date_range = 1

            return len(tweets) / date_range

        except Exception:
            return 0.0

    def _calculate_engagement_rate(self, tweets: List[Dict]) -> float:
        """è®¡ç®—äº’åŠ¨ç‡"""
        if not tweets:
            return 0.0

        total_engagement = sum(
            tweet.get('like_count', 0) + tweet.get('retweet_count', 0) + tweet.get('reply_count', 0)
            for tweet in tweets
        )

        return total_engagement / len(tweets) if tweets else 0.0
```

#### 2.4 å•æ¡æ¨æ–‡é‡‡é›†æ¨¡å—
```python
# modules/tweet_scraper.py
from typing import Dict, List, Optional
from .base_scraper import TwitterBaseScraper
import logging

logger = logging.getLogger(__name__)

class TwitterTweetScraper(TwitterBaseScraper):
    """å•æ¡æ¨æ–‡é‡‡é›†å™¨"""

    async def get_tweet_detail(self, tweet_id: str) -> Optional[Dict]:
        """è·å–æ¨æ–‡è¯¦ç»†ä¿¡æ¯"""
        try:
            # é¦–å…ˆéœ€è¦è·å–æ¨æ–‡çš„ç”¨æˆ·åï¼Œè¿™é‡Œå‡è®¾ä»tweet_idå¯ä»¥æ¨å¯¼æˆ–è€…éœ€è¦é¢å¤–å‚æ•°
            # å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦å…ˆæŸ¥è¯¢æ•°æ®åº“æˆ–ä½¿ç”¨å…¶ä»–æ–¹æ³•è·å–username

            # å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µé¢ - è¿™é‡Œéœ€è¦usernameï¼Œå®é™…å®ç°ä¸­éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜
            # success = await self.navigate_to_page('tweet_detail_page', username=username, tweet_id=tweet_id)

            # ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šç›´æ¥æ„é€ URL
            tweet_url = f"https://x.com/i/web/status/{tweet_id}"
            response = await self.page.goto(tweet_url, wait_until='networkidle', timeout=30000)

            if response.status >= 400:
                raise RuntimeError(f"Failed to load tweet {tweet_id}, status: {response.status}")

            await asyncio.sleep(3)

            # æå–æ¨æ–‡è¯¦æƒ…
            tweet_data = await self.extract_page_data('tweet_detail_page')

            # ä»æ‹¦æˆªçš„APIæ•°æ®ä¸­è·å–æ›´å®Œæ•´ä¿¡æ¯
            api_data = await self.get_intercepted_data('browser_tweet_detail_request')
            if api_data:
                tweet_data.update(self._parse_tweet_detail_api_data(api_data['data'], tweet_id))

            return tweet_data

        except Exception as e:
            logger.error(f"Failed to get tweet detail for {tweet_id}: {str(e)}")
            raise

    async def get_tweet_replies(self, tweet_id: str) -> List[Dict]:
        """è·å–æ¨æ–‡è¯„è®º"""
        try:
            # ç¡®ä¿å·²ç»åœ¨æ¨æ–‡è¯¦æƒ…é¡µé¢
            current_url = self.page.url
            if f"status/{tweet_id}" not in current_url:
                await self.get_tweet_detail(tweet_id)

            replies = []
            scroll_attempts = 0
            max_scrolls = 10

            while scroll_attempts < max_scrolls:
                # æå–å½“å‰é¡µé¢çš„å›å¤
                reply_elements = await self.page.query_selector_all('[data-testid="tweet"]')

                for element in reply_elements:
                    try:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å›å¤ï¼ˆä¸æ˜¯ä¸»æ¨æ–‡ï¼‰
                        reply_indicator = await element.query_selector('[data-testid="reply"]')
                        if reply_indicator:
                            reply_data = await self._extract_reply_from_element(element, tweet_id)
                            if reply_data and reply_data not in replies:
                                replies.append(reply_data)
                    except Exception as e:
                        logger.warning(f"Failed to extract reply: {str(e)}")

                # æ»šåŠ¨åŠ è½½æ›´å¤šå›å¤
                scroll_count = await self.scroll_and_load_more(max_scrolls=1)
                if scroll_count == 0:
                    break

                scroll_attempts += 1
                await asyncio.sleep(2)

            return replies

        except Exception as e:
            logger.error(f"Failed to get tweet replies for {tweet_id}: {str(e)}")
            raise

    async def get_video_stats(self, tweet_id: str) -> Dict:
        """è·å–è§†é¢‘æ’­æ”¾æ•°æ®"""
        try:
            # è·å–æ¨æ–‡è¯¦æƒ…
            tweet_detail = await self.get_tweet_detail(tweet_id)
            if not tweet_detail:
                raise RuntimeError(f"Failed to get tweet detail for video stats: {tweet_id}")

            video_stats = {
                'tweet_id': tweet_id,
                'view_count': tweet_detail.get('view_count', 0),
                'videos': []
            }

            # æ£€æŸ¥æ¨æ–‡ä¸­çš„è§†é¢‘
            media_elements = await self.page.query_selector_all('[data-testid="videoPlayer"]')

            for element in media_elements:
                try:
                    video_data = await self._extract_video_stats_from_element(element)
                    if video_data:
                        video_stats['videos'].append(video_data)
                except Exception as e:
                    logger.warning(f"Failed to extract video stats: {str(e)}")

            return video_stats

        except Exception as e:
            logger.error(f"Failed to get video stats for {tweet_id}: {str(e)}")
            raise

    def _parse_tweet_detail_api_data(self, api_data: Dict, tweet_id: str) -> Dict:
        """è§£ææ¨æ–‡è¯¦æƒ…APIæ•°æ®"""
        try:
            instructions = api_data.get('threaded_conversation_with_injections_v2', {}).get('instructions', [])

            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])

                    for entry in entries:
                        if entry.get('entryId') == f'tweet-{tweet_id}':
                            tweet_result = entry.get('content', {}).get('itemContent', {}).get('tweet_results', {}).get('result', {})
                            legacy = tweet_result.get('legacy', {})

                            return {
                                'view_count': tweet_result.get('views', {}).get('count', 0),
                                'bookmark_count': tweet_result.get('bookmark_count', 0),
                                'quote_count': legacy.get('quote_count', 0),
                                'reply_count': legacy.get('reply_count', 0),
                                'retweet_count': legacy.get('retweet_count', 0),
                                'favorite_count': legacy.get('favorite_count', 0)
                            }

            return {}

        except Exception as e:
            logger.warning(f"Failed to parse tweet detail API data: {str(e)}")
            return {}
```

### é˜¶æ®µä¸‰ï¼šåˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿ (1-2å‘¨)

#### 3.1 Celeryä»»åŠ¡è°ƒåº¦
```python
# core/tasks.py
from celery import Celery

app = Celery('weget')

@app.task
def scrape_search_task(keyword, count):
    """æœç´¢é‡‡é›†ä»»åŠ¡"""
    try:
        scraper = TwitterSearchScraper()
        results = scraper.search_tweets(keyword, count)
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        return {"status": "success", "count": len(results.get("data", {}).get("search_by_raw_query", {}).get("search_timeline", {}).get("timeline", {}).get("instructions", []))}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.task
def scrape_profile_task(username):
    """ç”¨æˆ·ä¸»é¡µé‡‡é›†ä»»åŠ¡"""
    try:
        scraper = TwitterProfileScraper()
        user_info = scraper.get_user_info(username)
        # ä¿å­˜ç”¨æˆ·ä¿¡æ¯åˆ°æ•°æ®åº“
        return {"status": "success", "username": username}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.task
def scrape_tweet_task(tweet_id):
    """å•æ¡æ¨æ–‡é‡‡é›†ä»»åŠ¡"""
    try:
        scraper = TwitterTweetScraper()
        tweet_detail = scraper.get_tweet_detail(tweet_id)
        # ä¿å­˜æ¨æ–‡è¯¦æƒ…åˆ°æ•°æ®åº“
        return {"status": "success", "tweet_id": tweet_id}
    except Exception as e:
        return {"status": "error", "error": str(e)}
```

#### 3.2 ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
```python
# core/scheduler.py
from typing import List, Dict, Optional
from celery import Celery
from celery.result import AsyncResult
import logging
import redis.asyncio as redis
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class TaskScheduler:
    """åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self, celery_app: Celery, redis_client: redis.Redis):
        self.celery_app = celery_app
        self.redis = redis_client
        self.task_priorities = {
            'high': 9,
            'normal': 5,
            'low': 1
        }

    async def submit_search_job(self, keywords: List[str], priority: str = 'normal',
                               options: Optional[Dict] = None) -> List[str]:
        """æäº¤æœç´¢ä»»åŠ¡"""
        try:
            if not keywords:
                raise ValueError("Keywords list cannot be empty")

            task_ids = []
            priority_value = self.task_priorities.get(priority, 5)

            for keyword in keywords:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒä»»åŠ¡åœ¨æ‰§è¡Œ
                existing_task = await self._check_duplicate_task('search', keyword)
                if existing_task:
                    logger.info(f"Search task for '{keyword}' already exists: {existing_task}")
                    task_ids.append(existing_task)
                    continue

                # æäº¤æ–°ä»»åŠ¡
                task_options = options or {}
                task_result = self.celery_app.send_task(
                    'core.tasks.scrape_search_task',
                    args=[keyword, task_options.get('count', 100)],
                    priority=priority_value,
                    retry=True,
                    retry_policy={
                        'max_retries': 3,
                        'interval_start': 60,
                        'interval_step': 60,
                        'interval_max': 300
                    }
                )

                task_ids.append(task_result.id)

                # è®°å½•ä»»åŠ¡ä¿¡æ¯
                await self._record_task_info(task_result.id, 'search', keyword, priority)

            logger.info(f"Submitted {len(task_ids)} search tasks with priority {priority}")
            return task_ids

        except Exception as e:
            logger.error(f"Failed to submit search jobs: {str(e)}")
            raise

    async def submit_profile_job(self, usernames: List[str], priority: str = 'normal',
                                options: Optional[Dict] = None) -> List[str]:
        """æäº¤ç”¨æˆ·é‡‡é›†ä»»åŠ¡"""
        try:
            if not usernames:
                raise ValueError("Usernames list cannot be empty")

            task_ids = []
            priority_value = self.task_priorities.get(priority, 5)

            for username in usernames:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒä»»åŠ¡åœ¨æ‰§è¡Œ
                existing_task = await self._check_duplicate_task('profile', username)
                if existing_task:
                    logger.info(f"Profile task for '{username}' already exists: {existing_task}")
                    task_ids.append(existing_task)
                    continue

                # æäº¤æ–°ä»»åŠ¡
                task_options = options or {}
                task_result = self.celery_app.send_task(
                    'core.tasks.scrape_profile_task',
                    args=[username, task_options],
                    priority=priority_value,
                    retry=True,
                    retry_policy={
                        'max_retries': 3,
                        'interval_start': 60,
                        'interval_step': 60,
                        'interval_max': 300
                    }
                )

                task_ids.append(task_result.id)

                # è®°å½•ä»»åŠ¡ä¿¡æ¯
                await self._record_task_info(task_result.id, 'profile', username, priority)

            logger.info(f"Submitted {len(task_ids)} profile tasks with priority {priority}")
            return task_ids

        except Exception as e:
            logger.error(f"Failed to submit profile jobs: {str(e)}")
            raise

    async def monitor_tasks(self) -> Dict:
        """ç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒä»»åŠ¡
            active_tasks = await self._get_active_tasks()

            # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
            status_counts = {
                'pending': 0,
                'running': 0,
                'success': 0,
                'failure': 0,
                'retry': 0
            }

            task_details = []

            for task_info in active_tasks:
                task_id = task_info['task_id']
                result = AsyncResult(task_id, app=self.celery_app)

                status = result.status
                status_counts[status.lower()] = status_counts.get(status.lower(), 0) + 1

                task_details.append({
                    'task_id': task_id,
                    'task_type': task_info['task_type'],
                    'target': task_info['target'],
                    'status': status,
                    'created_at': task_info['created_at'],
                    'result': result.result if status == 'SUCCESS' else None,
                    'error': str(result.result) if status == 'FAILURE' else None
                })

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance_metrics = await self._calculate_performance_metrics()

            return {
                'status_counts': status_counts,
                'task_details': task_details,
                'performance_metrics': performance_metrics,
                'total_tasks': len(active_tasks),
                'timestamp': datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"Failed to monitor tasks: {str(e)}")
            raise

    async def _check_duplicate_task(self, task_type: str, target: str) -> Optional[str]:
        """æ£€æŸ¥é‡å¤ä»»åŠ¡ - ä¿®å¤å¼‚æ­¥Redisè°ƒç”¨"""
        try:
            # åœ¨Redisä¸­æŸ¥æ‰¾ç›¸åŒçš„ä»»åŠ¡
            task_key = f"task:{task_type}:{target}"
            existing_task_id = await self.redis.get(task_key)

            if existing_task_id:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ä»åœ¨æ‰§è¡Œ
                task_id_str = existing_task_id if isinstance(existing_task_id, str) else existing_task_id.decode()
                result = AsyncResult(task_id_str, app=self.celery_app)
                if result.status in ['PENDING', 'STARTED', 'RETRY']:
                    return task_id_str
                else:
                    # ä»»åŠ¡å·²å®Œæˆï¼Œæ¸…é™¤è®°å½•
                    await self.redis.delete(task_key)

            return None

        except Exception as e:
            logger.warning(f"Failed to check duplicate task: {str(e)}")
            return None

    async def _record_task_info(self, task_id: str, task_type: str, target: str, priority: str):
        """è®°å½•ä»»åŠ¡ä¿¡æ¯ - ä½¿ç”¨å¼‚æ­¥Redisæ“ä½œ"""
        try:
            task_info = {
                'task_id': task_id,
                'task_type': task_type,
                'target': target,
                'priority': priority,
                'created_at': datetime.utcnow().isoformat()
            }

            # ä½¿ç”¨å¼‚æ­¥ç®¡é“æé«˜æ€§èƒ½
            async with self.redis.pipeline() as pipe:
                # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
                await pipe.hset(f"task_info:{task_id}", mapping=task_info)
                await pipe.expire(f"task_info:{task_id}", 86400)  # 24å°æ—¶è¿‡æœŸ

                # æ·»åŠ åˆ°æ´»è·ƒä»»åŠ¡é›†åˆ
                await pipe.sadd("active_tasks", task_id)

                # è®°å½•ä»»åŠ¡ç±»å‹æ˜ å°„
                task_key = f"task:{task_type}:{target}"
                await pipe.setex(task_key, 3600, task_id)  # 1å°æ—¶è¿‡æœŸ

                await pipe.execute()

        except Exception as e:
            logger.warning(f"Failed to record task info: {str(e)}")

    async def _get_active_tasks(self) -> List[Dict]:
        """è·å–æ´»è·ƒä»»åŠ¡åˆ—è¡¨ - ä¿®å¤å­—èŠ‚è§£ç é—®é¢˜"""
        try:
            task_ids = await self.redis.smembers("active_tasks")
            active_tasks = []

            for task_id in task_ids:
                # ç¡®ä¿task_idæ˜¯å­—ç¬¦ä¸²
                task_id_str = task_id if isinstance(task_id, str) else task_id.decode()
                task_info = await self.redis.hgetall(f"task_info:{task_id_str}")

                if task_info:
                    # è½¬æ¢å­—èŠ‚åˆ°å­—ç¬¦ä¸²ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if task_info and isinstance(next(iter(task_info.keys())), bytes):
                        task_info = {k.decode(): v.decode() for k, v in task_info.items()}
                    active_tasks.append(task_info)
                else:
                    # æ¸…ç†æ— æ•ˆçš„ä»»åŠ¡ID
                    await self.redis.srem("active_tasks", task_id)

            return active_tasks

        except Exception as e:
            logger.error(f"Failed to get active tasks: {str(e)}")
            return []

    async def _calculate_performance_metrics(self) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–æœ€è¿‘24å°æ—¶çš„ä»»åŠ¡ç»Ÿè®¡
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=24)

            # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–Redisä¸­è·å–å†å²ä»»åŠ¡æ•°æ®
            # ç®€åŒ–å®ç°ï¼Œè¿”å›åŸºæœ¬æŒ‡æ ‡
            return {
                'tasks_per_hour': 0,  # æ¯å°æ—¶ä»»åŠ¡æ•°
                'success_rate': 0.0,  # æˆåŠŸç‡
                'avg_execution_time': 0.0,  # å¹³å‡æ‰§è¡Œæ—¶é—´
                'error_rate': 0.0,  # é”™è¯¯ç‡
                'queue_length': 0  # é˜Ÿåˆ—é•¿åº¦
            }

        except Exception as e:
            logger.warning(f"Failed to calculate performance metrics: {str(e)}")
            return {}
```

### é˜¶æ®µå››ï¼šæ•°æ®å­˜å‚¨ä¸å¤„ç† (1å‘¨)

#### 4.1 æ•°æ®æ¨¡å‹è®¾è®¡
```python
# models/twitter_models.py
from mongoengine import Document, StringField, IntField, ListField, DateTimeField, BooleanField, DictField, EmbeddedDocument, EmbeddedDocumentField
from datetime import datetime
from typing import Optional, List, Dict
import logging

logger = logging.getLogger(__name__)

class MediaItem(EmbeddedDocument):
    """åª’ä½“é¡¹åµŒå…¥æ–‡æ¡£"""
    media_type = StringField(choices=['photo', 'video', 'gif'], required=True)
    url = StringField(required=True)
    thumbnail_url = StringField()
    duration_ms = IntField()  # è§†é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
    width = IntField()
    height = IntField()
    file_size = IntField()
    alt_text = StringField()

class TwitterUser(Document):
    """Twitterç”¨æˆ·æ¨¡å‹"""
    user_id = StringField(required=True, unique=True, primary_key=True)
    username = StringField(required=True, max_length=15)
    display_name = StringField(max_length=50)
    description = StringField(max_length=160)

    # ç»Ÿè®¡æ•°æ®
    followers_count = IntField(default=0)
    following_count = IntField(default=0)
    tweet_count = IntField(default=0)
    listed_count = IntField(default=0)

    # è´¦å·ä¿¡æ¯
    verified = BooleanField(default=False)
    protected = BooleanField(default=False)
    created_at = DateTimeField()
    location = StringField()
    url = StringField()

    # å¤´åƒå’ŒèƒŒæ™¯
    profile_image_url = StringField()
    profile_banner_url = StringField()

    # ç³»ç»Ÿå­—æ®µ
    collected_at = DateTimeField(default=datetime.utcnow)
    updated_at = DateTimeField(default=datetime.utcnow)
    takedown_at = DateTimeField()  # GDPRåˆè§„ï¼šæ•°æ®åˆ é™¤æ—¶é—´

    # ç´¢å¼•
    meta = {
        'collection': 'twitter_users',
        'indexes': [
            'username',
            'verified',
            'created_at',
            'collected_at',
            ('followers_count', -1),
            ('tweet_count', -1),
            {'fields': ['takedown_at'], 'expireAfterSeconds': 0}  # TTLç´¢å¼•
        ]
    }

class TwitterTweet(Document):
    """Twitteræ¨æ–‡æ¨¡å‹"""
    tweet_id = StringField(required=True, unique=True, primary_key=True)
    user_id = StringField(required=True)

    # å†…å®¹
    content = StringField(required=True)
    lang = StringField(max_length=10)
    source = StringField()  # å‘å¸ƒæ¥æº

    # æ—¶é—´
    created_at = DateTimeField(required=True)  # ä¿®å¤ï¼šä½¿ç”¨DateTimeFieldè€ŒéStringField

    # äº’åŠ¨æ•°æ®
    retweet_count = IntField(default=0)
    like_count = IntField(default=0)
    reply_count = IntField(default=0)
    quote_count = IntField(default=0)
    view_count = IntField(default=0)
    bookmark_count = IntField(default=0)

    # æ¨æ–‡ç±»å‹
    is_retweet = BooleanField(default=False)
    is_quote = BooleanField(default=False)
    is_reply = BooleanField(default=False)

    # å…³è”æ¨æ–‡
    retweeted_tweet_id = StringField()
    quoted_tweet_id = StringField()
    reply_to_tweet_id = StringField()
    reply_to_user_id = StringField()

    # åª’ä½“å†…å®¹
    media = ListField(EmbeddedDocumentField(MediaItem))

    # å®ä½“æå–
    hashtags = ListField(StringField(max_length=100))
    urls = ListField(StringField())
    user_mentions = ListField(StringField())

    # åœ°ç†ä½ç½®
    geo_coordinates = ListField(FloatField())  # [longitude, latitude]
    place_name = StringField()

    # ç³»ç»Ÿå­—æ®µ
    collected_at = DateTimeField(default=datetime.utcnow)
    updated_at = DateTimeField(default=datetime.utcnow)
    takedown_at = DateTimeField()  # GDPRåˆè§„ï¼šæ•°æ®åˆ é™¤æ—¶é—´

    # ç´¢å¼•
    meta = {
        'collection': 'twitter_tweets',
        'indexes': [
            'user_id',
            'created_at',
            'collected_at',
            'hashtags',
            'lang',
            ('like_count', -1),
            ('retweet_count', -1),
            ('view_count', -1),
            ('user_id', 'created_at'),  # å¤åˆç´¢å¼•
            {'fields': ['takedown_at'], 'expireAfterSeconds': 0}  # TTLç´¢å¼•
        ]
    }

class TwitterRelationship(Document):
    """Twitterå…³ç³»æ¨¡å‹ï¼ˆç”¨äºNeo4jåŒæ­¥ï¼‰"""
    from_user_id = StringField(required=True)
    to_user_id = StringField(required=True)
    relationship_type = StringField(choices=['follows', 'mentions', 'replies', 'retweets', 'quotes'], required=True)
    created_at = DateTimeField(default=datetime.utcnow)
    tweet_id = StringField()  # å¦‚æœå…³ç³»åŸºäºæ¨æ–‡

    meta = {
        'collection': 'twitter_relationships',
        'indexes': [
            ('from_user_id', 'relationship_type'),
            ('to_user_id', 'relationship_type'),
            'created_at',
            ('from_user_id', 'to_user_id', 'relationship_type')  # å”¯ä¸€å¤åˆç´¢å¼•
        ]
    }

class DataIngestionLog(Document):
    """æ•°æ®æ‘„å–æ—¥å¿—"""
    batch_id = StringField(required=True)
    data_type = StringField(choices=['user', 'tweet', 'relationship'], required=True)
    source = StringField(required=True)  # æ•°æ®æ¥æº
    records_processed = IntField(default=0)
    records_success = IntField(default=0)
    records_failed = IntField(default=0)
    errors = ListField(DictField())
    started_at = DateTimeField(default=datetime.utcnow)
    completed_at = DateTimeField()
    status = StringField(choices=['running', 'completed', 'failed'], default='running')

    meta = {
        'collection': 'data_ingestion_logs',
        'indexes': [
            'batch_id',
            'data_type',
            'started_at',
            'status'
        ]
    }
```

#### 4.2 æ•°æ®å­˜å‚¨ç®¡ç†
```python
# core/data_manager.py
from typing import Dict, List, Optional, Any
import asyncio
import logging
from datetime import datetime, timedelta
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo.errors import DuplicateKeyError, BulkWriteError
from pydantic import BaseModel, ValidationError
import redis.asyncio as redis
from models.twitter_models import TwitterUser, TwitterTweet, TwitterRelationship, DataIngestionLog

logger = logging.getLogger(__name__)

class DataValidationError(Exception):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass

class DataManager:
    """å¼‚æ­¥æ•°æ®å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, mongodb_client: AsyncIOMotorClient, redis_client: redis.Redis, neo4j_client=None):
        self.mongodb = mongodb_client
        self.redis = redis_client
        self.neo4j = neo4j_client
        self.db = mongodb_client.weget

        # æ‰¹é‡å†™å…¥é…ç½®
        self.batch_size = 1000
        self.batch_timeout = 30  # ç§’

        # æ•°æ®éªŒè¯å™¨
        self.validators = {
            'user': self._validate_user_data,
            'tweet': self._validate_tweet_data,
            'relationship': self._validate_relationship_data
        }

    async def save_user(self, user_data: Dict) -> bool:
        """ä¿å­˜ç”¨æˆ·æ•°æ®"""
        try:
            # æ•°æ®éªŒè¯
            validated_data = await self._validate_and_clean_data('user', user_data)

            # æ£€æŸ¥é‡å¤
            existing_user = await self.db.twitter_users.find_one({'user_id': validated_data['user_id']})

            if existing_user:
                # æ›´æ–°ç°æœ‰ç”¨æˆ·
                validated_data['updated_at'] = datetime.utcnow()
                result = await self.db.twitter_users.update_one(
                    {'user_id': validated_data['user_id']},
                    {'$set': validated_data}
                )
                logger.debug(f"Updated user {validated_data['user_id']}")
                return result.modified_count > 0
            else:
                # æ’å…¥æ–°ç”¨æˆ·
                validated_data['collected_at'] = datetime.utcnow()
                result = await self.db.twitter_users.insert_one(validated_data)
                logger.debug(f"Inserted new user {validated_data['user_id']}")
                return result.inserted_id is not None

        except ValidationError as e:
            logger.error(f"User data validation failed: {str(e)}")
            raise DataValidationError(f"Invalid user data: {str(e)}")
        except DuplicateKeyError:
            logger.warning(f"Duplicate user_id: {user_data.get('user_id')}")
            return False
        except Exception as e:
            logger.error(f"Failed to save user data: {str(e)}")
            raise

    async def save_tweet(self, tweet_data: Dict) -> bool:
        """ä¿å­˜æ¨æ–‡æ•°æ®"""
        try:
            # æ•°æ®éªŒè¯
            validated_data = await self._validate_and_clean_data('tweet', tweet_data)

            # æ£€æŸ¥é‡å¤
            existing_tweet = await self.db.twitter_tweets.find_one({'tweet_id': validated_data['tweet_id']})

            if existing_tweet:
                # æ›´æ–°äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è½¬å‘ç­‰å¯èƒ½å˜åŒ–ï¼‰
                update_fields = {
                    'retweet_count': validated_data.get('retweet_count', existing_tweet.get('retweet_count', 0)),
                    'like_count': validated_data.get('like_count', existing_tweet.get('like_count', 0)),
                    'reply_count': validated_data.get('reply_count', existing_tweet.get('reply_count', 0)),
                    'quote_count': validated_data.get('quote_count', existing_tweet.get('quote_count', 0)),
                    'view_count': validated_data.get('view_count', existing_tweet.get('view_count', 0)),
                    'updated_at': datetime.utcnow()
                }

                result = await self.db.twitter_tweets.update_one(
                    {'tweet_id': validated_data['tweet_id']},
                    {'$set': update_fields}
                )
                logger.debug(f"Updated tweet {validated_data['tweet_id']}")
                return result.modified_count > 0
            else:
                # æ’å…¥æ–°æ¨æ–‡
                validated_data['collected_at'] = datetime.utcnow()
                result = await self.db.twitter_tweets.insert_one(validated_data)
                logger.debug(f"Inserted new tweet {validated_data['tweet_id']}")

                # å¼‚æ­¥å¤„ç†å…³ç³»æ•°æ®
                asyncio.create_task(self._process_tweet_relationships(validated_data))

                return result.inserted_id is not None

        except ValidationError as e:
            logger.error(f"Tweet data validation failed: {str(e)}")
            raise DataValidationError(f"Invalid tweet data: {str(e)}")
        except DuplicateKeyError:
            logger.warning(f"Duplicate tweet_id: {tweet_data.get('tweet_id')}")
            return False
        except Exception as e:
            logger.error(f"Failed to save tweet data: {str(e)}")
            raise

    async def save_batch(self, data_type: str, data_list: List[Dict], batch_id: str = None) -> Dict:
        """æ‰¹é‡ä¿å­˜æ•°æ®"""
        try:
            if not data_list:
                return {'success': 0, 'failed': 0, 'errors': []}

            batch_id = batch_id or f"{data_type}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"

            # è®°å½•æ‰¹é‡å¤„ç†å¼€å§‹
            log_entry = {
                'batch_id': batch_id,
                'data_type': data_type,
                'source': 'scraper',
                'records_processed': len(data_list),
                'started_at': datetime.utcnow(),
                'status': 'running'
            }
            await self.db.data_ingestion_logs.insert_one(log_entry)

            success_count = 0
            failed_count = 0
            errors = []

            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(data_list), self.batch_size):
                batch = data_list[i:i + self.batch_size]

                if data_type == 'user':
                    batch_result = await self._save_user_batch(batch)
                elif data_type == 'tweet':
                    batch_result = await self._save_tweet_batch(batch)
                else:
                    raise ValueError(f"Unsupported data type: {data_type}")

                success_count += batch_result['success']
                failed_count += batch_result['failed']
                errors.extend(batch_result['errors'])

            # æ›´æ–°æ‰¹é‡å¤„ç†æ—¥å¿—
            await self.db.data_ingestion_logs.update_one(
                {'batch_id': batch_id},
                {
                    '$set': {
                        'records_success': success_count,
                        'records_failed': failed_count,
                        'errors': errors,
                        'completed_at': datetime.utcnow(),
                        'status': 'completed' if failed_count == 0 else 'failed'
                    }
                }
            )

            logger.info(f"Batch {batch_id} completed: {success_count} success, {failed_count} failed")

            return {
                'batch_id': batch_id,
                'success': success_count,
                'failed': failed_count,
                'errors': errors
            }

        except Exception as e:
            logger.error(f"Batch save failed: {str(e)}")
            if batch_id:
                await self.db.data_ingestion_logs.update_one(
                    {'batch_id': batch_id},
                    {
                        '$set': {
                            'status': 'failed',
                            'completed_at': datetime.utcnow(),
                            'errors': [{'error': str(e), 'timestamp': datetime.utcnow().isoformat()}]
                        }
                    }
                )
            raise

    async def deduplicate(self, collection_name: str, key_field: str, time_window_hours: int = 24) -> int:
        """æ•°æ®å»é‡"""
        try:
            collection = getattr(self.db, collection_name)

            # æŸ¥æ‰¾æŒ‡å®šæ—¶é—´çª—å£å†…çš„é‡å¤æ•°æ®
            cutoff_time = datetime.utcnow() - timedelta(hours=time_window_hours)

            pipeline = [
                {'$match': {'collected_at': {'$gte': cutoff_time}}},
                {'$group': {
                    '_id': f'${key_field}',
                    'count': {'$sum': 1},
                    'docs': {'$push': '$$ROOT'}
                }},
                {'$match': {'count': {'$gt': 1}}}
            ]

            duplicates = await collection.aggregate(pipeline).to_list(None)
            removed_count = 0

            for duplicate_group in duplicates:
                docs = duplicate_group['docs']
                # ä¿ç•™æœ€æ–°çš„æ–‡æ¡£ï¼Œåˆ é™¤å…¶ä»–çš„
                docs.sort(key=lambda x: x['collected_at'], reverse=True)
                docs_to_remove = docs[1:]  # é™¤äº†ç¬¬ä¸€ä¸ªï¼ˆæœ€æ–°çš„ï¼‰

                for doc in docs_to_remove:
                    await collection.delete_one({'_id': doc['_id']})
                    removed_count += 1

            logger.info(f"Removed {removed_count} duplicate records from {collection_name}")
            return removed_count

        except Exception as e:
            logger.error(f"Deduplication failed for {collection_name}: {str(e)}")
            raise

    async def _validate_and_clean_data(self, data_type: str, data: Dict) -> Dict:
        """éªŒè¯å’Œæ¸…ç†æ•°æ®"""
        validator = self.validators.get(data_type)
        if not validator:
            raise ValueError(f"No validator for data type: {data_type}")

        return await validator(data)

    async def _validate_user_data(self, data: Dict) -> Dict:
        """éªŒè¯ç”¨æˆ·æ•°æ®"""
        required_fields = ['user_id', 'username']
        for field in required_fields:
            if field not in data or not data[field]:
                raise ValidationError(f"Missing required field: {field}")

        # æ¸…ç†å’Œè½¬æ¢æ•°æ®
        cleaned_data = {
            'user_id': str(data['user_id']),
            'username': data['username'].strip(),
            'display_name': data.get('display_name', '').strip(),
            'description': data.get('description', '').strip(),
            'followers_count': max(0, int(data.get('followers_count', 0))),
            'following_count': max(0, int(data.get('following_count', 0))),
            'tweet_count': max(0, int(data.get('tweet_count', 0))),
            'verified': bool(data.get('verified', False)),
            'protected': bool(data.get('protected', False)),
            'location': data.get('location', '').strip(),
            'url': data.get('url', '').strip(),
            'profile_image_url': data.get('profile_image_url', '').strip(),
            'profile_banner_url': data.get('profile_banner_url', '').strip()
        }

        # å¤„ç†åˆ›å»ºæ—¶é—´
        if 'created_at' in data and data['created_at']:
            try:
                if isinstance(data['created_at'], str):
                    cleaned_data['created_at'] = datetime.fromisoformat(data['created_at'].replace('Z', '+00:00'))
                elif isinstance(data['created_at'], datetime):
                    cleaned_data['created_at'] = data['created_at']
            except ValueError:
                logger.warning(f"Invalid created_at format: {data['created_at']}")

        return cleaned_data

    async def _validate_tweet_data(self, data: Dict) -> Dict:
        """éªŒè¯æ¨æ–‡æ•°æ®"""
        required_fields = ['tweet_id', 'user_id', 'content']
        for field in required_fields:
            if field not in data or not data[field]:
                raise ValidationError(f"Missing required field: {field}")

        # æ¸…ç†å’Œè½¬æ¢æ•°æ®
        cleaned_data = {
            'tweet_id': str(data['tweet_id']),
            'user_id': str(data['user_id']),
            'content': data['content'].strip(),
            'lang': data.get('lang', 'en'),
            'source': data.get('source', '').strip(),
            'retweet_count': max(0, int(data.get('retweet_count', 0))),
            'like_count': max(0, int(data.get('like_count', 0))),
            'reply_count': max(0, int(data.get('reply_count', 0))),
            'quote_count': max(0, int(data.get('quote_count', 0))),
            'view_count': max(0, int(data.get('view_count', 0))),
            'is_retweet': bool(data.get('is_retweet', False)),
            'is_quote': bool(data.get('is_quote', False)),
            'is_reply': bool(data.get('is_reply', False)),
            'hashtags': data.get('hashtags', []),
            'urls': data.get('urls', []),
            'user_mentions': data.get('user_mentions', [])
        }

        # å¤„ç†åˆ›å»ºæ—¶é—´
        if 'created_at' in data and data['created_at']:
            try:
                if isinstance(data['created_at'], str):
                    cleaned_data['created_at'] = datetime.fromisoformat(data['created_at'].replace('Z', '+00:00'))
                elif isinstance(data['created_at'], datetime):
                    cleaned_data['created_at'] = data['created_at']
            except ValueError:
                raise ValidationError(f"Invalid created_at format: {data['created_at']}")
        else:
            cleaned_data['created_at'] = datetime.utcnow()

        # å¤„ç†åª’ä½“æ•°æ®
        if 'media' in data and data['media']:
            cleaned_data['media'] = []
            for media_item in data['media']:
                if isinstance(media_item, dict) and 'url' in media_item:
                    cleaned_media = {
                        'media_type': media_item.get('type', 'photo'),
                        'url': media_item['url'],
                        'thumbnail_url': media_item.get('thumbnail_url', ''),
                        'width': media_item.get('width', 0),
                        'height': media_item.get('height', 0)
                    }
                    if media_item.get('type') == 'video':
                        cleaned_media['duration_ms'] = media_item.get('duration_ms', 0)
                    cleaned_data['media'].append(cleaned_media)

        return cleaned_data

    async def _validate_relationship_data(self, data: Dict) -> Dict:
        """éªŒè¯å…³ç³»æ•°æ®"""
        required_fields = ['from_user_id', 'to_user_id', 'relationship_type']
        for field in required_fields:
            if field not in data or not data[field]:
                raise ValidationError(f"Missing required field: {field}")

        valid_types = ['follows', 'mentions', 'replies', 'retweets', 'quotes']
        if data['relationship_type'] not in valid_types:
            raise ValidationError(f"Invalid relationship_type: {data['relationship_type']}")

        return {
            'from_user_id': str(data['from_user_id']),
            'to_user_id': str(data['to_user_id']),
            'relationship_type': data['relationship_type'],
            'tweet_id': str(data['tweet_id']) if data.get('tweet_id') else None,
            'created_at': datetime.utcnow()
        }
```

## é£æ§ä¸å®‰å…¨ç­–ç•¥

### 1. è´¦å·ç®¡ç†ç­–ç•¥
- **è´¦å·åˆ†çº§**: æ–°å·ã€è€å·ã€æ´»è·ƒå·åˆ†ç±»ç®¡ç†
- **ç™»å½•éªŒè¯**: è‡ªåŠ¨å¤„ç†é‚®ç®±éªŒè¯ç ï¼Œäººå·¥å¤„ç†æ‰‹æœºéªŒè¯
- **å°å·æ£€æµ‹**: å®æ—¶ç›‘æ§è´¦å·çŠ¶æ€ï¼ŒåŠæ—¶æ›¿æ¢è¢«å°è´¦å·
- **ä½¿ç”¨é¢‘ç‡**: æ¯ä¸ªè´¦å·æ¯å°æ—¶è¯·æ±‚æ•°é™åˆ¶åœ¨åˆç†èŒƒå›´

### 2. ä»£ç†IPç­–ç•¥  
- **IPç»‘å®š**: ç‰¹å®šè´¦å·ç»‘å®šç‰¹å®šä»£ç†IPæ®µ
- **å¥åº·æ£€æµ‹**: å®šæœŸæ£€æµ‹ä»£ç†å¯ç”¨æ€§å’Œå°ç¦çŠ¶æ€
- **è½®æ¢æœºåˆ¶**: æ™ºèƒ½è½®æ¢ç­–ç•¥ï¼Œé¿å…é¢‘ç¹åˆ‡æ¢
- **åœ°ç†åˆ†å¸ƒ**: æŒ‰éœ€é€‰æ‹©ä¸åŒåœ°åŒºçš„ä»£ç†å‡ºå£

### 3. è¡Œä¸ºæ¨¡æ‹Ÿç­–ç•¥
- **æµè§ˆå™¨æŒ‡çº¹**: æ¯ä¸ªè´¦å·ç‹¬ç«‹çš„æµè§ˆå™¨ç¯å¢ƒ
- **éšæœºå»¶æ—¶**: æ¨¡æ‹Ÿäººç±»æ“ä½œçš„éšæœºé—´éš”
- **äº¤äº’æ¨¡æ‹Ÿ**: é€‚å½“çš„é¡µé¢æ»šåŠ¨ã€ç‚¹å‡»ç­‰æ“ä½œ
- **è¯·æ±‚æ¨¡å¼**: é¿å…æœºæ¢°åŒ–çš„å›ºå®šè¯·æ±‚æ¨¡å¼

### 4. ç›‘æ§å‘Šè­¦ç³»ç»Ÿ
```python
# core/monitor.py
from typing import Dict, List, Optional, Any
import asyncio
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
import redis.asyncio as redis
from prometheus_client import Counter, Histogram, Gauge
import aiohttp
import json

logger = logging.getLogger(__name__)

@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    name: str
    condition: str
    threshold: float
    duration: int  # æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    severity: str  # critical, warning, info
    message_template: str

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, redis_client: redis.Redis, alert_webhook_url: str = None):
        self.redis = redis_client
        self.alert_webhook_url = alert_webhook_url

        # PrometheusæŒ‡æ ‡
        self.account_health_gauge = Gauge('account_health_score', 'Account health score', ['account_id'])
        self.proxy_success_rate = Gauge('proxy_success_rate', 'Proxy success rate', ['proxy_id'])
        self.scraping_errors = Counter('scraping_errors_total', 'Total scraping errors', ['error_type'])
        self.response_time = Histogram('scraping_response_time_seconds', 'Scraping response time')

        # å‘Šè­¦è§„åˆ™
        self.alert_rules = [
            AlertRule(
                name="account_ban_rate_high",
                condition="account_ban_rate > 0.1",
                threshold=0.1,
                duration=300,  # 5åˆ†é’Ÿ
                severity="critical",
                message_template="è´¦å·å°ç¦ç‡è¿‡é«˜: {value:.2%} (é˜ˆå€¼: {threshold:.2%})"
            ),
            AlertRule(
                name="proxy_success_rate_low",
                condition="proxy_success_rate < 0.8",
                threshold=0.8,
                duration=300,
                severity="warning",
                message_template="ä»£ç†æˆåŠŸç‡è¿‡ä½: {value:.2%} (é˜ˆå€¼: {threshold:.2%})"
            ),
            AlertRule(
                name="stream_lag_high",
                condition="stream_lag > 60",
                threshold=60,
                duration=60,
                severity="warning",
                message_template="æ•°æ®æµå»¶è¿Ÿè¿‡é«˜: {value}ç§’ (é˜ˆå€¼: {threshold}ç§’)"
            )
        ]

        # å‘Šè­¦çŠ¶æ€è·Ÿè¸ª
        self.active_alerts = {}

    async def monitor_account_health(self) -> Dict[str, float]:
        """ç›‘æ§è´¦å·å¥åº·åº¦"""
        try:
            account_health = {}

            # è·å–æ‰€æœ‰è´¦å·åˆ—è¡¨
            available_accounts = await self.redis.smembers('accounts:available')
            banned_accounts = await self.redis.smembers('accounts:banned')

            total_accounts = len(available_accounts) + len(banned_accounts)
            if total_accounts == 0:
                logger.warning("No accounts found for health monitoring")
                return {}

            # è®¡ç®—æ•´ä½“è´¦å·å¥åº·åº¦
            overall_health = len(available_accounts) / total_accounts
            account_health['overall'] = overall_health

            # æ›´æ–°PrometheusæŒ‡æ ‡
            self.account_health_gauge.labels(account_id='overall').set(overall_health)

            # æ£€æŸ¥ä¸ªåˆ«è´¦å·å¥åº·åº¦
            for account_id in available_accounts:
                try:
                    account_data = await self.redis.hgetall(f'account:{account_id.decode()}')
                    if account_data:
                        # è®¡ç®—è´¦å·å¥åº·åˆ†æ•°
                        health_score = await self._calculate_account_health_score(account_id.decode(), account_data)
                        account_health[account_id.decode()] = health_score

                        # æ›´æ–°PrometheusæŒ‡æ ‡
                        self.account_health_gauge.labels(account_id=account_id.decode()).set(health_score)

                except Exception as e:
                    logger.warning(f"Failed to check health for account {account_id}: {str(e)}")

            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            ban_rate = len(banned_accounts) / total_accounts if total_accounts > 0 else 0
            await self._check_alert_condition("account_ban_rate_high", ban_rate)

            logger.info(f"Account health monitoring completed. Overall health: {overall_health:.2%}")
            return account_health

        except Exception as e:
            logger.error(f"Account health monitoring failed: {str(e)}")
            raise

    async def monitor_proxy_status(self) -> Dict[str, Dict]:
        """ç›‘æ§ä»£ç†çŠ¶æ€"""
        try:
            proxy_status = {}

            # è·å–æ‰€æœ‰ä»£ç†åˆ—è¡¨
            healthy_proxies = await self.redis.smembers('proxies:healthy')
            unhealthy_proxies = await self.redis.smembers('proxies:unhealthy')

            total_proxies = len(healthy_proxies) + len(unhealthy_proxies)
            if total_proxies == 0:
                logger.warning("No proxies found for status monitoring")
                return {}

            # è®¡ç®—æ•´ä½“ä»£ç†æˆåŠŸç‡
            overall_success_rate = len(healthy_proxies) / total_proxies
            proxy_status['overall'] = {
                'success_rate': overall_success_rate,
                'healthy_count': len(healthy_proxies),
                'unhealthy_count': len(unhealthy_proxies)
            }

            # æ›´æ–°PrometheusæŒ‡æ ‡
            self.proxy_success_rate.labels(proxy_id='overall').set(overall_success_rate)

            # æ£€æŸ¥ä¸ªåˆ«ä»£ç†çŠ¶æ€
            for proxy_id in healthy_proxies:
                try:
                    proxy_data = await self.redis.hgetall(f'proxy:{proxy_id.decode()}')
                    if proxy_data:
                        # è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯
                        stats = await self._get_proxy_stats(proxy_id.decode())
                        proxy_status[proxy_id.decode()] = stats

                        # æ›´æ–°PrometheusæŒ‡æ ‡
                        self.proxy_success_rate.labels(proxy_id=proxy_id.decode()).set(stats.get('success_rate', 0))

                except Exception as e:
                    logger.warning(f"Failed to check status for proxy {proxy_id}: {str(e)}")

            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            await self._check_alert_condition("proxy_success_rate_low", overall_success_rate)

            logger.info(f"Proxy status monitoring completed. Overall success rate: {overall_success_rate:.2%}")
            return proxy_status

        except Exception as e:
            logger.error(f"Proxy status monitoring failed: {str(e)}")
            raise

    async def alert_on_anomaly(self, event_type: str, details: Dict):
        """å¼‚å¸¸å‘Šè­¦"""
        try:
            alert_data = {
                'event_type': event_type,
                'timestamp': datetime.utcnow().isoformat(),
                'details': details,
                'severity': details.get('severity', 'warning')
            }

            # è®°å½•å‘Šè­¦åˆ°Redis
            alert_key = f"alert:{event_type}:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
            await self.redis.setex(alert_key, 86400, json.dumps(alert_data))  # 24å°æ—¶è¿‡æœŸ

            # æ·»åŠ åˆ°å‘Šè­¦é˜Ÿåˆ—
            await self.redis.lpush('alert_queue', json.dumps(alert_data))

            # å‘é€Webhooké€šçŸ¥
            if self.alert_webhook_url:
                await self._send_webhook_alert(alert_data)

            # æ›´æ–°Prometheusè®¡æ•°å™¨
            self.scraping_errors.labels(error_type=event_type).inc()

            logger.warning(f"Alert triggered: {event_type} - {details}")

        except Exception as e:
            logger.error(f"Failed to send alert: {str(e)}")

    async def _calculate_account_health_score(self, account_id: str, account_data: Dict) -> float:
        """è®¡ç®—è´¦å·å¥åº·åˆ†æ•°"""
        try:
            score = 1.0  # åŸºç¡€åˆ†æ•°

            # æ£€æŸ¥æœ€åä½¿ç”¨æ—¶é—´
            last_used = account_data.get(b'last_used')
            if last_used:
                last_used_time = datetime.fromisoformat(last_used.decode())
                hours_since_use = (datetime.utcnow() - last_used_time).total_seconds() / 3600

                # è¶…è¿‡24å°æ—¶æœªä½¿ç”¨ï¼Œåˆ†æ•°ä¸‹é™
                if hours_since_use > 24:
                    score *= 0.8
                elif hours_since_use > 12:
                    score *= 0.9

            # æ£€æŸ¥é”™è¯¯ç‡
            error_count = int(account_data.get(b'error_count', 0))
            total_requests = int(account_data.get(b'total_requests', 1))
            error_rate = error_count / total_requests

            if error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                score *= (1 - error_rate)

            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é—®é¢˜
            verification_required = account_data.get(b'verification_required')
            if verification_required and verification_required.decode().lower() == 'true':
                score *= 0.5

            return max(0.0, min(1.0, score))  # ç¡®ä¿åˆ†æ•°åœ¨0-1ä¹‹é—´

        except Exception as e:
            logger.warning(f"Failed to calculate health score for account {account_id}: {str(e)}")
            return 0.5  # é»˜è®¤ä¸­ç­‰å¥åº·åº¦

    async def _get_proxy_stats(self, proxy_id: str) -> Dict:
        """è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»Redisè·å–ä»£ç†ç»Ÿè®¡æ•°æ®
            stats_key = f"proxy_stats:{proxy_id}"
            stats_data = await self.redis.hgetall(stats_key)

            if not stats_data:
                return {'success_rate': 0.0, 'total_requests': 0, 'successful_requests': 0}

            total_requests = int(stats_data.get(b'total_requests', 0))
            successful_requests = int(stats_data.get(b'successful_requests', 0))

            success_rate = successful_requests / total_requests if total_requests > 0 else 0.0

            return {
                'success_rate': success_rate,
                'total_requests': total_requests,
                'successful_requests': successful_requests,
                'last_success': stats_data.get(b'last_success', b'').decode(),
                'avg_response_time': float(stats_data.get(b'avg_response_time', 0))
            }

        except Exception as e:
            logger.warning(f"Failed to get stats for proxy {proxy_id}: {str(e)}")
            return {'success_rate': 0.0, 'total_requests': 0, 'successful_requests': 0}

    async def _check_alert_condition(self, rule_name: str, current_value: float):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        try:
            rule = next((r for r in self.alert_rules if r.name == rule_name), None)
            if not rule:
                return

            # æ£€æŸ¥æ˜¯å¦è§¦å‘å‘Šè­¦æ¡ä»¶
            triggered = False
            if ">" in rule.condition:
                triggered = current_value > rule.threshold
            elif "<" in rule.condition:
                triggered = current_value < rule.threshold

            current_time = datetime.utcnow()
            alert_key = f"alert_state:{rule_name}"

            if triggered:
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å‘Šè­¦çŠ¶æ€
                alert_state = await self.redis.hgetall(alert_key)

                if alert_state:
                    # å·²åœ¨å‘Šè­¦çŠ¶æ€ï¼Œæ£€æŸ¥æŒç»­æ—¶é—´
                    start_time = datetime.fromisoformat(alert_state[b'start_time'].decode())
                    duration = (current_time - start_time).total_seconds()

                    if duration >= rule.duration and not alert_state.get(b'notified'):
                        # è¾¾åˆ°æŒç»­æ—¶é—´é˜ˆå€¼ï¼Œå‘é€å‘Šè­¦
                        await self.alert_on_anomaly(rule_name, {
                            'rule': rule.name,
                            'value': current_value,
                            'threshold': rule.threshold,
                            'duration': duration,
                            'severity': rule.severity,
                            'message': rule.message_template.format(
                                value=current_value,
                                threshold=rule.threshold
                            )
                        })

                        # æ ‡è®°å·²é€šçŸ¥
                        await self.redis.hset(alert_key, 'notified', 'true')
                else:
                    # æ–°çš„å‘Šè­¦çŠ¶æ€
                    await self.redis.hset(alert_key, mapping={
                        'start_time': current_time.isoformat(),
                        'value': str(current_value),
                        'notified': 'false'
                    })
                    await self.redis.expire(alert_key, 3600)  # 1å°æ—¶è¿‡æœŸ
            else:
                # æ¡ä»¶ä¸æ»¡è¶³ï¼Œæ¸…é™¤å‘Šè­¦çŠ¶æ€
                await self.redis.delete(alert_key)

        except Exception as e:
            logger.warning(f"Failed to check alert condition for {rule_name}: {str(e)}")

    async def _send_webhook_alert(self, alert_data: Dict):
        """å‘é€Webhookå‘Šè­¦"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.alert_webhook_url,
                    json=alert_data,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        logger.info(f"Alert webhook sent successfully for {alert_data['event_type']}")
                    else:
                        logger.warning(f"Alert webhook failed with status {response.status}")

        except Exception as e:
            logger.error(f"Failed to send webhook alert: {str(e)}")
```

## ç°ä»£åŒ–éƒ¨ç½²æ¶æ„

### 1. Kubernetes + Helm Chart éƒ¨ç½²

#### 1.1 Helm Chart ç»“æ„
```
weget-chart/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â”œâ”€â”€ values-prod.yaml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ secret.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ pdb.yaml
â”‚   â”œâ”€â”€ vault-auth.yaml
â”‚   â”œâ”€â”€ external-secrets.yaml
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ servicemonitor.yaml
â”‚       â””â”€â”€ prometheusrule.yaml
â””â”€â”€ charts/
    â”œâ”€â”€ redis/
    â”œâ”€â”€ mongodb/
    â””â”€â”€ neo4j/
```

#### 1.2 ä¸»è¦é…ç½®æ–‡ä»¶

**Chart.yaml**
```yaml
apiVersion: v2
name: weget
description: WeGet X(Twitter) Data Collection System
type: application
version: 1.0.0
appVersion: "1.0.0"

dependencies:
  - name: redis
    version: "17.15.6"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
  - name: mongodb
    version: "13.18.5"
    repository: "https://charts.bitnami.com/bitnami"
    condition: mongodb.enabled
  - name: neo4j
    version: "4.4.27"
    repository: "https://helm.neo4j.com/neo4j"
    condition: neo4j.enabled
  - name: external-secrets
    version: "0.9.11"
    repository: "https://charts.external-secrets.io"
    condition: externalSecrets.enabled
```

**values.yaml**
```yaml
# WeGet åº”ç”¨é…ç½®
replicaCount: 3

image:
  repository: weget/scraper
  pullPolicy: IfNotPresent
  tag: "latest"

# æœåŠ¡é…ç½®
service:
  type: ClusterIP
  port: 8000

# èµ„æºé™åˆ¶
resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

# è‡ªåŠ¨æ‰©ç¼©å®¹
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# æµè§ˆå™¨æ± é…ç½®
browserPool:
  enabled: true
  browserless:
    enabled: true
    replicaCount: 5
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

# External Secrets é…ç½®
externalSecrets:
  enabled: true
  secretStore:
    provider: vault
    vault:
      server: "https://vault.internal.company.com"
      path: "secret"
      version: "v2"
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "weget-scraper"

# Redis é…ç½® - ä½¿ç”¨ External Secrets
redis:
  enabled: true
  auth:
    enabled: true
    existingSecret: "weget-redis-secret"
    existingSecretPasswordKey: "password"
  master:
    persistence:
      enabled: true
      size: 20Gi
  replica:
    replicaCount: 2

# MongoDB é…ç½® - ä½¿ç”¨ External Secrets
mongodb:
  enabled: true
  auth:
    enabled: true
    existingSecret: "weget-mongodb-secret"
    existingSecretPasswordKey: "mongodb-password"
  persistence:
    enabled: true
    size: 100Gi
  replicaSet:
    enabled: true
    replicas:
      secondary: 2

# Neo4j é…ç½® - ä½¿ç”¨ External Secrets
neo4j:
  enabled: true
  neo4j:
    passwordFromSecret: "weget-neo4j-secret"
    passwordSecretKey: "neo4j-password"
  volumes:
    data:
      mode: "volume"
      volume:
        persistentVolumeClaim:
          claimName: "neo4j-data"

# ç›‘æ§é…ç½®
monitoring:
  enabled: true
  prometheus:
    enabled: true
  grafana:
    enabled: true
  jaeger:
    enabled: true

# å®‰å…¨é…ç½®
security:
  vault:
    enabled: true
    address: "https://vault.internal.company.com"
    role: "weget-scraper"
    secretPath: "secret/weget"

  networkPolicy:
    enabled: true

  podSecurityPolicy:
    enabled: true

# æ•°æ®æ‘„å–ç®¡é“
dataIngestion:
  redisStreams:
    enabled: true
    maxLength: 10000
    deadLetterQueue:
      enabled: true
      maxRetries: 3

  workers:
    validation:
      replicaCount: 3
    storage:
      replicaCount: 5
    relationship:
      replicaCount: 2

# ä»£ç†é…ç½®
proxy:
  rotation:
    enabled: true
    interval: "300s"
  healthCheck:
    enabled: true
    interval: "60s"
    timeout: "10s"

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  format: "json"
  elasticsearch:
    enabled: true
    host: "elasticsearch.logging.svc.cluster.local"
    index: "weget-logs"
```

**values-prod.yaml**
```yaml
# ç”Ÿäº§ç¯å¢ƒç‰¹å®šé…ç½®
replicaCount: 10

image:
  tag: "v1.0.0"  # å›ºå®šç‰ˆæœ¬

resources:
  limits:
    cpu: 4000m
    memory: 8Gi
  requests:
    cpu: 2000m
    memory: 4Gi

autoscaling:
  minReplicas: 10
  maxReplicas: 50

# ç”Ÿäº§çº§æµè§ˆå™¨æ± 
browserPool:
  browserless:
    replicaCount: 20
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi

# ç”Ÿäº§çº§æ•°æ®åº“é…ç½®
mongodb:
  replicaSet:
    replicas:
      secondary: 4
  persistence:
    size: 1Ti

redis:
  master:
    persistence:
      size: 100Gi
  replica:
    replicaCount: 3

# ç”Ÿäº§çº§ç›‘æ§
monitoring:
  prometheus:
    retention: "30d"
    storage: "500Gi"
  grafana:
    persistence:
      enabled: true
      size: "10Gi"

# å®‰å…¨åŠ å›º
security:
  podSecurityPolicy:
    enabled: true
    runAsNonRoot: true
    readOnlyRootFilesystem: true

  networkPolicy:
    enabled: true
    ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx

# æ•°æ®æ²»ç†
dataGovernance:
  retention:
    tweets: "90d"
    users: "180d"
    relationships: "365d"

  archival:
    enabled: true
    schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
    destination: "s3://weget-archive"

  gdpr:
    enabled: true
    takedownSchedule: "0 3 * * *"  # æ¯å¤©å‡Œæ™¨3ç‚¹
```

### 2. HashiCorp Vault é›†æˆä¸ External Secrets

#### 2.1 External Secrets é…ç½®
```yaml
# templates/external-secrets.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: vault-backend
  namespace: {{ .Release.Namespace }}
spec:
  provider:
    vault:
      server: {{ .Values.externalSecrets.secretStore.vault.server }}
      path: {{ .Values.externalSecrets.secretStore.vault.path }}
      version: {{ .Values.externalSecrets.secretStore.vault.version }}
      auth:
        kubernetes:
          mountPath: {{ .Values.externalSecrets.secretStore.vault.auth.kubernetes.mountPath }}
          role: {{ .Values.externalSecrets.secretStore.vault.auth.kubernetes.role }}
          serviceAccountRef:
            name: {{ include "weget.fullname" . }}-vault
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: weget-database-secret
  namespace: {{ .Release.Namespace }}
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: weget-database-secret
    creationPolicy: Owner
  data:
  - secretKey: mongodb-uri
    remoteRef:
      key: weget/database
      property: mongodb_uri
  - secretKey: mongodb-password
    remoteRef:
      key: weget/database
      property: mongodb_password
  - secretKey: redis-password
    remoteRef:
      key: weget/database
      property: redis_password
  - secretKey: neo4j-password
    remoteRef:
      key: weget/database
      property: neo4j_password
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: weget-proxy-secret
  namespace: {{ .Release.Namespace }}
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: weget-proxy-secret
    creationPolicy: Owner
  data:
  - secretKey: api-key
    remoteRef:
      key: weget/proxy
      property: api_key
  - secretKey: secret
    remoteRef:
      key: weget/proxy
      property: secret
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: weget-accounts-secret
  namespace: {{ .Release.Namespace }}
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: weget-accounts-secret
    creationPolicy: Owner
  data:
  - secretKey: encryption-key
    remoteRef:
      key: weget/accounts
      property: encryption_key
```

#### 2.2 Vault é…ç½®ç»“æ„
```hcl
# vault/policies/weget-policy.hcl
path "secret/data/weget/*" {
  capabilities = ["read"]
}

path "secret/data/weget/database/*" {
  capabilities = ["read"]
}

path "secret/data/weget/proxy/*" {
  capabilities = ["read"]
}

path "secret/data/weget/accounts/*" {
  capabilities = ["read"]
}

path "auth/token/lookup-self" {
  capabilities = ["read"]
}

# Kubernetes è®¤è¯é…ç½®
path "auth/kubernetes/login" {
  capabilities = ["create", "update"]
}
```

### 3. ç»Ÿä¸€é…ç½®ç®¡ç†æ¶æ„ (Helm â†’ Docker Compose)

#### 3.1 é…ç½®ç®¡ç†åŸåˆ™
- **å•ä¸€çœŸç†æº**: Helm Chart ä½œä¸ºå”¯ä¸€é…ç½®æº
- **è‡ªåŠ¨æ¸²æŸ“**: Docker Compose æ–‡ä»¶ç”± Helm æ¨¡æ¿è‡ªåŠ¨ç”Ÿæˆ
- **ç¯å¢ƒéš”ç¦»**: ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒçš„ values æ–‡ä»¶
- **é…ç½®éªŒè¯**: è‡ªåŠ¨æ£€æŸ¥é…ç½®ä¸€è‡´æ€§

#### 3.2 Helm æ¨¡æ¿ç”Ÿæˆ Docker Compose
```bash
#!/bin/bash
# scripts/generate-compose.sh
# ä» Helm Chart ç”Ÿæˆ Docker Compose æ–‡ä»¶

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
CHART_DIR="$PROJECT_ROOT/weget-chart"

# æ£€æŸ¥ Helm æ˜¯å¦å®‰è£…
if ! command -v helm &> /dev/null; then
    echo "Error: Helm is not installed"
    exit 1
fi

# ç”Ÿæˆå¼€å‘ç¯å¢ƒ Docker Compose
echo "Generating Docker Compose for development environment..."
helm template weget-dev "$CHART_DIR" \
    --values "$CHART_DIR/values-dev.yaml" \
    --set global.environment=development \
    --set global.generateCompose=true \
    --output-dir "$PROJECT_ROOT/generated" \
    --include-crds

# æå– Docker Compose é…ç½®
if [ -f "$PROJECT_ROOT/generated/weget/templates/docker-compose.yaml" ]; then
    cp "$PROJECT_ROOT/generated/weget/templates/docker-compose.yaml" "$PROJECT_ROOT/docker-compose.dev.yml"
    echo "âœ… Generated docker-compose.dev.yml"
else
    echo "âŒ Failed to generate Docker Compose file"
    exit 1
fi

# ç”Ÿæˆç¯å¢ƒå˜é‡æ–‡ä»¶
cat > "$PROJECT_ROOT/.env.dev" << EOF
# Auto-generated from Helm values - DO NOT EDIT MANUALLY
# To modify these values, edit weget-chart/values-dev.yaml and run scripts/generate-compose.sh

# Database Configuration
REDIS_PASSWORD=\${REDIS_PASSWORD:-dev-redis-password}
MONGODB_USER=\${MONGODB_USER:-weget}
MONGODB_PASSWORD=\${MONGODB_PASSWORD:-dev-mongo-password}
NEO4J_PASSWORD=\${NEO4J_PASSWORD:-dev-neo4j-password}

# Application Configuration
LOG_LEVEL=\${LOG_LEVEL:-DEBUG}
WORKER_CONCURRENCY=\${WORKER_CONCURRENCY:-4}
MAX_BROWSER_SESSIONS=\${MAX_BROWSER_SESSIONS:-10}

# External Services
PROXY_API_KEY=\${PROXY_API_KEY:-}
VAULT_ADDR=\${VAULT_ADDR:-http://localhost:8200}
VAULT_TOKEN=\${VAULT_TOKEN:-}

# Generated at: $(date -u +"%Y-%m-%dT%H:%M:%SZ")
EOF

echo "âœ… Generated .env.dev"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf "$PROJECT_ROOT/generated"

echo "ğŸ‰ Configuration generation completed!"
echo "ğŸ“ To start development environment: docker-compose -f docker-compose.dev.yml up"
```

#### 3.3 å¼€å‘ç¯å¢ƒ Values é…ç½®
```yaml
# weget-chart/values-dev.yaml
global:
  environment: development
  generateCompose: true
  logLevel: DEBUG

# å¼€å‘ç¯å¢ƒé•œåƒé…ç½®
image:
  repository: weget/scraper
  tag: "dev"
  pullPolicy: IfNotPresent

# å¼€å‘ç¯å¢ƒèµ„æºé™åˆ¶
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

# å¼€å‘ç¯å¢ƒæ•°æ®åº“é…ç½®
redis:
  enabled: true
  image:
    repository: redis
    tag: "7-alpine"
  service:
    port: 6379
  auth:
    enabled: true
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

mongodb:
  enabled: true
  image:
    repository: mongo
    tag: "6"
  service:
    port: 27017
  auth:
    enabled: true
    database: weget
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi

neo4j:
  enabled: true
  image:
    repository: neo4j
    tag: "5"
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi

# Celery é…ç½®
celery:
  logLevel: debug
  worker:
    concurrency: 2
    replicas: 1

# æµè§ˆå™¨æ± é…ç½®
browserPool:
  enabled: true
  image:
    repository: browserless/chrome
    tag: latest
  maxSessions: 5
  connectionTimeout: 60000
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi

# å¼€å‘ç¯å¢ƒç‰¹å®šé…ç½®
env:
  DEBUG: "true"
  DEVELOPMENT_MODE: "true"
```

#### 3.4 Helm æ¨¡æ¿ - Docker Compose ç”Ÿæˆå™¨
```yaml
# weget-chart/templates/docker-compose.yaml
{{- if .Values.global.generateCompose }}
# Auto-generated Docker Compose file from Helm Chart
# DO NOT EDIT MANUALLY - Generated at {{ now | date "2006-01-02T15:04:05Z" }}
# Source: {{ .Chart.Name }}-{{ .Chart.Version }}

version: '3.8'

services:
  # Redis æœåŠ¡
  redis:
    image: {{ .Values.redis.image.repository }}:{{ .Values.redis.image.tag }}
    ports:
      - "{{ .Values.redis.service.port }}:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    {{- if .Values.redis.resources }}
    deploy:
      resources:
        limits:
          cpus: '{{ .Values.redis.resources.limits.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.redis.resources.limits.memory }}
        reservations:
          cpus: '{{ .Values.redis.resources.requests.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.redis.resources.requests.memory }}
    {{- end }}

  # MongoDB æœåŠ¡
  mongodb:
    image: {{ .Values.mongodb.image.repository }}:{{ .Values.mongodb.image.tag }}
    ports:
      - "{{ .Values.mongodb.service.port }}:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD}
      - MONGO_INITDB_DATABASE={{ .Values.mongodb.auth.database | default "weget" }}
    volumes:
      - mongodb_data:/data/db
    {{- if .Values.mongodb.resources }}
    deploy:
      resources:
        limits:
          cpus: '{{ .Values.mongodb.resources.limits.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.mongodb.resources.limits.memory }}
        reservations:
          cpus: '{{ .Values.mongodb.resources.requests.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.mongodb.resources.requests.memory }}
    {{- end }}

  # Neo4j æœåŠ¡
  neo4j:
    image: {{ .Values.neo4j.image.repository }}:{{ .Values.neo4j.image.tag }}
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - neo4j_data:/data
    {{- if .Values.neo4j.resources }}
    deploy:
      resources:
        limits:
          cpus: '{{ .Values.neo4j.resources.limits.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.neo4j.resources.limits.memory }}
        reservations:
          cpus: '{{ .Values.neo4j.resources.requests.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.neo4j.resources.requests.memory }}
    {{- end }}

  # WeGet ä¸»æœåŠ¡
  weget-scraper:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "{{ .Values.service.port | default 8000 }}:8000"
    environment:
      # ä½¿ç”¨ç¯å¢ƒå˜é‡å ä½ç¬¦ï¼Œé¿å…ç¡¬ç¼–ç 
      - REDIS_URL=${REDIS_URL}
      - MONGODB_URI=${MONGODB_URI}
      - NEO4J_URI=${NEO4J_URI}
      - LOG_LEVEL={{ .Values.global.logLevel | default "INFO" }}
      {{- range $key, $value := .Values.env }}
      - {{ $key }}={{ $value }}
      {{- end }}
    depends_on:
      - redis
      - mongodb
      - neo4j
    volumes:
      - ./logs:/app/logs
    {{- if .Values.resources }}
    deploy:
      resources:
        limits:
          cpus: '{{ .Values.resources.limits.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.resources.limits.memory }}
        reservations:
          cpus: '{{ .Values.resources.requests.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.resources.requests.memory }}
    {{- end }}

  # Celery Worker
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
    command: celery -A core.celery_app worker --loglevel={{ .Values.celery.logLevel | default "info" }} --concurrency={{ .Values.celery.worker.concurrency | default 4 }}
    environment:
      # ä½¿ç”¨ç¯å¢ƒå˜é‡å ä½ç¬¦ï¼Œé¿å…ç¡¬ç¼–ç 
      - REDIS_URL=${REDIS_URL}
      - MONGODB_URI=${MONGODB_URI}
      - LOG_LEVEL={{ .Values.global.logLevel | default "INFO" }}
    depends_on:
      - redis
      - mongodb
    volumes:
      - ./logs:/app/logs
    {{- if .Values.celery.worker.replicas }}
    deploy:
      replicas: {{ .Values.celery.worker.replicas }}
    {{- end }}

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
    command: celery -A core.celery_app beat --loglevel={{ .Values.celery.logLevel | default "info" }}
    environment:
      # ä½¿ç”¨ç¯å¢ƒå˜é‡å ä½ç¬¦ï¼Œé¿å…ç¡¬ç¼–ç 
      - REDIS_URL=${REDIS_URL}
      - MONGODB_URI=${MONGODB_URI}
      - LOG_LEVEL={{ .Values.global.logLevel | default "INFO" }}
    depends_on:
      - redis
      - mongodb
    volumes:
      - ./logs:/app/logs

  # Browserless (æµè§ˆå™¨æ± )
  {{- if .Values.browserPool.enabled }}
  browserless:
    image: {{ .Values.browserPool.image.repository }}:{{ .Values.browserPool.image.tag | default "latest" }}
    ports:
      - "3000:3000"
    environment:
      - MAX_CONCURRENT_SESSIONS={{ .Values.browserPool.maxSessions | default 10 }}
      - CONNECTION_TIMEOUT={{ .Values.browserPool.connectionTimeout | default 60000 }}
    {{- if .Values.browserPool.resources }}
    deploy:
      resources:
        limits:
          cpus: '{{ .Values.browserPool.resources.limits.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.browserPool.resources.limits.memory }}
        reservations:
          cpus: '{{ .Values.browserPool.resources.requests.cpu | replace "m" "" | div 1000 }}'
          memory: {{ .Values.browserPool.resources.requests.memory }}
    {{- end }}
  {{- end }}

volumes:
  redis_data:
  mongodb_data:
  neo4j_data:

# Configuration checksum: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
{{- end }}
```

#### 3.5 é…ç½®éªŒè¯è„šæœ¬
```bash
#!/bin/bash
# scripts/validate-config.sh
# éªŒè¯ Helm å’Œ Docker Compose é…ç½®ä¸€è‡´æ€§

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
CHART_DIR="$PROJECT_ROOT/weget-chart"

echo "ğŸ” Validating configuration consistency..."

# æ£€æŸ¥ Helm Chart è¯­æ³•
echo "Checking Helm Chart syntax..."
helm lint "$CHART_DIR"

# éªŒè¯æ¨¡æ¿æ¸²æŸ“
echo "Validating template rendering..."
helm template weget-test "$CHART_DIR" \
    --values "$CHART_DIR/values-dev.yaml" \
    --dry-run > /dev/null

# æ£€æŸ¥ç”Ÿæˆçš„ Docker Compose æ–‡ä»¶
if [ -f "$PROJECT_ROOT/docker-compose.dev.yml" ]; then
    echo "Validating Docker Compose syntax..."
    docker-compose -f "$PROJECT_ROOT/docker-compose.dev.yml" config > /dev/null
    echo "âœ… Docker Compose syntax is valid"
else
    echo "âš ï¸  Docker Compose file not found, run generate-compose.sh first"
fi

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo "Checking environment variables..."
if [ -f "$PROJECT_ROOT/.env.dev" ]; then
    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    required_vars=("REDIS_PASSWORD" "MONGODB_USER" "MONGODB_PASSWORD" "NEO4J_PASSWORD")
    for var in "${required_vars[@]}"; do
        if ! grep -q "^$var=" "$PROJECT_ROOT/.env.dev"; then
            echo "âŒ Missing required environment variable: $var"
            exit 1
        fi
    done
    echo "âœ… All required environment variables are present"
else
    echo "âš ï¸  Environment file not found, run generate-compose.sh first"
fi

echo "ğŸ‰ Configuration validation completed successfully!"
```

#### 3.6 ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
```bash
# .env.example - ç¯å¢ƒå˜é‡æ¨¡æ¿æ–‡ä»¶
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env.dev å¹¶å¡«å…¥å®é™…å€¼

# ==================== æ•°æ®åº“é…ç½® ====================
# MongoDB è¿æ¥é…ç½®
MONGODB_URI=${MONGODB_URI}
MONGO_USER=${MONGO_USER}
MONGO_PASSWORD=${MONGO_PASSWORD}
MONGO_HOST=${MONGO_HOST:-mongodb}
MONGO_PORT=${MONGO_PORT:-27017}
MONGO_DATABASE=${MONGO_DATABASE:-weget}

# Redis è¿æ¥é…ç½®
REDIS_URL=redis://:password@redis:6379

# Neo4j è¿æ¥é…ç½®
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# ==================== åº”ç”¨é…ç½® ====================
# æ—¥å¿—çº§åˆ«
LOG_LEVEL=INFO

# Worker å¹¶å‘æ•°
WORKER_CONCURRENCY=4

# æµè§ˆå™¨æ± é…ç½®
MAX_BROWSER_SESSIONS=10

# ==================== å¤–éƒ¨æœåŠ¡ ====================
# Vault é…ç½®
VAULT_URL=http://vault:8200
VAULT_TOKEN=your-vault-token

# ä»£ç†æœåŠ¡é…ç½®
PROXY_API_KEY=your-proxy-api-key
PROXY_SECRET=your-proxy-secret

# ==================== å®‰å…¨é…ç½® ====================
# åŠ å¯†å¯†é’¥
ENCRYPTION_KEY=your-32-char-encryption-key

# JWT å¯†é’¥
JWT_SECRET=your-jwt-secret

# ==================== ç›‘æ§é…ç½® ====================
# Prometheus é…ç½®
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090

# Grafana é…ç½®
GRAFANA_ADMIN_PASSWORD=admin

# ==================== å­˜å‚¨é…ç½® ====================
# S3/MinIO é…ç½®
S3_ENDPOINT=http://minio:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_BUCKET=weget-archive

# ==================== å¼€å‘ç¯å¢ƒç‰¹å®š ====================
DEBUG=false
DEVELOPMENT_MODE=false
```

#### 3.7 CI ç¡¬ç¼–ç æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# scripts/check-hardcoded-secrets.sh
# æ£€æŸ¥ç¡¬ç¼–ç å¯†ç å’Œè¿æ¥å­—ç¬¦ä¸²

set -e

echo "ğŸ” Checking for hardcoded secrets and connection strings..."

# æ£€æŸ¥ç¡¬ç¼–ç çš„æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
echo "Checking for hardcoded database connections..."
if grep -r "mongodb://.*:.*@\|redis://.*:.*@" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v "\${" | grep -v ".env.example"; then
    echo "âŒ Found hardcoded database connection strings"
    echo "Please use environment variables like \${MONGODB_URI} instead"
    exit 1
fi

# æ£€æŸ¥ç¡¬ç¼–ç å¯†ç 
echo "Checking for hardcoded passwords..."
if grep -r -i "password.*=.*['\"][^$].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v ".env.example"; then
    echo "âŒ Found potential hardcoded passwords"
    exit 1
fi

# æ£€æŸ¥ç¡¬ç¼–ç APIå¯†é’¥
echo "Checking for hardcoded API keys..."
if grep -r -i "api_key.*=.*['\"][^$].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v ".env.example"; then
    echo "âŒ Found potential hardcoded API keys"
    exit 1
fi

# æ£€æŸ¥ç§é’¥
echo "Checking for private keys..."
if grep -r "BEGIN.*PRIVATE.*KEY\|BEGIN.*RSA.*PRIVATE" --include="*.py" --include="*.pem" --include="*.key" .; then
    echo "âŒ Found potential private keys in code"
    exit 1
fi

# æ£€æŸ¥JWTå¯†é’¥
echo "Checking for JWT secrets..."
if grep -r -i "jwt.*secret.*=.*['\"][^$].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v ".env.example"; then
    echo "âŒ Found potential hardcoded JWT secrets"
    exit 1
fi

echo "âœ… No hardcoded secrets found"

# æ£€æŸ¥é«˜é£é™©æ®‹ç•™é—®é¢˜
echo "ğŸ” Checking for high-risk residual issues..."

# æ£€æŸ¥æ˜æ–‡MongoDB URI
echo "Checking for hardcoded MongoDB URIs..."
if grep -r "mongodb://.*:.*@" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v "\${" | grep -v ".env.example" | grep -v "# æ£€æŸ¥" | grep -v "grep"; then
    echo "âŒ Found hardcoded MongoDB URIs"
    exit 1
fi

# æ£€æŸ¥é‡å¤Docker Composeæ–‡ä»¶
echo "Checking for duplicate Docker Compose files..."
compose_count=$(find . -name "docker-compose*.yml" -not -path "./generated/*" | wc -l)
if [ "$compose_count" -gt 1 ]; then
    echo "âŒ Found multiple Docker Compose files: $compose_count"
    echo "Only auto-generated docker-compose.dev.yml should exist"
    exit 1
fi

# æ£€æŸ¥AsyncRedisClientæ®‹ç•™
echo "Checking for deprecated AsyncRedisClient..."
if grep -r "class AsyncRedisClient" --include="*.py" .; then
    echo "âŒ Found deprecated AsyncRedisClient class"
    echo "Use unified AsyncRedisManager instead"
    exit 1
fi

echo "âœ… All high-risk checks passed"
```

#### 2.2 Kubernetes Vault é›†æˆ
```yaml
# templates/vault-auth.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "weget.fullname" . }}-vault
  labels:
    {{- include "weget.labels" . | nindent 4 }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ include "weget.fullname" . }}-vault-auth
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: {{ include "weget.fullname" . }}-vault
  namespace: {{ .Release.Namespace }}
---
apiVersion: v1
kind: Secret
metadata:
  name: {{ include "weget.fullname" . }}-vault-secret
  annotations:
    kubernetes.io/service-account.name: {{ include "weget.fullname" . }}-vault
type: kubernetes.io/service-account-token
```

#### 2.3 Vault Agent Sidecar
```yaml
# templates/deployment.yaml (éƒ¨åˆ†)
spec:
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/role: "weget-scraper"
        vault.hashicorp.com/agent-inject-secret-database: "secret/weget/database"
        vault.hashicorp.com/agent-inject-template-database: |
          {{- with secret "secret/weget/database" -}}
          MONGO_USER="{{ .Data.data.username }}"
          MONGO_PASSWORD="{{ .Data.data.password }}"
          MONGO_HOST="{{ .Data.data.host }}"
          MONGO_PORT="{{ .Data.data.port }}"
          MONGO_DATABASE="{{ .Data.data.database }}"
          REDIS_URL="redis://:{{ .Data.data.redis_password }}@{{ .Data.data.redis_host }}:{{ .Data.data.redis_port }}"
          NEO4J_URI="bolt://{{ .Data.data.neo4j_host }}:{{ .Data.data.neo4j_port }}"
          NEO4J_USER="{{ .Data.data.neo4j_username }}"
          NEO4J_PASSWORD="{{ .Data.data.neo4j_password }}"
          {{- end }}
        vault.hashicorp.com/agent-inject-secret-proxy: "secret/weget/proxy"
        vault.hashicorp.com/agent-inject-template-proxy: |
          {{- with secret "secret/weget/proxy" -}}
          PROXY_PROVIDER_API_KEY="{{ .Data.data.api_key }}"
          PROXY_PROVIDER_SECRET="{{ .Data.data.secret }}"
          {{- end }}
        vault.hashicorp.com/agent-inject-secret-accounts: "secret/weget/accounts"
        vault.hashicorp.com/agent-inject-template-accounts: |
          {{- with secret "secret/weget/accounts" -}}
          ACCOUNT_POOL_ENCRYPTION_KEY="{{ .Data.data.encryption_key }}"
          {{- end }}
    spec:
      serviceAccountName: {{ include "weget.fullname" . }}-vault
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        env:
        - name: VAULT_ADDR
          value: "{{ .Values.security.vault.address }}"
        - name: VAULT_ROLE
          value: "{{ .Values.security.vault.role }}"
        volumeMounts:
        - name: vault-secrets
          mountPath: /vault/secrets
          readOnly: true
      volumes:
      - name: vault-secrets
        emptyDir:
          medium: Memory
```

#### 2.4 åº”ç”¨ç¨‹åº Vault å®¢æˆ·ç«¯
```python
# core/vault_client.py
import hvac
import os
import logging
from typing import Dict, Optional
import asyncio
from functools import lru_cache

logger = logging.getLogger(__name__)

class VaultClient:
    """HashiCorp Vault å®¢æˆ·ç«¯"""

    def __init__(self, vault_addr: str = None, vault_role: str = None):
        self.vault_addr = vault_addr or os.getenv('VAULT_ADDR')
        self.vault_role = vault_role or os.getenv('VAULT_ROLE')
        self.client = None
        self._authenticated = False

    async def authenticate(self) -> bool:
        """ä½¿ç”¨ Kubernetes æœåŠ¡è´¦å·è®¤è¯"""
        try:
            self.client = hvac.Client(url=self.vault_addr)

            # è¯»å– Kubernetes æœåŠ¡è´¦å· token
            with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:
                jwt_token = f.read()

            # ä½¿ç”¨ Kubernetes è®¤è¯æ–¹æ³•
            auth_response = self.client.auth.kubernetes.login(
                role=self.vault_role,
                jwt=jwt_token
            )

            if auth_response and 'auth' in auth_response:
                self._authenticated = True
                logger.info("Successfully authenticated with Vault")
                return True
            else:
                logger.error("Failed to authenticate with Vault")
                return False

        except Exception as e:
            logger.error(f"Vault authentication failed: {str(e)}")
            return False

    @lru_cache(maxsize=128)
    def get_secret(self, secret_path: str) -> Optional[Dict]:
        """è·å–å¯†é’¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            if not self._authenticated:
                if not asyncio.run(self.authenticate()):
                    return None

            response = self.client.secrets.kv.v2.read_secret_version(
                path=secret_path
            )

            if response and 'data' in response and 'data' in response['data']:
                return response['data']['data']
            else:
                logger.warning(f"No data found for secret path: {secret_path}")
                return None

        except Exception as e:
            logger.error(f"Failed to get secret {secret_path}: {str(e)}")
            return None

    def get_database_config(self) -> Dict:
        """è·å–æ•°æ®åº“é…ç½®"""
        return self.get_secret('weget/database') or {}

    def get_proxy_config(self) -> Dict:
        """è·å–ä»£ç†é…ç½®"""
        return self.get_secret('weget/proxy') or {}

    def get_account_config(self) -> Dict:
        """è·å–è´¦å·é…ç½®"""
        return self.get_secret('weget/accounts') or {}

    def refresh_token(self):
        """åˆ·æ–° Vault token"""
        try:
            if self.client and self._authenticated:
                self.client.auth.token.renew_self()
                logger.info("Vault token refreshed successfully")
        except Exception as e:
            logger.warning(f"Failed to refresh Vault token: {str(e)}")
            self._authenticated = False

# å…¨å±€ Vault å®¢æˆ·ç«¯å®ä¾‹
vault_client = VaultClient()
```

#### 2.5 é…ç½®ç®¡ç†å™¨
```python
# core/config_manager.py
import os
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
from core.vault_client import vault_client
import logging

logger = logging.getLogger(__name__)

@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    mongodb_uri: str = ""
    redis_url: str = ""
    neo4j_uri: str = ""
    neo4j_user: str = ""
    neo4j_password: str = ""

@dataclass
class ProxyConfig:
    """ä»£ç†é…ç½®"""
    provider_api_key: str = ""
    provider_secret: str = ""
    rotation_interval: int = 300
    health_check_interval: int = 60

@dataclass
class AccountConfig:
    """è´¦å·é…ç½®"""
    encryption_key: str = ""
    max_accounts_per_proxy: int = 5
    account_rotation_interval: int = 3600

@dataclass
class ScrapingConfig:
    """çˆ¬å–é…ç½®"""
    max_concurrent_tasks: int = 100
    request_timeout: int = 30
    retry_attempts: int = 3
    rate_limit_per_account: int = 100  # æ¯å°æ—¶

@dataclass
class MonitoringConfig:
    """ç›‘æ§é…ç½®"""
    prometheus_enabled: bool = True
    grafana_enabled: bool = True
    jaeger_enabled: bool = True
    alert_webhook_url: str = ""

@dataclass
class WeGetConfig:
    """WeGet ä¸»é…ç½®"""
    database: DatabaseConfig = field(default_factory=DatabaseConfig)
    proxy: ProxyConfig = field(default_factory=ProxyConfig)
    account: AccountConfig = field(default_factory=AccountConfig)
    scraping: ScrapingConfig = field(default_factory=ScrapingConfig)
    monitoring: MonitoringConfig = field(default_factory=MonitoringConfig)

    def __post_init__(self):
        """åˆå§‹åŒ–ååŠ è½½é…ç½®"""
        self.load_from_vault()
        self.load_from_env()

    def load_from_vault(self):
        """ä» Vault åŠ è½½æ•æ„Ÿé…ç½®"""
        try:
            # åŠ è½½æ•°æ®åº“é…ç½®
            db_secrets = vault_client.get_database_config()
            if db_secrets:
                self.database.mongodb_uri = db_secrets.get('mongodb_uri', '')
                self.database.redis_url = db_secrets.get('redis_url', '')
                self.database.neo4j_uri = db_secrets.get('neo4j_uri', '')
                self.database.neo4j_user = db_secrets.get('neo4j_user', '')
                self.database.neo4j_password = db_secrets.get('neo4j_password', '')

            # åŠ è½½ä»£ç†é…ç½®
            proxy_secrets = vault_client.get_proxy_config()
            if proxy_secrets:
                self.proxy.provider_api_key = proxy_secrets.get('api_key', '')
                self.proxy.provider_secret = proxy_secrets.get('secret', '')

            # åŠ è½½è´¦å·é…ç½®
            account_secrets = vault_client.get_account_config()
            if account_secrets:
                self.account.encryption_key = account_secrets.get('encryption_key', '')

            logger.info("Configuration loaded from Vault successfully")

        except Exception as e:
            logger.error(f"Failed to load configuration from Vault: {str(e)}")

    def load_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½éæ•æ„Ÿé…ç½®"""
        # çˆ¬å–é…ç½®
        self.scraping.max_concurrent_tasks = int(os.getenv('MAX_CONCURRENT_TASKS', '100'))
        self.scraping.request_timeout = int(os.getenv('REQUEST_TIMEOUT', '30'))
        self.scraping.retry_attempts = int(os.getenv('RETRY_ATTEMPTS', '3'))
        self.scraping.rate_limit_per_account = int(os.getenv('RATE_LIMIT_PER_ACCOUNT', '100'))

        # ä»£ç†é…ç½®
        self.proxy.rotation_interval = int(os.getenv('PROXY_ROTATION_INTERVAL', '300'))
        self.proxy.health_check_interval = int(os.getenv('PROXY_HEALTH_CHECK_INTERVAL', '60'))

        # è´¦å·é…ç½®
        self.account.max_accounts_per_proxy = int(os.getenv('MAX_ACCOUNTS_PER_PROXY', '5'))
        self.account.account_rotation_interval = int(os.getenv('ACCOUNT_ROTATION_INTERVAL', '3600'))

        # ç›‘æ§é…ç½®
        self.monitoring.prometheus_enabled = os.getenv('PROMETHEUS_ENABLED', 'true').lower() == 'true'
        self.monitoring.grafana_enabled = os.getenv('GRAFANA_ENABLED', 'true').lower() == 'true'
        self.monitoring.jaeger_enabled = os.getenv('JAEGER_ENABLED', 'true').lower() == 'true'
        self.monitoring.alert_webhook_url = os.getenv('ALERT_WEBHOOK_URL', '')

    def validate(self) -> bool:
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        errors = []

        # éªŒè¯å¿…éœ€çš„æ•°æ®åº“é…ç½®
        if not self.database.mongodb_uri:
            errors.append("MongoDB URI is required")
        if not self.database.redis_url:
            errors.append("Redis URL is required")

        # éªŒè¯ä»£ç†é…ç½®
        if not self.proxy.provider_api_key:
            errors.append("Proxy provider API key is required")

        # éªŒè¯è´¦å·é…ç½®
        if not self.account.encryption_key:
            errors.append("Account encryption key is required")

        if errors:
            logger.error(f"Configuration validation failed: {', '.join(errors)}")
            return False

        logger.info("Configuration validation passed")
        return True

# å…¨å±€é…ç½®å®ä¾‹
config = WeGetConfig()
```

### 3. æµè§ˆå™¨æ± åŒ–æ¶æ„

#### 3.1 Browserless é›†æˆ
```python
# core/browser_pool.py
import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import random
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

@dataclass
class BrowserInstance:
    """æµè§ˆå™¨å®ä¾‹ä¿¡æ¯"""
    instance_id: str
    endpoint_url: str
    status: str  # 'available', 'busy', 'unhealthy'
    created_at: datetime
    last_used: datetime
    session_count: int = 0
    max_sessions: int = 10

class BrowserPool(ABC):
    """æµè§ˆå™¨æ± æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def get_page(self, **kwargs) -> Dict:
        """è·å–é¡µé¢å®ä¾‹"""
        pass

    @abstractmethod
    async def release_page(self, page_info: Dict):
        """é‡Šæ”¾é¡µé¢å®ä¾‹"""
        pass

    @abstractmethod
    async def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        pass

class BrowserlessPool(BrowserPool):
    """Browserless æµè§ˆå™¨æ± """

    def __init__(self, browserless_endpoints: List[str], max_sessions_per_instance: int = 10):
        self.endpoints = browserless_endpoints
        self.max_sessions_per_instance = max_sessions_per_instance
        self.instances: Dict[str, BrowserInstance] = {}
        self.session_timeout = 300  # 5åˆ†é’Ÿ
        self._lock = asyncio.Lock()

        # åˆå§‹åŒ–å®ä¾‹
        for i, endpoint in enumerate(browserless_endpoints):
            instance_id = f"browserless-{i}"
            self.instances[instance_id] = BrowserInstance(
                instance_id=instance_id,
                endpoint_url=endpoint,
                status='available',
                created_at=datetime.utcnow(),
                last_used=datetime.utcnow()
            )

    async def get_page(self, **kwargs) -> Dict:
        """è·å–æµè§ˆå™¨é¡µé¢"""
        async with self._lock:
            # é€‰æ‹©å¯ç”¨çš„å®ä¾‹
            available_instance = await self._select_available_instance()
            if not available_instance:
                raise RuntimeError("No available browser instances")

            try:
                # åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¼šè¯
                session_info = await self._create_browser_session(available_instance, **kwargs)

                # æ›´æ–°å®ä¾‹çŠ¶æ€
                available_instance.session_count += 1
                available_instance.last_used = datetime.utcnow()

                if available_instance.session_count >= available_instance.max_sessions:
                    available_instance.status = 'busy'

                return {
                    'instance_id': available_instance.instance_id,
                    'session_id': session_info['session_id'],
                    'websocket_url': session_info['websocket_url'],
                    'page_url': session_info.get('page_url'),
                    'created_at': datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"Failed to create browser session: {str(e)}")
                available_instance.status = 'unhealthy'
                raise

    async def release_page(self, page_info: Dict):
        """é‡Šæ”¾æµè§ˆå™¨é¡µé¢"""
        try:
            instance_id = page_info['instance_id']
            session_id = page_info['session_id']

            if instance_id in self.instances:
                instance = self.instances[instance_id]

                # å…³é—­æµè§ˆå™¨ä¼šè¯
                await self._close_browser_session(instance, session_id)

                # æ›´æ–°å®ä¾‹çŠ¶æ€
                instance.session_count = max(0, instance.session_count - 1)
                if instance.session_count < instance.max_sessions and instance.status == 'busy':
                    instance.status = 'available'

                logger.debug(f"Released browser session {session_id} from instance {instance_id}")

        except Exception as e:
            logger.error(f"Failed to release browser session: {str(e)}")

    async def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            'total_instances': len(self.instances),
            'available_instances': 0,
            'busy_instances': 0,
            'unhealthy_instances': 0,
            'total_sessions': 0,
            'instances': {}
        }

        for instance_id, instance in self.instances.items():
            try:
                # æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€
                is_healthy = await self._check_instance_health(instance)

                if is_healthy:
                    if instance.status == 'unhealthy':
                        instance.status = 'available'
                        logger.info(f"Instance {instance_id} recovered")
                else:
                    instance.status = 'unhealthy'
                    logger.warning(f"Instance {instance_id} is unhealthy")

                # ç»Ÿè®¡çŠ¶æ€
                if instance.status == 'available':
                    health_status['available_instances'] += 1
                elif instance.status == 'busy':
                    health_status['busy_instances'] += 1
                elif instance.status == 'unhealthy':
                    health_status['unhealthy_instances'] += 1

                health_status['total_sessions'] += instance.session_count
                health_status['instances'][instance_id] = {
                    'status': instance.status,
                    'session_count': instance.session_count,
                    'last_used': instance.last_used.isoformat()
                }

            except Exception as e:
                logger.error(f"Health check failed for instance {instance_id}: {str(e)}")
                instance.status = 'unhealthy'
                health_status['unhealthy_instances'] += 1

        return health_status

    async def _select_available_instance(self) -> Optional[BrowserInstance]:
        """é€‰æ‹©å¯ç”¨çš„å®ä¾‹"""
        available_instances = [
            instance for instance in self.instances.values()
            if instance.status == 'available' and instance.session_count < instance.max_sessions
        ]

        if not available_instances:
            return None

        # é€‰æ‹©ä¼šè¯æ•°æœ€å°‘çš„å®ä¾‹
        return min(available_instances, key=lambda x: x.session_count)

    async def _create_browser_session(self, instance: BrowserInstance, **kwargs) -> Dict:
        """åˆ›å»ºæµè§ˆå™¨ä¼šè¯"""
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            session_params = {
                'timeout': kwargs.get('timeout', 30000),
                'viewport': kwargs.get('viewport', {'width': 1920, 'height': 1080}),
                'userAgent': kwargs.get('user_agent'),
                'proxy': kwargs.get('proxy'),
                'stealth': True,
                'blockAds': True
            }

            # ç§»é™¤ None å€¼
            session_params = {k: v for k, v in session_params.items() if v is not None}

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{instance.endpoint_url}/session",
                    json=session_params,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        session_data = await response.json()
                        return {
                            'session_id': session_data['id'],
                            'websocket_url': session_data['webSocketDebuggerUrl'],
                            'page_url': session_data.get('url')
                        }
                    else:
                        error_text = await response.text()
                        raise RuntimeError(f"Failed to create session: {response.status} - {error_text}")

        except Exception as e:
            logger.error(f"Failed to create browser session on {instance.endpoint_url}: {str(e)}")
            raise

    async def _close_browser_session(self, instance: BrowserInstance, session_id: str):
        """å…³é—­æµè§ˆå™¨ä¼šè¯"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.delete(
                    f"{instance.endpoint_url}/session/{session_id}",
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status not in [200, 404]:  # 404 è¡¨ç¤ºä¼šè¯å·²ç»ä¸å­˜åœ¨
                        logger.warning(f"Failed to close session {session_id}: {response.status}")

        except Exception as e:
            logger.warning(f"Failed to close browser session {session_id}: {str(e)}")

    async def _check_instance_health(self, instance: BrowserInstance) -> bool:
        """æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{instance.endpoint_url}/health",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    return response.status == 200

        except Exception:
            return False

class ChromeDPPool(BrowserPool):
    """Chrome DevTools Protocol æµè§ˆå™¨æ± """

    def __init__(self, chrome_instances: List[str]):
        self.chrome_instances = chrome_instances
        self.sessions: Dict[str, Dict] = {}
        self._lock = asyncio.Lock()

    async def get_page(self, **kwargs) -> Dict:
        """è·å– Chrome DevTools é¡µé¢"""
        async with self._lock:
            # é€‰æ‹©å¯ç”¨çš„ Chrome å®ä¾‹
            chrome_endpoint = random.choice(self.chrome_instances)

            try:
                # åˆ›å»ºæ–°æ ‡ç­¾é¡µ
                async with aiohttp.ClientSession() as session:
                    async with session.get(f"{chrome_endpoint}/json/new") as response:
                        if response.status == 200:
                            tab_info = await response.json()

                            session_id = tab_info['id']
                            self.sessions[session_id] = {
                                'endpoint': chrome_endpoint,
                                'tab_id': tab_info['id'],
                                'websocket_url': tab_info['webSocketDebuggerUrl'],
                                'created_at': datetime.utcnow()
                            }

                            return {
                                'session_id': session_id,
                                'websocket_url': tab_info['webSocketDebuggerUrl'],
                                'tab_id': tab_info['id'],
                                'created_at': datetime.utcnow().isoformat()
                            }
                        else:
                            raise RuntimeError(f"Failed to create Chrome tab: {response.status}")

            except Exception as e:
                logger.error(f"Failed to create Chrome session: {str(e)}")
                raise

    async def release_page(self, page_info: Dict):
        """é‡Šæ”¾ Chrome é¡µé¢"""
        try:
            session_id = page_info['session_id']

            if session_id in self.sessions:
                session_info = self.sessions[session_id]
                endpoint = session_info['endpoint']
                tab_id = session_info['tab_id']

                # å…³é—­æ ‡ç­¾é¡µ
                async with aiohttp.ClientSession() as session:
                    async with session.get(f"{endpoint}/json/close/{tab_id}") as response:
                        if response.status == 200:
                            logger.debug(f"Closed Chrome tab {tab_id}")
                        else:
                            logger.warning(f"Failed to close Chrome tab {tab_id}: {response.status}")

                # ç§»é™¤ä¼šè¯è®°å½•
                del self.sessions[session_id]

        except Exception as e:
            logger.error(f"Failed to release Chrome session: {str(e)}")

    async def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            'total_instances': len(self.chrome_instances),
            'healthy_instances': 0,
            'active_sessions': len(self.sessions),
            'instances': {}
        }

        for endpoint in self.chrome_instances:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"{endpoint}/json/version",
                        timeout=aiohttp.ClientTimeout(total=5)
                    ) as response:
                        if response.status == 200:
                            version_info = await response.json()
                            health_status['healthy_instances'] += 1
                            health_status['instances'][endpoint] = {
                                'status': 'healthy',
                                'version': version_info.get('Browser', 'unknown')
                            }
                        else:
                            health_status['instances'][endpoint] = {
                                'status': 'unhealthy',
                                'error': f"HTTP {response.status}"
                            }

            except Exception as e:
                health_status['instances'][endpoint] = {
                    'status': 'unhealthy',
                    'error': str(e)
                }

        return health_status

class HybridBrowserManager:
    """æ··åˆæµè§ˆå™¨ç®¡ç†å™¨"""

    def __init__(self, browserless_endpoints: List[str] = None, chrome_endpoints: List[str] = None):
        self.pools = {}

        # åˆå§‹åŒ– Browserless æ± 
        if browserless_endpoints:
            self.pools['browserless'] = BrowserlessPool(browserless_endpoints)

        # åˆå§‹åŒ– Chrome DevTools æ± 
        if chrome_endpoints:
            self.pools['chrome'] = ChromeDPPool(chrome_endpoints)

        # é»˜è®¤ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ Browserlessï¼Œå›é€€åˆ° Chrome
        self.fallback_order = ['browserless', 'chrome']

        # è´Ÿè½½å‡è¡¡ç­–ç•¥
        self.load_balancer = LoadBalancer(self.pools)

    async def get_page(self, preferred_pool: str = None, **kwargs) -> Dict:
        """è·å–æµè§ˆå™¨é¡µé¢"""
        try:
            # å¦‚æœæŒ‡å®šäº†é¦–é€‰æ± ï¼Œå…ˆå°è¯•ä½¿ç”¨
            if preferred_pool and preferred_pool in self.pools:
                try:
                    page_info = await self.pools[preferred_pool].get_page(**kwargs)
                    page_info['pool_type'] = preferred_pool
                    return page_info
                except Exception as e:
                    logger.warning(f"Failed to get page from preferred pool {preferred_pool}: {str(e)}")

            # ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨é€‰æ‹©æœ€ä½³æ± 
            selected_pool = await self.load_balancer.select_pool()
            if selected_pool:
                try:
                    page_info = await self.pools[selected_pool].get_page(**kwargs)
                    page_info['pool_type'] = selected_pool
                    return page_info
                except Exception as e:
                    logger.warning(f"Failed to get page from selected pool {selected_pool}: {str(e)}")

            # å›é€€ç­–ç•¥ï¼šæŒ‰é¡ºåºå°è¯•æ‰€æœ‰å¯ç”¨æ± 
            for pool_name in self.fallback_order:
                if pool_name in self.pools:
                    try:
                        page_info = await self.pools[pool_name].get_page(**kwargs)
                        page_info['pool_type'] = pool_name
                        logger.info(f"Successfully got page from fallback pool {pool_name}")
                        return page_info
                    except Exception as e:
                        logger.warning(f"Fallback pool {pool_name} failed: {str(e)}")
                        continue

            raise RuntimeError("All browser pools are unavailable")

        except Exception as e:
            logger.error(f"Failed to get browser page: {str(e)}")
            raise

    async def release_page(self, page_info: Dict):
        """é‡Šæ”¾æµè§ˆå™¨é¡µé¢"""
        try:
            pool_type = page_info.get('pool_type')
            if pool_type and pool_type in self.pools:
                await self.pools[pool_type].release_page(page_info)
            else:
                logger.warning(f"Unknown pool type: {pool_type}")

        except Exception as e:
            logger.error(f"Failed to release browser page: {str(e)}")

    async def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥æ‰€æœ‰æ± """
        health_status = {
            'overall_status': 'healthy',
            'pools': {},
            'total_available_capacity': 0
        }

        healthy_pools = 0
        total_pools = len(self.pools)

        for pool_name, pool in self.pools.items():
            try:
                pool_health = await pool.health_check()
                health_status['pools'][pool_name] = pool_health

                # åˆ¤æ–­æ± æ˜¯å¦å¥åº·
                if pool_name == 'browserless':
                    if pool_health['available_instances'] > 0:
                        healthy_pools += 1
                        health_status['total_available_capacity'] += pool_health['available_instances']
                elif pool_name == 'chrome':
                    if pool_health['healthy_instances'] > 0:
                        healthy_pools += 1
                        health_status['total_available_capacity'] += pool_health['healthy_instances']

            except Exception as e:
                logger.error(f"Health check failed for pool {pool_name}: {str(e)}")
                health_status['pools'][pool_name] = {'status': 'error', 'error': str(e)}

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if healthy_pools == 0:
            health_status['overall_status'] = 'critical'
        elif healthy_pools < total_pools:
            health_status['overall_status'] = 'degraded'

        return health_status

class LoadBalancer:
    """æµè§ˆå™¨æ± è´Ÿè½½å‡è¡¡å™¨"""

    def __init__(self, pools: Dict[str, BrowserPool]):
        self.pools = pools
        self.pool_weights = {
            'browserless': 0.7,  # ä¼˜å…ˆä½¿ç”¨ Browserless
            'chrome': 0.3
        }
        self.health_cache = {}
        self.cache_ttl = 30  # å¥åº·çŠ¶æ€ç¼“å­˜30ç§’

    async def select_pool(self) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³æµè§ˆå™¨æ± """
        try:
            # è·å–æ‰€æœ‰æ± çš„å¥åº·çŠ¶æ€
            pool_scores = {}

            for pool_name, pool in self.pools.items():
                health_info = await self._get_cached_health(pool_name, pool)
                score = await self._calculate_pool_score(pool_name, health_info)
                pool_scores[pool_name] = score

            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ± 
            if pool_scores:
                best_pool = max(pool_scores.items(), key=lambda x: x[1])
                if best_pool[1] > 0:  # ç¡®ä¿å¾—åˆ†å¤§äº0
                    return best_pool[0]

            return None

        except Exception as e:
            logger.error(f"Load balancer selection failed: {str(e)}")
            return None

    async def _get_cached_health(self, pool_name: str, pool: BrowserPool) -> Dict:
        """è·å–ç¼“å­˜çš„å¥åº·çŠ¶æ€"""
        current_time = datetime.utcnow()

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if pool_name in self.health_cache:
            cached_info = self.health_cache[pool_name]
            if (current_time - cached_info['timestamp']).total_seconds() < self.cache_ttl:
                return cached_info['health']

        # è·å–æ–°çš„å¥åº·çŠ¶æ€
        try:
            health_info = await pool.health_check()
            self.health_cache[pool_name] = {
                'health': health_info,
                'timestamp': current_time
            }
            return health_info
        except Exception as e:
            logger.warning(f"Failed to get health for pool {pool_name}: {str(e)}")
            return {}

    async def _calculate_pool_score(self, pool_name: str, health_info: Dict) -> float:
        """è®¡ç®—æ± çš„å¾—åˆ†"""
        try:
            base_weight = self.pool_weights.get(pool_name, 0.5)

            if pool_name == 'browserless':
                available = health_info.get('available_instances', 0)
                total = health_info.get('total_instances', 1)
                availability_ratio = available / total if total > 0 else 0

                # è€ƒè™‘ä¼šè¯è´Ÿè½½
                total_sessions = health_info.get('total_sessions', 0)
                max_possible_sessions = total * 10  # å‡è®¾æ¯ä¸ªå®ä¾‹æœ€å¤š10ä¸ªä¼šè¯
                load_factor = 1 - (total_sessions / max_possible_sessions) if max_possible_sessions > 0 else 0

                return base_weight * availability_ratio * load_factor

            elif pool_name == 'chrome':
                healthy = health_info.get('healthy_instances', 0)
                total = health_info.get('total_instances', 1)
                health_ratio = healthy / total if total > 0 else 0

                # Chrome å®ä¾‹é€šå¸¸è´Ÿè½½è¾ƒè½»
                return base_weight * health_ratio * 0.8

            return 0

        except Exception as e:
            logger.warning(f"Failed to calculate score for pool {pool_name}: {str(e)}")
            return 0

# å…¨å±€æµè§ˆå™¨ç®¡ç†å™¨å®ä¾‹
browser_manager = None

async def initialize_browser_manager():
    """åˆå§‹åŒ–æµè§ˆå™¨ç®¡ç†å™¨"""
    global browser_manager

    try:
        # ä»é…ç½®è·å–ç«¯ç‚¹
        browserless_endpoints = [
            "http://browserless-1:3000",
            "http://browserless-2:3000",
            "http://browserless-3:3000"
        ]

        chrome_endpoints = [
            "http://chrome-1:9222",
            "http://chrome-2:9222"
        ]

        browser_manager = HybridBrowserManager(
            browserless_endpoints=browserless_endpoints,
            chrome_endpoints=chrome_endpoints
        )

        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        health_status = await browser_manager.health_check()
        logger.info(f"Browser manager initialized. Health status: {health_status['overall_status']}")

        return browser_manager

    except Exception as e:
        logger.error(f"Failed to initialize browser manager: {str(e)}")
        raise

async def get_browser_page(**kwargs) -> Dict:
    """è·å–æµè§ˆå™¨é¡µé¢çš„ä¾¿æ·å‡½æ•°"""
    global browser_manager

    if not browser_manager:
        browser_manager = await initialize_browser_manager()

    return await browser_manager.get_page(**kwargs)

async def release_browser_page(page_info: Dict):
    """é‡Šæ”¾æµè§ˆå™¨é¡µé¢çš„ä¾¿æ·å‡½æ•°"""
    global browser_manager

    if browser_manager:
        await browser_manager.release_page(page_info)
```

### 4. å¼‚æ­¥ Redis æ¶æ„

#### 4.1 å¼‚æ­¥ Redis å®¢æˆ·ç«¯è¯´æ˜

**é‡è¦**: å¼‚æ­¥ Redis å®¢æˆ·ç«¯å·²ç»Ÿä¸€åˆ° `core.redis_manager.AsyncRedisManager`ã€‚

**ä½¿ç”¨æ–¹æ³•**:
```python
# æ¨èä½¿ç”¨æ–¹å¼
from core.redis_manager import get_async_redis

# è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯
redis_client = await get_async_redis()

# åŸºæœ¬æ“ä½œç¤ºä¾‹
await redis_client.set("key", "value")
value = await redis_client.get("key")
await redis_client.hset("hash", "field", "value")
```

**é…ç½®è¯´æ˜**:
```python
# Redisé…ç½®é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=${REDIS_PASSWORD}
REDIS_MAX_CONNECTIONS=20
```

**é‡è¦å˜æ›´**:
- âœ… **`AsyncRedisClient` ç±»å·²ç‰©ç†åˆ é™¤** - é˜²æ­¢è¯¯ç”¨å’ŒAPIåŒè½¨é—®é¢˜
- âœ… **ç»Ÿä¸€ä½¿ç”¨ `AsyncRedisManager`** - é€šè¿‡ `get_async_redis()` è·å–å®ä¾‹
- âœ… **CI æ£€æŸ¥ç¡®ä¿é›¶æ®‹ç•™** - `grep -R "class AsyncRedisClient"` å¿…é¡»è¿”å› 0

**è¿ç§»æŒ‡å—**:
```python
# âŒ æ—§ä»£ç  (å·²åˆ é™¤)
# from core.async_redis import AsyncRedisClient
# client = AsyncRedisClient(config)

# âœ… æ–°ä»£ç  (æ¨è)
from core.redis_manager import get_async_redis
client = await get_async_redis()
```



#### 4.2 Redis Streams æ•°æ®æ‘„å–ç®¡é“
```python
# core/data_ingestion.py
import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import uuid
from core.redis_manager import get_async_redis
from core.data_manager import DataManager

logger = logging.getLogger(__name__)

class MessageStatus(Enum):
    """æ¶ˆæ¯çŠ¶æ€"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    DEAD_LETTER = "dead_letter"

@dataclass
class StreamMessage:
    """æµæ¶ˆæ¯"""
    id: str
    stream_name: str
    data: Dict[str, Any]
    timestamp: datetime
    retry_count: int = 0
    status: MessageStatus = MessageStatus.PENDING
    error_message: Optional[str] = None

class DataIngestionPipeline:
    """æ•°æ®æ‘„å–ç®¡é“"""

    def __init__(self, redis_client, data_manager: DataManager):
        self.redis = redis_client
        self.data_manager = data_manager

        # æµé…ç½®
        self.streams = {
            'tweets': 'weget:stream:tweets',
            'users': 'weget:stream:users',
            'relationships': 'weget:stream:relationships'
        }

        # æ¶ˆè´¹è€…ç»„é…ç½®
        self.consumer_groups = {
            'validation': 'validation-group',
            'storage': 'storage-group',
            'relationship': 'relationship-group'
        }

        # æ­»ä¿¡é˜Ÿåˆ—
        self.dead_letter_queue = 'weget:dlq'
        self.max_retries = 3

        # å¤„ç†å™¨æ˜ å°„
        self.processors = {
            'tweets': self._process_tweet_message,
            'users': self._process_user_message,
            'relationships': self._process_relationship_message
        }

        # éªŒè¯å™¨æ˜ å°„
        self.validators = {
            'tweets': self._validate_tweet_data,
            'users': self._validate_user_data,
            'relationships': self._validate_relationship_data
        }

    async def initialize(self):
        """åˆå§‹åŒ–ç®¡é“"""
        try:
            # åˆ›å»ºæ¶ˆè´¹è€…ç»„
            for stream_name, stream_key in self.streams.items():
                for group_name in self.consumer_groups.values():
                    await self.redis.xgroup_create(
                        stream_key,
                        group_name,
                        id="0",
                        mkstream=True
                    )

            logger.info("Data ingestion pipeline initialized")

        except Exception as e:
            logger.error(f"Failed to initialize data ingestion pipeline: {str(e)}")
            raise

    async def publish_message(self, stream_type: str, data: Dict[str, Any],
                            message_id: str = None) -> str:
        """å‘å¸ƒæ¶ˆæ¯åˆ°æµ"""
        try:
            if stream_type not in self.streams:
                raise ValueError(f"Unknown stream type: {stream_type}")

            stream_key = self.streams[stream_type]

            # å‡†å¤‡æ¶ˆæ¯æ•°æ®
            message_data = {
                'type': stream_type,
                'data': json.dumps(data, default=str),
                'timestamp': datetime.utcnow().isoformat(),
                'message_id': message_id or str(uuid.uuid4())
            }

            # å‘å¸ƒåˆ°æµ
            msg_id = await self.redis.xadd(
                stream_key,
                message_data,
                maxlen=10000  # é™åˆ¶æµé•¿åº¦
            )

            logger.debug(f"Published message {msg_id} to stream {stream_type}")
            return msg_id

        except Exception as e:
            logger.error(f"Failed to publish message to stream {stream_type}: {str(e)}")
            raise

    async def start_validation_worker(self, worker_id: str):
        """å¯åŠ¨éªŒè¯å·¥ä½œå™¨"""
        logger.info(f"Starting validation worker {worker_id}")

        while True:
            try:
                # ä»æ‰€æœ‰æµè¯»å–æ¶ˆæ¯
                streams_to_read = {
                    stream_key: ">" for stream_key in self.streams.values()
                }

                messages = await self.redis.xreadgroup(
                    self.consumer_groups['validation'],
                    worker_id,
                    streams_to_read,
                    count=10,
                    block=1000  # 1ç§’è¶…æ—¶
                )

                if messages:
                    await self._process_validation_messages(messages, worker_id)

            except Exception as e:
                logger.error(f"Validation worker {worker_id} error: {str(e)}")
                await asyncio.sleep(5)  # é”™è¯¯åç­‰å¾…5ç§’

    async def start_storage_worker(self, worker_id: str):
        """å¯åŠ¨å­˜å‚¨å·¥ä½œå™¨"""
        logger.info(f"Starting storage worker {worker_id}")

        while True:
            try:
                # ä»éªŒè¯åçš„æµè¯»å–æ¶ˆæ¯
                validated_stream = "weget:stream:validated"

                messages = await self.redis.xreadgroup(
                    self.consumer_groups['storage'],
                    worker_id,
                    {validated_stream: ">"},
                    count=50,  # æ‰¹é‡å¤„ç†
                    block=1000
                )

                if messages:
                    await self._process_storage_messages(messages, worker_id)

            except Exception as e:
                logger.error(f"Storage worker {worker_id} error: {str(e)}")
                await asyncio.sleep(5)

    async def start_relationship_worker(self, worker_id: str):
        """å¯åŠ¨å…³ç³»å¤„ç†å·¥ä½œå™¨"""
        logger.info(f"Starting relationship worker {worker_id}")

        while True:
            try:
                # ä»å­˜å‚¨åçš„æµè¯»å–æ¶ˆæ¯
                stored_stream = "weget:stream:stored"

                messages = await self.redis.xreadgroup(
                    self.consumer_groups['relationship'],
                    worker_id,
                    {stored_stream: ">"},
                    count=20,
                    block=1000
                )

                if messages:
                    await self._process_relationship_messages(messages, worker_id)

            except Exception as e:
                logger.error(f"Relationship worker {worker_id} error: {str(e)}")
                await asyncio.sleep(5)

    async def _process_validation_messages(self, messages: List, worker_id: str):
        """å¤„ç†éªŒè¯æ¶ˆæ¯"""
        for stream_name, stream_messages in messages:
            for message_id, fields in stream_messages:
                try:
                    # è§£ææ¶ˆæ¯
                    message_type = fields.get('type')
                    data = json.loads(fields.get('data', '{}'))

                    # éªŒè¯æ•°æ®
                    validator = self.validators.get(message_type)
                    if validator:
                        is_valid, validation_result = await validator(data)

                        if is_valid:
                            # å‘é€åˆ°éªŒè¯åçš„æµ
                            await self._forward_to_validated_stream(
                                message_type, validation_result, fields
                            )
                        else:
                            # å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                            await self._send_to_dead_letter_queue(
                                message_id, fields, f"Validation failed: {validation_result}"
                            )

                    # ç¡®è®¤æ¶ˆæ¯
                    await self.redis.xack(
                        stream_name,
                        self.consumer_groups['validation'],
                        message_id
                    )

                except Exception as e:
                    logger.error(f"Failed to process validation message {message_id}: {str(e)}")
                    await self._handle_message_error(stream_name, message_id, fields, str(e))

    async def _process_storage_messages(self, messages: List, worker_id: str):
        """å¤„ç†å­˜å‚¨æ¶ˆæ¯"""
        batch_data = {'tweets': [], 'users': [], 'relationships': []}
        message_ids = []

        for stream_name, stream_messages in messages:
            for message_id, fields in stream_messages:
                try:
                    message_type = fields.get('type')
                    data = json.loads(fields.get('data', '{}'))

                    if message_type in batch_data:
                        batch_data[message_type].append(data)
                        message_ids.append((stream_name, message_id))

                except Exception as e:
                    logger.error(f"Failed to parse storage message {message_id}: {str(e)}")

        # æ‰¹é‡å­˜å‚¨
        for data_type, data_list in batch_data.items():
            if data_list:
                try:
                    result = await self.data_manager.save_batch(data_type, data_list)
                    logger.info(f"Stored {result['success']} {data_type} records")

                    # å‘é€æˆåŠŸå­˜å‚¨çš„æ•°æ®åˆ°ä¸‹ä¸€ä¸ªæµ
                    for data_item in data_list:
                        await self._forward_to_stored_stream(data_type, data_item)

                except Exception as e:
                    logger.error(f"Failed to store {data_type} batch: {str(e)}")

        # ç¡®è®¤æ‰€æœ‰æ¶ˆæ¯
        for stream_name, message_id in message_ids:
            try:
                await self.redis.xack(
                    stream_name,
                    self.consumer_groups['storage'],
                    message_id
                )
            except Exception as e:
                logger.error(f"Failed to ack message {message_id}: {str(e)}")

    async def _process_relationship_messages(self, messages: List, worker_id: str):
        """å¤„ç†å…³ç³»æ¶ˆæ¯"""
        for stream_name, stream_messages in messages:
            for message_id, fields in stream_messages:
                try:
                    message_type = fields.get('type')
                    data = json.loads(fields.get('data', '{}'))

                    # æå–å…³ç³»æ•°æ®
                    relationships = await self._extract_relationships(message_type, data)

                    if relationships:
                        # å­˜å‚¨å…³ç³»æ•°æ®
                        await self.data_manager.save_batch('relationships', relationships)
                        logger.debug(f"Processed {len(relationships)} relationships from {message_type}")

                    # ç¡®è®¤æ¶ˆæ¯
                    await self.redis.xack(
                        stream_name,
                        self.consumer_groups['relationship'],
                        message_id
                    )

                except Exception as e:
                    logger.error(f"Failed to process relationship message {message_id}: {str(e)}")

    async def _validate_tweet_data(self, data: Dict) -> tuple[bool, Any]:
        """éªŒè¯æ¨æ–‡æ•°æ®"""
        try:
            required_fields = ['tweet_id', 'user_id', 'content']
            for field in required_fields:
                if field not in data or not data[field]:
                    return False, f"Missing required field: {field}"

            # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
            cleaned_data = {
                'tweet_id': str(data['tweet_id']),
                'user_id': str(data['user_id']),
                'content': data['content'].strip(),
                'created_at': data.get('created_at'),
                'retweet_count': max(0, int(data.get('retweet_count', 0))),
                'like_count': max(0, int(data.get('like_count', 0))),
                'reply_count': max(0, int(data.get('reply_count', 0))),
                'hashtags': data.get('hashtags', []),
                'user_mentions': data.get('user_mentions', []),
                'urls': data.get('urls', [])
            }

            return True, cleaned_data

        except Exception as e:
            return False, f"Validation error: {str(e)}"

    async def _validate_user_data(self, data: Dict) -> tuple[bool, Any]:
        """éªŒè¯ç”¨æˆ·æ•°æ®"""
        try:
            required_fields = ['user_id', 'username']
            for field in required_fields:
                if field not in data or not data[field]:
                    return False, f"Missing required field: {field}"

            cleaned_data = {
                'user_id': str(data['user_id']),
                'username': data['username'].strip(),
                'display_name': data.get('display_name', '').strip(),
                'followers_count': max(0, int(data.get('followers_count', 0))),
                'following_count': max(0, int(data.get('following_count', 0))),
                'verified': bool(data.get('verified', False))
            }

            return True, cleaned_data

        except Exception as e:
            return False, f"Validation error: {str(e)}"

    async def _validate_relationship_data(self, data: Dict) -> tuple[bool, Any]:
        """éªŒè¯å…³ç³»æ•°æ®"""
        try:
            required_fields = ['from_user_id', 'to_user_id', 'relationship_type']
            for field in required_fields:
                if field not in data or not data[field]:
                    return False, f"Missing required field: {field}"

            valid_types = ['follows', 'mentions', 'replies', 'retweets', 'quotes']
            if data['relationship_type'] not in valid_types:
                return False, f"Invalid relationship type: {data['relationship_type']}"

            cleaned_data = {
                'from_user_id': str(data['from_user_id']),
                'to_user_id': str(data['to_user_id']),
                'relationship_type': data['relationship_type'],
                'tweet_id': str(data['tweet_id']) if data.get('tweet_id') else None
            }

            return True, cleaned_data

        except Exception as e:
            return False, f"Validation error: {str(e)}"
```

### 5. OpenTelemetry å¯è§‚æµ‹æ€§

#### 5.1 OpenTelemetry é…ç½®
```python
# core/observability.py
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.pymongo import PymongoInstrumentor
from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes
import logging
from typing import Dict, Optional
import os

logger = logging.getLogger(__name__)

class ObservabilityManager:
    """å¯è§‚æµ‹æ€§ç®¡ç†å™¨"""

    def __init__(self, service_name: str = "weget-scraper", service_version: str = "1.0.0"):
        self.service_name = service_name
        self.service_version = service_version
        self.tracer = None
        self.meter = None

        # é…ç½®èµ„æº
        self.resource = Resource.create({
            ResourceAttributes.SERVICE_NAME: service_name,
            ResourceAttributes.SERVICE_VERSION: service_version,
            ResourceAttributes.SERVICE_NAMESPACE: "weget",
            ResourceAttributes.DEPLOYMENT_ENVIRONMENT: os.getenv("ENVIRONMENT", "development")
        })

    def initialize_tracing(self, jaeger_endpoint: str = None):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è¿½è¸ª"""
        try:
            # è®¾ç½®è¿½è¸ªæä¾›è€…
            trace.set_tracer_provider(TracerProvider(resource=self.resource))

            # é…ç½® Jaeger å¯¼å‡ºå™¨
            jaeger_endpoint = jaeger_endpoint or os.getenv("JAEGER_ENDPOINT", "http://jaeger:14268/api/traces")
            jaeger_exporter = JaegerExporter(
                endpoint=jaeger_endpoint,
                collector_endpoint=jaeger_endpoint
            )

            # æ·»åŠ æ‰¹é‡å¤„ç†å™¨
            span_processor = BatchSpanProcessor(jaeger_exporter)
            trace.get_tracer_provider().add_span_processor(span_processor)

            # è·å–è¿½è¸ªå™¨
            self.tracer = trace.get_tracer(self.service_name, self.service_version)

            # è‡ªåŠ¨ä»ªè¡¨åŒ–
            self._setup_auto_instrumentation()

            logger.info("OpenTelemetry tracing initialized")

        except Exception as e:
            logger.error(f"Failed to initialize tracing: {str(e)}")
            raise

    def initialize_metrics(self, prometheus_port: int = 8000):
        """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†"""
        try:
            # è®¾ç½®æŒ‡æ ‡æä¾›è€…
            prometheus_reader = PrometheusMetricReader(port=prometheus_port)
            metrics.set_meter_provider(MeterProvider(
                resource=self.resource,
                metric_readers=[prometheus_reader]
            ))

            # è·å–æŒ‡æ ‡å™¨
            self.meter = metrics.get_meter(self.service_name, self.service_version)

            logger.info(f"OpenTelemetry metrics initialized on port {prometheus_port}")

        except Exception as e:
            logger.error(f"Failed to initialize metrics: {str(e)}")
            raise

    def _setup_auto_instrumentation(self):
        """è®¾ç½®è‡ªåŠ¨ä»ªè¡¨åŒ–"""
        try:
            # Redis ä»ªè¡¨åŒ–
            RedisInstrumentor().instrument()

            # MongoDB ä»ªè¡¨åŒ–
            PymongoInstrumentor().instrument()

            # HTTP å®¢æˆ·ç«¯ä»ªè¡¨åŒ–
            AioHttpClientInstrumentor().instrument()

            logger.info("Auto-instrumentation setup completed")

        except Exception as e:
            logger.warning(f"Failed to setup auto-instrumentation: {str(e)}")

    def get_tracer(self):
        """è·å–è¿½è¸ªå™¨"""
        return self.tracer

    def get_meter(self):
        """è·å–æŒ‡æ ‡å™¨"""
        return self.meter

# å…¨å±€å¯è§‚æµ‹æ€§ç®¡ç†å™¨
observability = ObservabilityManager()

# è£…é¥°å™¨ç”¨äºè¿½è¸ªå‡½æ•°
def trace_function(operation_name: str = None):
    """å‡½æ•°è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        async def async_wrapper(*args, **kwargs):
            if not observability.tracer:
                return await func(*args, **kwargs)

            span_name = operation_name or f"{func.__module__}.{func.__name__}"
            with observability.tracer.start_as_current_span(span_name) as span:
                try:
                    # æ·»åŠ å‡½æ•°å‚æ•°ä½œä¸ºå±æ€§
                    if args:
                        span.set_attribute("function.args_count", len(args))
                    if kwargs:
                        span.set_attribute("function.kwargs_count", len(kwargs))

                    result = await func(*args, **kwargs)
                    span.set_attribute("function.success", True)
                    return result

                except Exception as e:
                    span.set_attribute("function.success", False)
                    span.set_attribute("function.error", str(e))
                    span.record_exception(e)
                    raise

        def sync_wrapper(*args, **kwargs):
            if not observability.tracer:
                return func(*args, **kwargs)

            span_name = operation_name or f"{func.__module__}.{func.__name__}"
            with observability.tracer.start_as_current_span(span_name) as span:
                try:
                    if args:
                        span.set_attribute("function.args_count", len(args))
                    if kwargs:
                        span.set_attribute("function.kwargs_count", len(kwargs))

                    result = func(*args, **kwargs)
                    span.set_attribute("function.success", True)
                    return result

                except Exception as e:
                    span.set_attribute("function.success", False)
                    span.set_attribute("function.error", str(e))
                    span.record_exception(e)
                    raise

        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper

    return decorator

# æŒ‡æ ‡æ”¶é›†å™¨
class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.meter = observability.get_meter()
        if self.meter:
            # åˆ›å»ºæŒ‡æ ‡
            self.scraping_requests = self.meter.create_counter(
                name="scraping_requests_total",
                description="Total number of scraping requests",
                unit="1"
            )

            self.scraping_duration = self.meter.create_histogram(
                name="scraping_duration_seconds",
                description="Duration of scraping operations",
                unit="s"
            )

            self.account_health = self.meter.create_gauge(
                name="account_health_score",
                description="Account health score",
                unit="1"
            )

            self.proxy_success_rate = self.meter.create_gauge(
                name="proxy_success_rate",
                description="Proxy success rate",
                unit="1"
            )

            self.data_ingestion_rate = self.meter.create_counter(
                name="data_ingestion_total",
                description="Total data ingestion events",
                unit="1"
            )

            self.browser_pool_usage = self.meter.create_gauge(
                name="browser_pool_usage",
                description="Browser pool usage percentage",
                unit="1"
            )

    def record_scraping_request(self, target_type: str, success: bool):
        """è®°å½•çˆ¬å–è¯·æ±‚"""
        if self.scraping_requests:
            self.scraping_requests.add(1, {
                "target_type": target_type,
                "success": str(success).lower()
            })

    def record_scraping_duration(self, duration: float, target_type: str):
        """è®°å½•çˆ¬å–æŒç»­æ—¶é—´"""
        if self.scraping_duration:
            self.scraping_duration.record(duration, {
                "target_type": target_type
            })

    def update_account_health(self, account_id: str, health_score: float):
        """æ›´æ–°è´¦å·å¥åº·åº¦"""
        if self.account_health:
            self.account_health.set(health_score, {
                "account_id": account_id
            })

    def update_proxy_success_rate(self, proxy_id: str, success_rate: float):
        """æ›´æ–°ä»£ç†æˆåŠŸç‡"""
        if self.proxy_success_rate:
            self.proxy_success_rate.set(success_rate, {
                "proxy_id": proxy_id
            })

    def record_data_ingestion(self, data_type: str, status: str):
        """è®°å½•æ•°æ®æ‘„å–"""
        if self.data_ingestion_rate:
            self.data_ingestion_rate.add(1, {
                "data_type": data_type,
                "status": status
            })

    def update_browser_pool_usage(self, pool_type: str, usage_percentage: float):
        """æ›´æ–°æµè§ˆå™¨æ± ä½¿ç”¨ç‡"""
        if self.browser_pool_usage:
            self.browser_pool_usage.set(usage_percentage, {
                "pool_type": pool_type
            })

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = MetricsCollector()
```

#### 5.2 PrometheusRule CRD é…ç½®
```yaml
# monitoring/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: weget-alerts
  namespace: weget
  labels:
    app: weget
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: weget.scraping
    interval: 30s
    rules:
    - alert: HighScrapingErrorRate
      expr: |
        (
          rate(scraping_requests_total{success="false"}[5m]) /
          rate(scraping_requests_total[5m])
        ) > 0.1
      for: 5m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "High scraping error rate detected"
        description: "Scraping error rate is {{ $value | humanizePercentage }} for the last 5 minutes"

    - alert: ScrapingLatencyHigh
      expr: |
        histogram_quantile(0.95, rate(scraping_duration_seconds_bucket[5m])) > 30
      for: 5m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "High scraping latency detected"
        description: "95th percentile scraping latency is {{ $value }}s"

    - alert: AccountHealthLow
      expr: |
        avg(account_health_score) < 0.7
      for: 10m
      labels:
        severity: critical
        service: weget
      annotations:
        summary: "Account health is critically low"
        description: "Average account health score is {{ $value | humanizePercentage }}"

    - alert: ProxySuccessRateLow
      expr: |
        avg(proxy_success_rate) < 0.8
      for: 5m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "Proxy success rate is low"
        description: "Average proxy success rate is {{ $value | humanizePercentage }}"

    - alert: BrowserPoolExhausted
      expr: |
        avg(browser_pool_usage) > 0.9
      for: 2m
      labels:
        severity: critical
        service: weget
      annotations:
        summary: "Browser pool is nearly exhausted"
        description: "Browser pool usage is {{ $value | humanizePercentage }}"

    - alert: DataIngestionLag
      expr: |
        rate(data_ingestion_total{status="failed"}[5m]) > 10
      for: 3m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "High data ingestion failure rate"
        description: "Data ingestion failure rate is {{ $value }} events/second"

  - name: weget.infrastructure
    interval: 30s
    rules:
    - alert: RedisConnectionFailure
      expr: |
        up{job="redis"} == 0
      for: 1m
      labels:
        severity: critical
        service: weget
      annotations:
        summary: "Redis connection failure"
        description: "Redis instance {{ $labels.instance }} is down"

    - alert: MongoDBConnectionFailure
      expr: |
        up{job="mongodb"} == 0
      for: 1m
      labels:
        severity: critical
        service: weget
      annotations:
        summary: "MongoDB connection failure"
        description: "MongoDB instance {{ $labels.instance }} is down"

    - alert: HighMemoryUsage
      expr: |
        (
          container_memory_working_set_bytes{container="weget-scraper"} /
          container_spec_memory_limit_bytes{container="weget-scraper"}
        ) > 0.9
      for: 5m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is {{ $value | humanizePercentage }} for container {{ $labels.container }}"

    - alert: HighCPUUsage
      expr: |
        rate(container_cpu_usage_seconds_total{container="weget-scraper"}[5m]) > 0.8
      for: 5m
      labels:
        severity: warning
        service: weget
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is {{ $value | humanizePercentage }} for container {{ $labels.container }}"
```

### 6. æ•°æ®å±‚ä¼˜åŒ–ä¸å†·å­˜å‚¨

#### 6.1 æ•°æ®å½’æ¡£ç®¡ç†å™¨
```python
# core/data_archival.py
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from motor.motor_asyncio import AsyncIOMotorClient
from botocore.exceptions import ClientError
import tempfile
import os
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ArchivalConfig:
    """å½’æ¡£é…ç½®"""
    s3_bucket: str
    s3_prefix: str = "weget-archive"
    aws_access_key_id: Optional[str] = None
    aws_secret_access_key: Optional[str] = None
    aws_region: str = "us-east-1"
    retention_days: Dict[str, int] = None
    batch_size: int = 10000
    compression: str = "snappy"

class DataArchivalManager:
    """æ•°æ®å½’æ¡£ç®¡ç†å™¨"""

    def __init__(self, mongodb_client: AsyncIOMotorClient, config: ArchivalConfig):
        self.mongodb = mongodb_client
        self.db = mongodb_client.weget
        self.config = config

        # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=config.aws_access_key_id,
            aws_secret_access_key=config.aws_secret_access_key,
            region_name=config.aws_region
        )

        # é»˜è®¤ä¿ç•™æœŸ
        self.retention_days = config.retention_days or {
            'tweets': 90,
            'users': 180,
            'relationships': 365
        }

    async def archive_old_data(self, collection_name: str, dry_run: bool = False) -> Dict[str, Any]:
        """å½’æ¡£æ—§æ•°æ®"""
        try:
            retention_days = self.retention_days.get(collection_name, 90)
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)

            logger.info(f"Starting archival for {collection_name}, cutoff date: {cutoff_date}")

            collection = getattr(self.db, collection_name)

            # æŸ¥è¯¢éœ€è¦å½’æ¡£çš„æ•°æ®
            query = {"created_at": {"$lt": cutoff_date}}
            total_count = await collection.count_documents(query)

            if total_count == 0:
                logger.info(f"No data to archive for {collection_name}")
                return {"archived_count": 0, "files_created": 0}

            logger.info(f"Found {total_count} records to archive for {collection_name}")

            if dry_run:
                return {"archived_count": total_count, "files_created": 0, "dry_run": True}

            # åˆ†æ‰¹å¤„ç†æ•°æ®
            archived_count = 0
            files_created = 0

            cursor = collection.find(query).batch_size(self.config.batch_size)

            batch_data = []
            async for document in cursor:
                # è½¬æ¢ ObjectId ä¸ºå­—ç¬¦ä¸²
                document['_id'] = str(document['_id'])

                # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²
                for field, value in document.items():
                    if isinstance(value, datetime):
                        document[field] = value.isoformat()

                batch_data.append(document)

                if len(batch_data) >= self.config.batch_size:
                    # å¤„ç†æ‰¹æ¬¡
                    file_path = await self._archive_batch(collection_name, batch_data, archived_count)
                    if file_path:
                        files_created += 1

                    archived_count += len(batch_data)
                    batch_data = []

                    logger.info(f"Archived {archived_count}/{total_count} records for {collection_name}")

            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_data:
                file_path = await self._archive_batch(collection_name, batch_data, archived_count)
                if file_path:
                    files_created += 1
                archived_count += len(batch_data)

            # åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
            delete_result = await collection.delete_many(query)
            logger.info(f"Deleted {delete_result.deleted_count} archived records from {collection_name}")

            return {
                "archived_count": archived_count,
                "files_created": files_created,
                "deleted_count": delete_result.deleted_count
            }

        except Exception as e:
            logger.error(f"Failed to archive data for {collection_name}: {str(e)}")
            raise

    async def _archive_batch(self, collection_name: str, data: List[Dict], batch_offset: int) -> Optional[str]:
        """å½’æ¡£æ•°æ®æ‰¹æ¬¡"""
        try:
            # åˆ›å»º DataFrame
            df = pd.DataFrame(data)

            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            file_name = f"{collection_name}_{timestamp}_{batch_offset}.parquet"
            s3_key = f"{self.config.s3_prefix}/{collection_name}/year={datetime.utcnow().year}/month={datetime.utcnow().month:02d}/{file_name}"

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as temp_file:
                temp_path = temp_file.name

            try:
                # å†™å…¥ Parquet æ–‡ä»¶
                table = pa.Table.from_pandas(df)
                pq.write_table(
                    table,
                    temp_path,
                    compression=self.config.compression,
                    use_dictionary=True,
                    row_group_size=5000
                )

                # ä¸Šä¼ åˆ° S3
                await self._upload_to_s3(temp_path, s3_key)

                logger.debug(f"Archived batch to {s3_key}")
                return s3_key

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        except Exception as e:
            logger.error(f"Failed to archive batch: {str(e)}")
            return None

    async def _upload_to_s3(self, file_path: str, s3_key: str):
        """ä¸Šä¼ æ–‡ä»¶åˆ° S3"""
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self.s3_client.upload_file,
                file_path,
                self.config.s3_bucket,
                s3_key
            )
        except ClientError as e:
            logger.error(f"Failed to upload {s3_key} to S3: {str(e)}")
            raise

    async def list_archived_files(self, collection_name: str, start_date: datetime = None,
                                 end_date: datetime = None) -> List[Dict]:
        """åˆ—å‡ºå½’æ¡£æ–‡ä»¶"""
        try:
            prefix = f"{self.config.s3_prefix}/{collection_name}/"

            response = self.s3_client.list_objects_v2(
                Bucket=self.config.s3_bucket,
                Prefix=prefix
            )

            files = []
            for obj in response.get('Contents', []):
                key = obj['Key']
                size = obj['Size']
                last_modified = obj['LastModified']

                # è§£ææ–‡ä»¶ä¿¡æ¯
                file_info = {
                    'key': key,
                    'size': size,
                    'last_modified': last_modified,
                    'collection': collection_name
                }

                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                if start_date and last_modified < start_date:
                    continue
                if end_date and last_modified > end_date:
                    continue

                files.append(file_info)

            return files

        except ClientError as e:
            logger.error(f"Failed to list archived files: {str(e)}")
            raise

    async def restore_archived_data(self, s3_key: str, target_collection: str = None) -> int:
        """æ¢å¤å½’æ¡£æ•°æ®"""
        try:
            # ä¸‹è½½æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as temp_file:
                temp_path = temp_file.name

            try:
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    self.s3_client.download_file,
                    self.config.s3_bucket,
                    s3_key,
                    temp_path
                )

                # è¯»å– Parquet æ–‡ä»¶
                df = pd.read_parquet(temp_path)

                # è½¬æ¢å› MongoDB æ–‡æ¡£æ ¼å¼
                documents = df.to_dict('records')

                # æ¢å¤æ—¥æœŸå­—æ®µ
                for doc in documents:
                    for field, value in doc.items():
                        if field.endswith('_at') and isinstance(value, str):
                            try:
                                doc[field] = datetime.fromisoformat(value)
                            except ValueError:
                                pass

                # ç¡®å®šç›®æ ‡é›†åˆ
                if not target_collection:
                    target_collection = s3_key.split('/')[-4]  # ä»è·¯å¾„ä¸­æå–é›†åˆå

                collection = getattr(self.db, target_collection)

                # æ’å…¥æ•°æ®
                if documents:
                    result = await collection.insert_many(documents, ordered=False)
                    logger.info(f"Restored {len(result.inserted_ids)} documents to {target_collection}")
                    return len(result.inserted_ids)

                return 0

            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        except Exception as e:
            logger.error(f"Failed to restore archived data from {s3_key}: {str(e)}")
            raise

    async def get_archival_stats(self) -> Dict[str, Any]:
        """è·å–å½’æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'collections': {},
                'total_archived_size': 0,
                'total_archived_files': 0
            }

            for collection_name in self.retention_days.keys():
                files = await self.list_archived_files(collection_name)

                collection_stats = {
                    'file_count': len(files),
                    'total_size': sum(f['size'] for f in files),
                    'oldest_file': min((f['last_modified'] for f in files), default=None),
                    'newest_file': max((f['last_modified'] for f in files), default=None)
                }

                stats['collections'][collection_name] = collection_stats
                stats['total_archived_size'] += collection_stats['total_size']
                stats['total_archived_files'] += collection_stats['file_count']

            return stats

        except Exception as e:
            logger.error(f"Failed to get archival stats: {str(e)}")
            raise

class ColdDataQueryEngine:
    """å†·æ•°æ®æŸ¥è¯¢å¼•æ“"""

    def __init__(self, archival_manager: DataArchivalManager):
        self.archival_manager = archival_manager

    async def query_archived_data(self, collection_name: str, query_filter: Dict = None,
                                 start_date: datetime = None, end_date: datetime = None,
                                 limit: int = 1000) -> List[Dict]:
        """æŸ¥è¯¢å½’æ¡£æ•°æ®"""
        try:
            # è·å–ç›¸å…³çš„å½’æ¡£æ–‡ä»¶
            files = await self.archival_manager.list_archived_files(
                collection_name, start_date, end_date
            )

            if not files:
                return []

            results = []
            processed_count = 0

            for file_info in files:
                if processed_count >= limit:
                    break

                # ä¸‹è½½å¹¶æŸ¥è¯¢æ–‡ä»¶
                file_results = await self._query_parquet_file(
                    file_info['key'], query_filter, limit - processed_count
                )

                results.extend(file_results)
                processed_count += len(file_results)

            return results[:limit]

        except Exception as e:
            logger.error(f"Failed to query archived data: {str(e)}")
            raise

    async def _query_parquet_file(self, s3_key: str, query_filter: Dict = None,
                                 limit: int = 1000) -> List[Dict]:
        """æŸ¥è¯¢å•ä¸ª Parquet æ–‡ä»¶"""
        try:
            # ä¸‹è½½æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as temp_file:
                temp_path = temp_file.name

            try:
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    self.archival_manager.s3_client.download_file,
                    self.archival_manager.config.s3_bucket,
                    s3_key,
                    temp_path
                )

                # è¯»å–å¹¶è¿‡æ»¤æ•°æ®
                df = pd.read_parquet(temp_path)

                # åº”ç”¨æŸ¥è¯¢è¿‡æ»¤å™¨
                if query_filter:
                    for field, condition in query_filter.items():
                        if isinstance(condition, dict):
                            # å¤„ç† MongoDB é£æ ¼çš„æŸ¥è¯¢
                            for op, value in condition.items():
                                if op == '$eq':
                                    df = df[df[field] == value]
                                elif op == '$ne':
                                    df = df[df[field] != value]
                                elif op == '$gt':
                                    df = df[df[field] > value]
                                elif op == '$gte':
                                    df = df[df[field] >= value]
                                elif op == '$lt':
                                    df = df[df[field] < value]
                                elif op == '$lte':
                                    df = df[df[field] <= value]
                                elif op == '$in':
                                    df = df[df[field].isin(value)]
                        else:
                            df = df[df[field] == condition]

                # é™åˆ¶ç»“æœæ•°é‡
                if len(df) > limit:
                    df = df.head(limit)

                return df.to_dict('records')

            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        except Exception as e:
            logger.error(f"Failed to query parquet file {s3_key}: {str(e)}")
            return []

    async def aggregate_archived_data(self, collection_name: str, pipeline: List[Dict],
                                     start_date: datetime = None, end_date: datetime = None) -> List[Dict]:
        """èšåˆå½’æ¡£æ•°æ®"""
        try:
            # è·å–ç›¸å…³æ–‡ä»¶
            files = await self.archival_manager.list_archived_files(
                collection_name, start_date, end_date
            )

            if not files:
                return []

            # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
            all_data = []
            for file_info in files:
                file_data = await self._query_parquet_file(file_info['key'])
                all_data.extend(file_data)

            if not all_data:
                return []

            # è½¬æ¢ä¸º DataFrame è¿›è¡Œèšåˆ
            df = pd.DataFrame(all_data)

            # ç®€å•çš„èšåˆæ“ä½œå®ç°
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ›´å¤æ‚çš„èšåˆé€»è¾‘
            result = []

            for stage in pipeline:
                if '$group' in stage:
                    group_spec = stage['$group']
                    group_by = group_spec.get('_id')

                    if group_by:
                        grouped = df.groupby(group_by)

                        # å¤„ç†èšåˆæ“ä½œ
                        agg_dict = {}
                        for field, operation in group_spec.items():
                            if field != '_id' and isinstance(operation, dict):
                                for op, source_field in operation.items():
                                    if op == '$sum':
                                        agg_dict[field] = (source_field, 'sum')
                                    elif op == '$avg':
                                        agg_dict[field] = (source_field, 'mean')
                                    elif op == '$count':
                                        agg_dict[field] = (source_field, 'count')

                        if agg_dict:
                            result_df = grouped.agg(dict(agg_dict.values()))
                            result = result_df.to_dict('records')

            return result

        except Exception as e:
            logger.error(f"Failed to aggregate archived data: {str(e)}")
            raise
```

### 7. CI/CD è´¨é‡é—¸é—¨ä¸å®‰å…¨

#### 7.1 GitHub Actions CI/CD ç®¡é“
```yaml
# .github/workflows/ci-cd.yml
name: WeGet CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Code quality checks
      run: |
        # Ruff linting with strict rules
        ruff check . --exit-zero --output-format=github

        # Check for prohibited patterns
        echo "Checking for prohibited patterns..."

        # Fail if 'pass' placeholders found
        if grep -r "pass\s*#\s*TODO" --include="*.py" .; then
          echo "âŒ Found 'pass # TODO' placeholders - these must be implemented"
          exit 1
        fi

        # Fail if hardcoded passwords found
        if grep -r "password=" --include="*.py" --include="*.yaml" --include="*.yml" .; then
          echo "âŒ Found hardcoded passwords - use secrets management"
          exit 1
        fi

        # Fail if direct playwright.launch() found
        if grep -r "playwright.*launch(" --include="*.py" .; then
          echo "âŒ Found direct playwright.launch() calls - use browser pool instead"
          exit 1
        fi

        echo "âœ… All prohibited pattern checks passed"

    - name: Security scanning with Bandit
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . --severity-level medium

    - name: Dependency vulnerability scan
      run: |
        pip-audit --format=json --output=pip-audit-report.json
        pip-audit --desc

    - name: Type checking with mypy
      run: |
        mypy . --ignore-missing-imports --no-strict-optional

    - name: Run tests with coverage
      run: |
        pytest --cov=. --cov-report=xml --cov-report=html --cov-fail-under=80

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

    - name: SonarCloud Scan
      uses: SonarSource/sonarcloud-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: quality-gates
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run TruffleHog secrets scan
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --debug --only-verified

  build-and-scan:
    name: Build and Scan Container
    runs-on: ubuntu-latest
    needs: [quality-gates, security-scan]
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-uri: ${{ steps.build.outputs.image-uri }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Install Cosign
      uses: sigstore/cosign-installer@v3

    - name: Install Syft
      uses: anchore/sbom-action/download-syft@v0

    - name: Generate SBOM
      run: |
        syft ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }} \
          -o spdx-json=sbom.spdx.json \
          -o cyclonedx-json=sbom.cyclonedx.json

    - name: Sign container image
      run: |
        cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}

    - name: Sign SBOM
      run: |
        cosign attest --yes --predicate sbom.spdx.json \
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}

    - name: Verify signatures
      run: |
        cosign verify ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }} \
          --certificate-identity-regexp="https://github.com/${{ github.repository }}" \
          --certificate-oidc-issuer="https://token.actions.githubusercontent.com"

    - name: Container vulnerability scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
        format: 'sarif'
        output: 'container-trivy-results.sarif'

    - name: Upload container scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'container-trivy-results.sarif'

    - name: Upload SBOM artifacts
      uses: actions/upload-artifact@v3
      with:
        name: sbom-files
        path: |
          sbom.spdx.json
          sbom.cyclonedx.json

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-and-scan
    if: github.ref == 'refs/heads/develop'
    environment: staging
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: '1.28.0'

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Verify image signature before deploy
      run: |
        cosign verify ${{ needs.build-and-scan.outputs.image-uri }} \
          --certificate-identity-regexp="https://github.com/${{ github.repository }}" \
          --certificate-oidc-issuer="https://token.actions.githubusercontent.com"

    - name: Deploy to staging
      run: |
        helm upgrade --install weget-staging ./helm/weget \
          --namespace weget-staging \
          --create-namespace \
          --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
          --set image.tag=${{ github.sha }} \
          --set environment=staging \
          --values ./helm/weget/values-staging.yaml \
          --wait --timeout=10m

    - name: Run smoke tests
      run: |
        kubectl wait --for=condition=ready pod -l app=weget -n weget-staging --timeout=300s
        kubectl port-forward svc/weget-staging 8080:8000 -n weget-staging &
        sleep 10
        curl -f http://localhost:8080/health || exit 1

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build-and-scan, deploy-staging]
    if: github.event_name == 'release'
    environment: production
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: '1.28.0'

    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG_PROD }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Verify image signature before deploy
      run: |
        cosign verify ${{ needs.build-and-scan.outputs.image-uri }} \
          --certificate-identity-regexp="https://github.com/${{ github.repository }}" \
          --certificate-oidc-issuer="https://token.actions.githubusercontent.com"

    - name: Blue-Green Deployment
      run: |
        # Deploy to green environment
        helm upgrade --install weget-green ./helm/weget \
          --namespace weget-production \
          --create-namespace \
          --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
          --set image.tag=${{ github.sha }} \
          --set environment=production \
          --set deployment.color=green \
          --values ./helm/weget/values-prod.yaml \
          --wait --timeout=15m

        # Health check
        kubectl wait --for=condition=ready pod -l app=weget,color=green -n weget-production --timeout=600s

        # Switch traffic
        kubectl patch service weget-production -n weget-production \
          -p '{"spec":{"selector":{"color":"green"}}}'

        # Clean up blue environment
        helm uninstall weget-blue -n weget-production || true

        # Rename green to blue for next deployment
        helm upgrade weget-blue ./helm/weget \
          --namespace weget-production \
          --reuse-values \
          --set deployment.color=blue

  security-monitoring:
    name: Security Monitoring Setup
    runs-on: ubuntu-latest
    needs: deploy-production
    if: github.event_name == 'release'
    steps:
    - name: Update security monitoring
      run: |
        # Update Falco rules for new deployment
        echo "Updating Falco security rules..."

        # Update SIEM with new image hashes
        echo "Updating SIEM with deployment information..."

        # Schedule security scan
        echo "Scheduling post-deployment security scan..."
```

#### 7.2 è´¨é‡é—¸é—¨é…ç½®
```yaml
# .github/workflows/quality-gates.yml
name: Quality Gates

on:
  pull_request:
    branches: [ main, develop ]

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install quality tools
      run: |
        pip install ruff black isort mypy bandit safety

    - name: Format check
      run: |
        black --check --diff .
        isort --check-only --diff .

    - name: Lint with Ruff
      run: |
        ruff check . --output-format=github

    - name: Type checking
      run: |
        mypy . --ignore-missing-imports

    - name: Security check
      run: |
        bandit -r . --severity-level medium
        safety check

    - name: Architecture compliance
      run: |
        # Check for architectural violations
        python scripts/check_architecture.py

  test-coverage:
    name: Test Coverage
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio

    - name: Run tests with coverage
      run: |
        pytest --cov=. --cov-report=xml --cov-fail-under=80 --cov-report=term-missing

    - name: Coverage comment
      uses: py-cov-action/python-coverage-comment-action@v3
      with:
        GITHUB_TOKEN: ${{ github.token }}

  dependency-check:
    name: Dependency Security Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Run dependency check
      uses: pypa/gh-action-pip-audit@v1.0.8
      with:
        inputs: requirements.txt

  license-check:
    name: License Compliance
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: License check
      uses: fossa-contrib/fossa-action@v2
      with:
        api-key: ${{ secrets.FOSSA_API_KEY }}
```

#### 7.3 SBOM ç”Ÿæˆå’Œç­¾åè„šæœ¬
```bash
#!/bin/bash
# scripts/generate-sbom.sh

set -euo pipefail

IMAGE_URI="${1:-}"
OUTPUT_DIR="${2:-./sbom}"

if [ -z "$IMAGE_URI" ]; then
    echo "Usage: $0 <image-uri> [output-dir]"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "ğŸ” Generating SBOM for $IMAGE_URI..."

# Generate SPDX SBOM
syft "$IMAGE_URI" -o spdx-json="$OUTPUT_DIR/sbom.spdx.json"

# Generate CycloneDX SBOM
syft "$IMAGE_URI" -o cyclonedx-json="$OUTPUT_DIR/sbom.cyclonedx.json"

# Generate Syft native format
syft "$IMAGE_URI" -o syft-json="$OUTPUT_DIR/sbom.syft.json"

echo "ğŸ“‹ SBOM files generated:"
ls -la "$OUTPUT_DIR"

# Validate SBOM
echo "âœ… Validating SBOM..."
python scripts/validate_sbom.py "$OUTPUT_DIR/sbom.spdx.json"

# Sign SBOM with Cosign
if command -v cosign &> /dev/null; then
    echo "ğŸ” Signing SBOM..."
    cosign attest --predicate "$OUTPUT_DIR/sbom.spdx.json" "$IMAGE_URI"
    echo "âœ… SBOM signed successfully"
else
    echo "âš ï¸  Cosign not found, skipping SBOM signing"
fi

echo "ğŸ‰ SBOM generation completed!"
```

```python
# scripts/validate_sbom.py
#!/usr/bin/env python3
"""SBOM validation script"""

import json
import sys
from typing import Dict, List, Any
import argparse

def validate_spdx_sbom(sbom_path: str) -> bool:
    """Validate SPDX SBOM format and content"""
    try:
        with open(sbom_path, 'r') as f:
            sbom = json.load(f)

        # Check required SPDX fields
        required_fields = [
            'spdxVersion',
            'dataLicense',
            'SPDXID',
            'name',
            'documentNamespace',
            'creationInfo',
            'packages'
        ]

        for field in required_fields:
            if field not in sbom:
                print(f"âŒ Missing required field: {field}")
                return False

        # Validate packages
        packages = sbom.get('packages', [])
        if not packages:
            print("âŒ No packages found in SBOM")
            return False

        # Check for security-relevant information
        security_checks = {
            'has_licenses': False,
            'has_vulnerabilities': False,
            'has_checksums': False
        }

        for package in packages:
            if 'licenseConcluded' in package or 'licenseDeclared' in package:
                security_checks['has_licenses'] = True

            if 'checksums' in package and package['checksums']:
                security_checks['has_checksums'] = True

        # Report findings
        print(f"âœ… SBOM validation passed")
        print(f"ğŸ“¦ Found {len(packages)} packages")
        print(f"ğŸ“„ License information: {'âœ…' if security_checks['has_licenses'] else 'âš ï¸'}")
        print(f"ğŸ”’ Checksums present: {'âœ…' if security_checks['has_checksums'] else 'âš ï¸'}")

        return True

    except Exception as e:
        print(f"âŒ SBOM validation failed: {str(e)}")
        return False

def main():
    parser = argparse.ArgumentParser(description='Validate SBOM files')
    parser.add_argument('sbom_file', help='Path to SBOM file')
    parser.add_argument('--format', choices=['spdx', 'cyclonedx'], default='spdx',
                       help='SBOM format to validate')

    args = parser.parse_args()

    if args.format == 'spdx':
        success = validate_spdx_sbom(args.sbom_file)
    else:
        print(f"âŒ Validation for {args.format} format not implemented yet")
        success = False

    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()
```

## å®æ–½è·¯çº¿å›¾ä¸éªŒæ”¶æ ‡å‡†

### Sprint è§„åˆ’

| Sprint | é‡ç‚¹äº¤ä»˜ | éªŒæ”¶æ ‡å‡† | é¢„è®¡å·¥æœŸ |
|--------|----------|----------|----------|
| **S-1** | æ¶ˆé™¤ pass å ä½ç¬¦ + å•æ–‡æ¡£æ•´åˆ | `pytest -q` å…¨ç»¿ï¼›`rg "pass\s*$" --type py` è¾“å‡ºä¸ºç©º | 1å‘¨ |
| **S-2** | Secrets å•æºåŒ– + æ˜æ–‡æ¸…é›¶ | repo ä¸­æ—  `password@mongodb` ç­‰æ˜æ–‡ï¼›Helm æŠ½è±¡å®Œæ¯• | 1å‘¨ |
| **S-3** | æµè§ˆå™¨æ± å…¨æ›¿æ¢ + Async Redis | 50 å¹¶å‘å‹æµ‹ P95 < 1sï¼›æ— åŒæ­¥ Redis è°ƒç”¨ | 2å‘¨ |
| **S-4** | Observability é—­ç¯ | Grafana èƒ½çœ‹ Traceï¼›å‘Šè­¦èƒ½æ¨é’‰é’‰/Slack | 1å‘¨ |
| **S-5** | å†·çƒ­åˆ†å±‚ & ç´¢å¼•ä¼˜åŒ– | åˆ é™¤ > 90 å¤©æ•°æ®åï¼ŒMongo é›†åˆ size ä¸‹é™ â‰¥ 60% | 1å‘¨ |
| **S-6** | CI é—¸é—¨ & SBOM | ä»»ä½•æœªç­¾åé•œåƒæ‹’ç»éƒ¨ç½²ï¼›è¦†ç›–ç‡ â‰¥ 80% | 1å‘¨ |

### å…³é”®æŠ€æœ¯å†³ç­–

#### 1. æ¶æ„æ¨¡å¼é€‰æ‹©
- **å¾®æœåŠ¡æ¶æ„**: é‡‡ç”¨ Kubernetes + Helm éƒ¨ç½²ï¼Œæ”¯æŒç‹¬ç«‹æ‰©ç¼©å®¹
- **äº‹ä»¶é©±åŠ¨**: Redis Streams å®ç°è§£è€¦çš„æ•°æ®æ‘„å–ç®¡é“
- **æ··åˆå­˜å‚¨**: MongoDB (æ–‡æ¡£) + Neo4j (å…³ç³») + S3 (å½’æ¡£)

#### 2. å¯è§‚æµ‹æ€§ç­–ç•¥
- **åˆ†å¸ƒå¼è¿½è¸ª**: OpenTelemetry + Jaeger å…¨é“¾è·¯è¿½è¸ª
- **æŒ‡æ ‡ç›‘æ§**: Prometheus + Grafana å®æ—¶ç›‘æ§
- **æ—¥å¿—èšåˆ**: ELK Stack é›†ä¸­å¼æ—¥å¿—ç®¡ç†
- **å‘Šè­¦æœºåˆ¶**: PrometheusRule CRD + Webhook é€šçŸ¥

#### 3. å®‰å…¨åˆè§„æªæ–½
- **å¯†é’¥ç®¡ç†**: HashiCorp Vault ç»Ÿä¸€å¯†é’¥ç®¡ç†
- **é•œåƒå®‰å…¨**: Cosign ç­¾å + Syft SBOM ç”Ÿæˆ
- **ç½‘ç»œå®‰å…¨**: NetworkPolicy + PodSecurityPolicy
- **æ•°æ®åˆè§„**: GDPR takedown_at å­—æ®µ + TTL ç´¢å¼•

#### 4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- **æµè§ˆå™¨æ± åŒ–**: Browserless + Chrome-DP æ··åˆæ± 
- **å¼‚æ­¥å¤„ç†**: å…¨é¢ async/await + Redis.asyncio
- **æ•°æ®åˆ†å±‚**: çƒ­æ•°æ® MongoDB + å†·æ•°æ® S3 Parquet
- **æ™ºèƒ½ç¼“å­˜**: Redis å¤šå±‚ç¼“å­˜ + LRU ç­–ç•¥

### ç”Ÿäº§å°±ç»ªæ£€æŸ¥æ¸…å•

#### åŸºç¡€è®¾æ–½å°±ç»ª
- [ ] Kubernetes é›†ç¾¤é…ç½®å®Œæˆ
- [ ] Helm Charts éƒ¨ç½²æµ‹è¯•é€šè¿‡
- [ ] Vault å¯†é’¥ç®¡ç†é…ç½®å®Œæˆ
- [ ] ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
- [ ] ç½‘ç»œç­–ç•¥å’Œå®‰å…¨ç­–ç•¥é…ç½®å®Œæˆ

#### åº”ç”¨å°±ç»ª
- [ ] æ‰€æœ‰ pass å ä½ç¬¦å·²å®ç°
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•è¾¾æ ‡ (50 å¹¶å‘ P95 < 1s)
- [ ] å®‰å…¨æ‰«æé€šè¿‡

#### è¿ç»´å°±ç»ª
- [ ] CI/CD ç®¡é“é…ç½®å®Œæˆ
- [ ] é•œåƒç­¾åå’Œ SBOM ç”Ÿæˆ
- [ ] è“ç»¿éƒ¨ç½²æµç¨‹éªŒè¯
- [ ] ç¾éš¾æ¢å¤è®¡åˆ’åˆ¶å®š
- [ ] è¿ç»´æ‰‹å†Œç¼–å†™å®Œæˆ

#### åˆè§„å°±ç»ª
- [ ] GDPR åˆè§„åŠŸèƒ½æµ‹è¯•
- [ ] æ•°æ®å½’æ¡£æµç¨‹éªŒè¯
- [ ] å®‰å…¨å®¡è®¡æ—¥å¿—é…ç½®
- [ ] è®¿é—®æ§åˆ¶ç­–ç•¥éªŒè¯
- [ ] æ•°æ®ä¿ç•™ç­–ç•¥å®æ–½

### é£é™©ç¼“è§£æªæ–½

#### æŠ€æœ¯é£é™©
1. **æµè§ˆå™¨æ± æ•…éšœ**: å¤šæ± å†—ä½™ + è‡ªåŠ¨æ•…éšœè½¬ç§»
2. **æ•°æ®ä¸¢å¤±**: å¤šå‰¯æœ¬ + å®šæœŸå¤‡ä»½ + å½’æ¡£éªŒè¯
3. **æ€§èƒ½ç“¶é¢ˆ**: æ°´å¹³æ‰©ç¼©å®¹ + è´Ÿè½½å‡è¡¡ + ç¼“å­˜ä¼˜åŒ–
4. **å®‰å…¨æ¼æ´**: å®šæœŸæ‰«æ + è‡ªåŠ¨æ›´æ–° + é›¶ä¿¡ä»»æ¶æ„

#### è¿ç»´é£é™©
1. **éƒ¨ç½²å¤±è´¥**: è“ç»¿éƒ¨ç½² + è‡ªåŠ¨å›æ»š + å¥åº·æ£€æŸ¥
2. **ç›‘æ§ç›²åŒº**: å…¨é“¾è·¯è¿½è¸ª + å¤šç»´åº¦ç›‘æ§ + ä¸»åŠ¨å‘Šè­¦
3. **å¯†é’¥æ³„éœ²**: å¯†é’¥è½®æ¢ + è®¿é—®å®¡è®¡ + æœ€å°æƒé™åŸåˆ™
4. **åˆè§„è¿è§„**: è‡ªåŠ¨åŒ–åˆè§„æ£€æŸ¥ + å®šæœŸå®¡è®¡ + æµç¨‹æ ‡å‡†åŒ–

### æˆåŠŸæŒ‡æ ‡

#### æŠ€æœ¯æŒ‡æ ‡
- **å¯ç”¨æ€§**: 99.9% SLA
- **æ€§èƒ½**: P95 å“åº”æ—¶é—´ < 1s
- **æ‰©å±•æ€§**: æ”¯æŒ 10x æµé‡å¢é•¿
- **å®‰å…¨æ€§**: é›¶å®‰å…¨äº‹ä»¶

#### ä¸šåŠ¡æŒ‡æ ‡
- **æ•°æ®è´¨é‡**: 99%+ æ•°æ®å®Œæ•´æ€§
- **é‡‡é›†æ•ˆç‡**: 10ä¸‡+ æ¨æ–‡/å°æ—¶
- **æˆæœ¬æ•ˆç›Š**: ç›¸æ¯”ç°æœ‰æ–¹æ¡ˆæˆæœ¬é™ä½ 30%
- **åˆè§„æ€§**: 100% GDPR åˆè§„

### åç»­æ¼”è¿›æ–¹å‘

#### çŸ­æœŸä¼˜åŒ– (3-6ä¸ªæœˆ)
- AI é©±åŠ¨çš„åæ£€æµ‹ç­–ç•¥
- å®æ—¶æ•°æ®æµå¤„ç†ä¼˜åŒ–
- å¤šåœ°åŸŸéƒ¨ç½²æ”¯æŒ
- é«˜çº§åˆ†æåŠŸèƒ½

#### ä¸­æœŸå‘å±• (6-12ä¸ªæœˆ)
- æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆ
- è‡ªåŠ¨åŒ–è¿ç»´å¹³å°
- å¤šå¹³å°æ•°æ®æºæ‰©å±•
- ä¼ä¸šçº§ SaaS æœåŠ¡

#### é•¿æœŸæ„¿æ™¯ (1-2å¹´)
- å…¨è‡ªåŠ¨åŒ–æ•°æ®æ™ºèƒ½å¹³å°
- è¡Œä¸šæ ‡å‡†åˆ¶å®šå‚ä¸
- å¼€æºç¤¾åŒºå»ºè®¾
- å›½é™…å¸‚åœºæ‹“å±•

---



### 2. ç›‘æ§æŒ‡æ ‡
- ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡
- è´¦å·å¯ç”¨ç‡  
- ä»£ç†IPå¯ç”¨ç‡
- æ•°æ®é‡‡é›†é€Ÿåº¦
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡

### 3. æ—¥å¿—ç®¡ç†
```python
# utils/logger.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logger(name, log_file, level=logging.INFO):
    handler = RotatingFileHandler(log_file, maxBytes=100*1024*1024, backupCount=5)
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    handler.setFormatter(formatter)
    
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)
    
    return logger
```

## é¢„æœŸæ€§èƒ½æŒ‡æ ‡

- **é‡‡é›†é€Ÿåº¦**: æ¯å°æ—¶10ä¸‡+æ¡æ¨æ–‡
- **è´¦å·åˆ©ç”¨ç‡**: >90%è´¦å·ä¿æŒå¯ç”¨çŠ¶æ€  
- **ä»£ç†åˆ©ç”¨ç‡**: >95%ä»£ç†IPæ­£å¸¸å·¥ä½œ
- **æ•°æ®å‡†ç¡®ç‡**: >99%æ•°æ®å®Œæ•´æ€§
- **ç³»ç»Ÿç¨³å®šæ€§**: 7x24å°æ—¶è¿ç»­è¿è¡Œ

## é£é™©è¯„ä¼°ä¸åº”å¯¹

### æŠ€æœ¯é£é™©
- **APIå˜æ›´**: Twitteræ¥å£å˜åŒ–å¯¼è‡´é‡‡é›†å¤±æ•ˆ
- **åçˆ¬å‡çº§**: å¹³å°åçˆ¬ç­–ç•¥åŠ å¼º
- **è´¦å·å°ç¦**: å¤§è§„æ¨¡è´¦å·è¢«å°é£é™©

### åº”å¯¹æªæ–½  
- å»ºç«‹å¤šå¥—å¤‡ç”¨é‡‡é›†æ–¹æ¡ˆ
- å®æ—¶ç›‘æ§å¹³å°å˜åŒ–ï¼Œå¿«é€Ÿé€‚é…
- ç»´æŠ¤å……è¶³çš„è´¦å·å’Œä»£ç†èµ„æºæ± 
- å»ºç«‹å®Œå–„çš„é£é™©é¢„è­¦æœºåˆ¶

---

## è¯¦ç»†æŠ€æœ¯å®ç°

### 1. ç½‘ç»œæŠ“å–æŠ€æœ¯æ–¹æ¡ˆè¯´æ˜

#### æŠ€æœ¯é€‰æ‹©è¯´æ˜
æœ¬æ–¹æ¡ˆé‡‡ç”¨**ç½‘ç»œæŠ“å–(Web Scraping)**è€Œéå®˜æ–¹APIçš„æ–¹å¼æ¥è·å–Twitteræ•°æ®ï¼Œä¸»è¦åŸå› å¦‚ä¸‹ï¼š

1. **æ— éœ€APIå¯†é’¥**: ä¸ä¾èµ–Twitterå®˜æ–¹APIï¼Œé¿å…ç”³è¯·é™åˆ¶å’Œè´¹ç”¨é—®é¢˜
2. **æ•°æ®å®Œæ•´æ€§**: å¯ä»¥è·å–åˆ°æ¯”APIæ›´ä¸°å¯Œçš„æ•°æ®ï¼ŒåŒ…æ‹¬ä¸€äº›APIä¸æä¾›çš„å­—æ®µ
3. **çµæ´»æ€§**: ä¸å—APIé€Ÿç‡é™åˆ¶å’ŒåŠŸèƒ½é™åˆ¶çš„çº¦æŸ
4. **æˆæœ¬æ§åˆ¶**: é¿å…é«˜æ˜‚çš„APIä½¿ç”¨è´¹ç”¨

#### æŠ“å–æ–¹æ³•ç»„åˆ
æˆ‘ä»¬é‡‡ç”¨**åŒé‡æŠ“å–ç­–ç•¥**ï¼š

**æ–¹æ³•ä¸€ï¼šæµè§ˆå™¨è‡ªåŠ¨åŒ– + é¡µé¢è§£æ**
- ä½¿ç”¨Playwrightæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
- è§£æé¡µé¢DOMå…ƒç´ è·å–å¯è§æ•°æ®
- é€‚ç”¨äºåŸºç¡€æ•°æ®é‡‡é›†å’Œåæ£€æµ‹

**æ–¹æ³•äºŒï¼šç½‘ç»œè¯·æ±‚æ‹¦æˆª + APIæ•°æ®è·å–**
- æ‹¦æˆªæµè§ˆå™¨ä¸Twitteråå°çš„GraphQLè¯·æ±‚
- è·å–ç»“æ„åŒ–çš„JSONæ•°æ®
- æä¾›æ›´å®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®

#### æŠ€æœ¯ä¼˜åŠ¿
1. **é«˜åº¦æ¨¡æ‹ŸçœŸå®ç”¨æˆ·**: é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œè¡Œä¸ºæ¨¡å¼æ¥è¿‘çœŸå®ç”¨æˆ·
2. **æ•°æ®è´¨é‡é«˜**: ç»“åˆé¡µé¢æ•°æ®å’ŒAPIæ•°æ®ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
3. **åæ£€æµ‹èƒ½åŠ›å¼º**: å¤šå±‚åæ£€æµ‹æœºåˆ¶ï¼Œé™ä½è¢«å°é£é™©
4. **å¯æ‰©å±•æ€§å¥½**: æ”¯æŒå¤§è§„æ¨¡åˆ†å¸ƒå¼éƒ¨ç½²

#### ç½‘ç»œæŠ“å–æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WeGet X(Twitter) ç½‘ç»œæŠ“å–ç³»ç»Ÿ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ä»»åŠ¡è°ƒåº¦å±‚     â”‚    â”‚   Cookieæ± ç®¡ç†   â”‚    â”‚   ä»£ç†IPæ± ç®¡ç†   â”‚
â”‚   (Celery)     â”‚    â”‚   (Redis)      â”‚    â”‚   (Redis)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æµè§ˆå™¨èŠ‚ç‚¹ 1   â”‚    â”‚   æµè§ˆå™¨èŠ‚ç‚¹ 2   â”‚    â”‚   æµè§ˆå™¨èŠ‚ç‚¹ N   â”‚
â”‚   (Playwright)  â”‚    â”‚   (Playwright)  â”‚    â”‚   (Playwright)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é¡µé¢æ•°æ®æå–    â”‚    â”‚  ç½‘ç»œè¯·æ±‚æ‹¦æˆª    â”‚    â”‚  æ•°æ®éªŒè¯æ¸…æ´—    â”‚
â”‚  (DOMè§£æ)     â”‚    â”‚  (GraphQL)     â”‚    â”‚  (Pydantic)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ•°æ®æ‘„å–ç®¡é“   â”‚
                    â”‚ (Redis Streams) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ··åˆå­˜å‚¨å±‚     â”‚
                    â”‚ MongoDB + Neo4j â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•°æ®æµå‘è¯´æ˜ï¼š
1. ä»»åŠ¡è°ƒåº¦å™¨åˆ†é…é‡‡é›†ä»»åŠ¡åˆ°å„ä¸ªæµè§ˆå™¨èŠ‚ç‚¹
2. æµè§ˆå™¨èŠ‚ç‚¹ä½¿ç”¨Cookieæ± ä¸­çš„è´¦å·å’Œä»£ç†IPæ± ä¸­çš„ä»£ç†
3. åŒæ—¶è¿›è¡Œé¡µé¢DOMè§£æå’Œç½‘ç»œè¯·æ±‚æ‹¦æˆª
4. æ•°æ®éªŒè¯åè¿›å…¥è§£è€¦çš„æ•°æ®æ‘„å–ç®¡é“
5. æœ€ç»ˆå­˜å‚¨åˆ°MongoDB(å†…å®¹)å’ŒNeo4j(å…³ç³»)
```

#### å…³é”®æŠ€æœ¯ç»„ä»¶è¯´æ˜

**æµè§ˆå™¨è‡ªåŠ¨åŒ–å±‚**
- **Playwright**: æ§åˆ¶Chromeæµè§ˆå™¨ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
- **åæ£€æµ‹è„šæœ¬**: ä¿®æ”¹æµè§ˆå™¨æŒ‡çº¹ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºè‡ªåŠ¨åŒ–å·¥å…·
- **éšæœºåŒ–ç­–ç•¥**: éšæœºUser-Agentã€è§†å£å¤§å°ã€æ“ä½œé—´éš”ç­‰

**ç½‘ç»œæ‹¦æˆªå±‚**
- **è¯·æ±‚ç›‘å¬**: ç›‘å¬æµè§ˆå™¨å‘å‡ºçš„æ‰€æœ‰ç½‘ç»œè¯·æ±‚
- **GraphQLæ‹¦æˆª**: ç‰¹åˆ«å…³æ³¨Twitterçš„GraphQL APIè¯·æ±‚
- **æ•°æ®æå–**: ä»æ‹¦æˆªçš„å“åº”ä¸­æå–ç»“æ„åŒ–JSONæ•°æ®

**æ•°æ®èåˆå±‚**
- **åŒæºåˆå¹¶**: å°†é¡µé¢æ•°æ®å’ŒAPIæ•°æ®è¿›è¡Œæ™ºèƒ½åˆå¹¶
- **æ•°æ®è¡¥å…¨**: ç”¨APIæ•°æ®è¡¥å…¨é¡µé¢æ•°æ®ä¸­ç¼ºå¤±çš„å­—æ®µ
- **å»é‡å¤„ç†**: ç¡®ä¿åŒä¸€æ¡æ¨æ–‡ä¸ä¼šé‡å¤é‡‡é›†

### 2. Twitteré¡µé¢æŠ“å–é…ç½®

#### ç½‘é¡µæŠ“å–ç›®æ ‡é¡µé¢é…ç½®
```python
# config/twitter_pages.py
X_SCRAPING_TARGETS = {
    'search_page': {
        'url_template': 'https://x.com/search?q={query}&src=typed_query&f={search_type}',
        'search_types': {
            'latest': 'live',
            'top': 'top',
            'people': 'user',
            'photos': 'image',
            'videos': 'video'
        },
        'selectors': {
            'tweet_container': '[data-testid="tweet"]',
            'tweet_text': '[data-testid="tweetText"]',
            'tweet_time': 'time',
            'user_name': '[data-testid="User-Name"]',
            'retweet_count': '[data-testid="retweet"]',
            'like_count': '[data-testid="like"]',
            'reply_count': '[data-testid="reply"]',
            'load_more_button': '[aria-label="Loading seems to be taking a while"]'
        }
    },
    'profile_page': {
        'url_template': 'https://x.com/{username}',
        'selectors': {
            'user_name': '[data-testid="UserName"]',
            'user_handle': '[data-testid="UserScreenName"]',
            'user_bio': '[data-testid="UserDescription"]',
            'followers_count': 'a[href$="/followers"] span',
            'following_count': 'a[href$="/following"] span',
            'tweet_count': '[data-testid="UserTweets"] span',
            'profile_image': '[data-testid="UserAvatar"] img',
            'verified_badge': '[data-testid="verifiedBadge"]',
            'tweet_container': '[data-testid="tweet"]'
        }
    },
    'tweet_detail_page': {
        'url_template': 'https://x.com/{username}/status/{tweet_id}',
        'selectors': {
            'main_tweet': '[data-testid="tweet"]',
            'reply_container': '[data-testid="tweet"]',
            'tweet_text': '[data-testid="tweetText"]',
            'tweet_stats': '[role="group"]',
            'media_container': '[data-testid="tweetPhoto"], [data-testid="videoPlayer"]',
            'quote_tweet': '[data-testid="quoteTweet"]'
        }
    },
    'followers_page': {
        'url_template': 'https://x.com/{username}/followers',
        'selectors': {
            'user_cell': '[data-testid="UserCell"]',
            'user_name': '[data-testid="UserName"]',
            'user_handle': '[data-testid="UserScreenName"]',
            'follow_button': '[data-testid="followButton"]',
            'user_bio': '[data-testid="UserDescription"]'
        }
    },
    'following_page': {
        'url_template': 'https://x.com/{username}/following',
        'selectors': {
            'user_cell': '[data-testid="UserCell"]',
            'user_name': '[data-testid="UserName"]',
            'user_handle': '[data-testid="UserScreenName"]',
            'follow_button': '[data-testid="followButton"]',
            'user_bio': '[data-testid="UserDescription"]'
        }
    }
}

# ç½‘ç»œè¯·æ±‚æ‹¦æˆªé…ç½®
NETWORK_INTERCEPT_PATTERNS = {
    # å½“æµè§ˆå™¨è®¿é—®æœç´¢é¡µé¢æ—¶ï¼Œä¼šè‡ªåŠ¨å‘å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œæˆ‘ä»¬æ‹¦æˆªå®ƒçš„å“åº”
    'browser_search_request': {
        'pattern': r'https://x\.com/i/api/graphql/.*/SearchTimeline',
        'description': 'æ‹¦æˆªæµè§ˆå™¨æœç´¢é¡µé¢æ—¶è‡ªåŠ¨å‘å‡ºçš„GraphQLè¯·æ±‚',
        'trigger_action': 'ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­è®¿é—®æœç´¢é¡µé¢æ—¶è‡ªåŠ¨è§¦å‘',
        'data_path': ['data', 'search_by_raw_query', 'search_timeline', 'timeline']
    },
    # å½“æµè§ˆå™¨è®¿é—®ç”¨æˆ·ä¸»é¡µæ—¶ï¼Œä¼šè‡ªåŠ¨å‘å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œæˆ‘ä»¬æ‹¦æˆªå®ƒçš„å“åº”
    'browser_user_request': {
        'pattern': r'https://x\.com/i/api/graphql/.*/UserByScreenName',
        'description': 'æ‹¦æˆªæµè§ˆå™¨è®¿é—®ç”¨æˆ·ä¸»é¡µæ—¶è‡ªåŠ¨å‘å‡ºçš„GraphQLè¯·æ±‚',
        'trigger_action': 'ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­è®¿é—®ç”¨æˆ·ä¸»é¡µæ—¶è‡ªåŠ¨è§¦å‘',
        'data_path': ['data', 'user', 'result']
    },
    # å½“æµè§ˆå™¨åŠ è½½ç”¨æˆ·æ¨æ–‡æ—¶ï¼Œä¼šè‡ªåŠ¨å‘å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œæˆ‘ä»¬æ‹¦æˆªå®ƒçš„å“åº”
    'browser_user_tweets_request': {
        'pattern': r'https://x\.com/i/api/graphql/.*/UserTweets',
        'description': 'æ‹¦æˆªæµè§ˆå™¨åŠ è½½ç”¨æˆ·æ¨æ–‡æ—¶è‡ªåŠ¨å‘å‡ºçš„GraphQLè¯·æ±‚',
        'trigger_action': 'æµè§ˆå™¨æ»šåŠ¨åŠ è½½ç”¨æˆ·æ¨æ–‡æ—¶è‡ªåŠ¨è§¦å‘',
        'data_path': ['data', 'user', 'result', 'timeline_v2', 'timeline']
    },
    # å½“æµè§ˆå™¨è®¿é—®æ¨æ–‡è¯¦æƒ…é¡µæ—¶ï¼Œä¼šè‡ªåŠ¨å‘å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œæˆ‘ä»¬æ‹¦æˆªå®ƒçš„å“åº”
    'browser_tweet_detail_request': {
        'pattern': r'https://x\.com/i/api/graphql/.*/TweetDetail',
        'description': 'æ‹¦æˆªæµè§ˆå™¨è®¿é—®æ¨æ–‡è¯¦æƒ…é¡µæ—¶è‡ªåŠ¨å‘å‡ºçš„GraphQLè¯·æ±‚',
        'trigger_action': 'ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­è®¿é—®æ¨æ–‡è¯¦æƒ…é¡µæ—¶è‡ªåŠ¨è§¦å‘',
        'data_path': ['data', 'threaded_conversation_with_injections_v2']
    },
    # å½“æµè§ˆå™¨è®¿é—®å…³æ³¨è€…é¡µé¢æ—¶ï¼Œä¼šè‡ªåŠ¨å‘å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œæˆ‘ä»¬æ‹¦æˆªå®ƒçš„å“åº”
    'browser_followers_request': {
        'pattern': r'https://x\.com/i/api/graphql/.*/Followers',
        'description': 'æ‹¦æˆªæµè§ˆå™¨è®¿é—®å…³æ³¨è€…é¡µé¢æ—¶è‡ªåŠ¨å‘å‡ºçš„GraphQLè¯·æ±‚',
        'trigger_action': 'ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­è®¿é—®å…³æ³¨è€…é¡µé¢æ—¶è‡ªåŠ¨è§¦å‘',
        'data_path': ['data', 'user', 'result', 'timeline', 'timeline']
    }
}

```

#### æµè§ˆå™¨ç¯å¢ƒé…ç½®
```python
# config/browser_config.py

def get_browser_user_agents():
    """è·å–éšæœºUser-Agentåˆ—è¡¨"""
    return [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    ]

def get_browser_viewports():
    """è·å–éšæœºè§†å£å¤§å°åˆ—è¡¨"""
    return [
        {'width': 1920, 'height': 1080},
        {'width': 1366, 'height': 768},
        {'width': 1536, 'height': 864},
        {'width': 1440, 'height': 900},
        {'width': 1280, 'height': 720}
    ]

def get_browser_locales():
    """è·å–éšæœºè¯­è¨€ç¯å¢ƒåˆ—è¡¨"""
    return [
        'en-US',
        'en-GB',
        'en-CA',
        'en-AU'
    ]

def get_browser_timezones():
    """è·å–éšæœºæ—¶åŒºåˆ—è¡¨"""
    return [
        'America/New_York',
        'America/Los_Angeles',
        'America/Chicago',
        'Europe/London',
        'Europe/Berlin'
    ]

```

#### ç½‘ç»œæŠ“å–æ–¹æ³•è¯¦ç»†è¯´æ˜

**æˆ‘ä»¬çš„æ–¹æ³• vs ç›´æ¥APIè°ƒç”¨çš„å¯¹æ¯”ï¼š**

| æ–¹é¢ | æˆ‘ä»¬çš„ç½‘ç»œæŠ“å–æ–¹æ³• | ç›´æ¥APIè°ƒç”¨æ–¹æ³• |
|------|------------------|----------------|
| **è®¿é—®æ–¹å¼** | æµè§ˆå™¨è®¿é—®ç½‘é¡µ | ç›´æ¥HTTPè¯·æ±‚åˆ°API |
| **è®¤è¯æ–¹å¼** | æµè§ˆå™¨Cookieç™»å½• | APIå¯†é’¥è®¤è¯ |
| **è¯·æ±‚å‘èµ·** | æµè§ˆå™¨è‡ªåŠ¨å‘èµ· | ç¨‹åºç›´æ¥å‘èµ· |
| **æ•°æ®è·å–** | æ‹¦æˆªæµè§ˆå™¨è¯·æ±‚ | ç›´æ¥æ¥æ”¶APIå“åº” |
| **è´¹ç”¨** | å…è´¹ | éœ€è¦ä»˜è´¹ |
| **é™åˆ¶** | ç½‘é¡µè®¿é—®é™åˆ¶ | APIé€Ÿç‡é™åˆ¶ |

**å…·ä½“å·¥ä½œæµç¨‹ï¼š**

1. **æµè§ˆå™¨å¯åŠ¨**: Playwrightå¯åŠ¨Chromeæµè§ˆå™¨
2. **è®¾ç½®ç¯å¢ƒ**: é…ç½®User-Agentã€ä»£ç†ã€Cookieç­‰
3. **è®¿é—®é¡µé¢**: æµè§ˆå™¨è®¿é—® `https://x.com/search?q=keyword`
4. **é¡µé¢åŠ è½½**: Xç½‘é¡µæ­£å¸¸åŠ è½½ï¼Œæµè§ˆå™¨è‡ªåŠ¨å‘å‡ºåå°è¯·æ±‚
5. **è¯·æ±‚æ‹¦æˆª**: æˆ‘ä»¬ç›‘å¬å¹¶æ‹¦æˆªæµè§ˆå™¨å‘å‡ºçš„GraphQLè¯·æ±‚
6. **æ•°æ®æå–**: ä»æ‹¦æˆªçš„å“åº”ä¸­æå–JSONæ•°æ®
7. **æ•°æ®èåˆ**: ç»“åˆé¡µé¢DOMæ•°æ®å’Œæ‹¦æˆªçš„APIæ•°æ®

**å…³é”®æŠ€æœ¯ç‚¹ï¼š**
- æˆ‘ä»¬**ä¸ç›´æ¥è°ƒç”¨**X(Twitter)çš„APIç«¯ç‚¹
- æˆ‘ä»¬**æ‹¦æˆªæµè§ˆå™¨**è‡ªåŠ¨å‘å‡ºçš„è¯·æ±‚
- æˆ‘ä»¬**æ¨¡æ‹ŸçœŸå®ç”¨æˆ·**çš„æµè§ˆè¡Œä¸º
- æˆ‘ä»¬**éµå¾ªç½‘ç»œæŠ“å–**çš„åˆè§„åŸåˆ™

### 2. æ ¸å¿ƒé‡‡é›†å¼•æ“å®ç°

#### åŸºç¡€é‡‡é›†å™¨ç±»
```python
# core/base_scraper.py
import asyncio
import json
import re
from typing import Dict, List, Optional, Any
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from urllib.parse import urljoin

class TwitterBaseScraper:
    """åŸºäºæµè§ˆå™¨è‡ªåŠ¨åŒ–çš„Twitteré‡‡é›†å™¨åŸºç±»"""

    def __init__(self, cookie_manager, proxy_manager):
        self.cookie_mgr = cookie_manager
        self.proxy_mgr = proxy_manager
        self.browser = None
        self.context = None
        self.page = None
        self.intercepted_data = {}

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.setup_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.cleanup_browser()

    async def setup_browser(self, account_id: str = None):
        """è®¾ç½®æµè§ˆå™¨ç¯å¢ƒ"""
        try:
            # è·å–ä»£ç†é…ç½®
            proxy_config = None
            if account_id:
                proxy_url = await self.proxy_mgr.get_proxy(account_id)
                if proxy_url:
                    proxy_config = self._parse_proxy_url(proxy_url)

            # å¯åŠ¨æµè§ˆå™¨
            playwright = await async_playwright().start()

            # æµè§ˆå™¨å¯åŠ¨å‚æ•°
            launch_options = {
                'headless': True,
                'args': [
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            }

            self.browser = await playwright.chromium.launch(**launch_options)

            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'user_agent': self._get_random_user_agent(),
                'locale': 'en-US',
                'timezone_id': 'America/New_York'
            }

            if proxy_config:
                context_options['proxy'] = proxy_config

            self.context = await self.browser.new_context(**context_options)

            # è®¾ç½®Cookie
            if account_id:
                await self._set_account_cookies(account_id)

            # åˆ›å»ºé¡µé¢
            self.page = await self.context.new_page()

            # è®¾ç½®ç½‘ç»œæ‹¦æˆª
            await self._setup_network_interception()

            # æ³¨å…¥åæ£€æµ‹è„šæœ¬
            await self._inject_stealth_scripts()

        except Exception as e:
            logger.error(f"Failed to setup browser: {str(e)}")
            await self.cleanup_browser()
            raise

    async def cleanup_browser(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
        except Exception as e:
            logger.error(f"Error during browser cleanup: {str(e)}")

    async def _set_account_cookies(self, account_id: str):
        """è®¾ç½®è´¦å·Cookie"""
        try:
            cookie_data = await self.cookie_mgr.get_available_cookie(account_id)
            if cookie_data and 'cookies' in cookie_data:
                # è§£æCookieå­—ç¬¦ä¸²å¹¶è®¾ç½®åˆ°æµè§ˆå™¨
                cookies = self._parse_cookie_string(cookie_data['cookies'])
                await self.context.add_cookies(cookies)
        except Exception as e:
            logger.error(f"Failed to set cookies for account {account_id}: {str(e)}")

    def _parse_cookie_string(self, cookie_string: str) -> List[Dict]:
        """è§£æCookieå­—ç¬¦ä¸²"""
        cookies = []
        for cookie_pair in cookie_string.split(';'):
            if '=' in cookie_pair:
                name, value = cookie_pair.strip().split('=', 1)
                cookies.append({
                    'name': name,
                    'value': value,
                    'domain': '.x.com',
                    'path': '/'
                })
        return cookies

    def _parse_proxy_url(self, proxy_url: str) -> Dict:
        """è§£æä»£ç†URL"""
        # è§£æä»£ç†URLæ ¼å¼: http://username:password@host:port
        import re
        match = re.match(r'(\w+)://(?:([^:]+):([^@]+)@)?([^:]+):(\d+)', proxy_url)
        if match:
            protocol, username, password, host, port = match.groups()
            proxy_config = {
                'server': f"{protocol}://{host}:{port}"
            }
            if username and password:
                proxy_config['username'] = username
                proxy_config['password'] = password
            return proxy_config
        return None

    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        import random
        return random.choice(user_agents)

    async def _setup_network_interception(self):
        """è®¾ç½®ç½‘ç»œè¯·æ±‚æ‹¦æˆª"""
        async def handle_response(response):
            """å¤„ç†ç½‘ç»œå“åº”"""
            url = response.url

            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„APIè¯·æ±‚
            for pattern_name, pattern_config in NETWORK_INTERCEPT_PATTERNS.items():
                if re.search(pattern_config['pattern'], url):
                    try:
                        if response.status == 200:
                            response_data = await response.json()
                            self.intercepted_data[pattern_name] = {
                                'url': url,
                                'data': response_data,
                                'timestamp': asyncio.get_event_loop().time()
                            }
                            logger.debug(f"Intercepted {pattern_name} response from {url}")
                    except Exception as e:
                        logger.warning(f"Failed to parse response from {url}: {str(e)}")

        # ç›‘å¬å“åº”
        self.page.on('response', handle_response)

    async def _inject_stealth_scripts(self):
        """æ³¨å…¥åæ£€æµ‹è„šæœ¬"""
        stealth_script = """
        // ç§»é™¤webdriverå±æ€§
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
        });

        // ä¿®æ”¹plugins
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5],
        });

        // ä¿®æ”¹languages
        Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en'],
        });

        // ä¿®æ”¹permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );

        // éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        Object.defineProperty(navigator, 'webdriver', {
            get: () => false,
        });
        """

        await self.page.add_init_script(stealth_script)

    async def navigate_to_page(self, page_type: str, **kwargs) -> bool:
        """å¯¼èˆªåˆ°æŒ‡å®šé¡µé¢"""
        try:
            if page_type not in X_SCRAPING_TARGETS:
                raise ValueError(f"Unknown page type: {page_type}")

            page_config = X_SCRAPING_TARGETS[page_type]
            url = page_config['url_template'].format(**kwargs)

            logger.info(f"Navigating to {url}")

            # æ¸…ç©ºä¹‹å‰çš„æ‹¦æˆªæ•°æ®
            self.intercepted_data.clear()

            # å¯¼èˆªåˆ°é¡µé¢
            response = await self.page.goto(url, wait_until='networkidle', timeout=30000)

            if response.status >= 400:
                logger.error(f"Failed to load page {url}, status: {response.status}")
                return False

            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await self.page.wait_for_load_state('domcontentloaded')

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if await self._check_login_required():
                logger.warning("Login required, page may not load completely")

            return True

        except Exception as e:
            logger.error(f"Failed to navigate to {page_type}: {str(e)}")
            return False

    async def _check_login_required(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•"""
        login_indicators = [
            'text="Log in"',
            'text="Sign up"',
            '[data-testid="loginButton"]'
        ]

        for indicator in login_indicators:
            try:
                element = await self.page.wait_for_selector(indicator, timeout=2000)
                if element:
                    return True
            except:
                continue

        return False

    async def scroll_and_load_more(self, max_scrolls: int = 10) -> int:
        """æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹"""
        scroll_count = 0
        last_height = 0

        for i in range(max_scrolls):
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')

            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            await asyncio.sleep(2)

            # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦å˜åŒ–
            current_height = await self.page.evaluate('document.body.scrollHeight')

            if current_height == last_height:
                # é¡µé¢é«˜åº¦æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
                break

            last_height = current_height
            scroll_count += 1

            # æ£€æŸ¥æ˜¯å¦æœ‰"åŠ è½½æ›´å¤š"æŒ‰é’®
            try:
                load_more_button = await self.page.wait_for_selector(
                    '[role="button"]:has-text("Show more")',
                    timeout=1000
                )
                if load_more_button:
                    await load_more_button.click()
                    await asyncio.sleep(2)
            except:
                pass

        return scroll_count

    async def get_intercepted_data(self, pattern_name: str) -> Optional[Dict]:
        """è·å–æ‹¦æˆªåˆ°çš„æ•°æ®"""
        return self.intercepted_data.get(pattern_name)

    async def extract_page_data(self, page_type: str) -> List[Dict]:
        """ä»é¡µé¢æå–æ•°æ®"""
        if page_type not in X_SCRAPING_TARGETS:
            return []

        page_config = X_SCRAPING_TARGETS[page_type]
        selectors = page_config['selectors']

        extracted_data = []

        try:
            # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
            main_selector = list(selectors.values())[0]
            await self.page.wait_for_selector(main_selector, timeout=10000)

            # æ ¹æ®é¡µé¢ç±»å‹æå–æ•°æ®
            if page_type == 'search_page':
                extracted_data = await self._extract_search_results(selectors)
            elif page_type == 'profile_page':
                extracted_data = await self._extract_profile_data(selectors)
            elif page_type == 'tweet_detail_page':
                extracted_data = await self._extract_tweet_detail(selectors)
            elif page_type in ['followers_page', 'following_page']:
                extracted_data = await self._extract_user_list(selectors)

        except Exception as e:
            logger.error(f"Failed to extract data from {page_type}: {str(e)}")

        return extracted_data

    async def _extract_search_results(self, selectors: Dict) -> List[Dict]:
        """æå–æœç´¢ç»“æœ"""
        tweets = []

        tweet_elements = await self.page.query_selector_all(selectors['tweet_container'])

        for element in tweet_elements:
            try:
                tweet_data = await self._extract_tweet_from_element(element, selectors)
                if tweet_data:
                    tweets.append(tweet_data)
            except Exception as e:
                logger.warning(f"Failed to extract tweet: {str(e)}")
                continue

        return tweets

    async def _extract_tweet_from_element(self, element, selectors: Dict) -> Optional[Dict]:
        """ä»å…ƒç´ ä¸­æå–æ¨æ–‡æ•°æ®"""
        try:
            # æå–æ¨æ–‡æ–‡æœ¬
            text_element = await element.query_selector(selectors['tweet_text'])
            tweet_text = await text_element.inner_text() if text_element else ""

            # æå–ç”¨æˆ·ä¿¡æ¯
            user_element = await element.query_selector(selectors['user_name'])
            user_name = await user_element.inner_text() if user_element else ""

            # æå–æ—¶é—´
            time_element = await element.query_selector(selectors['tweet_time'])
            tweet_time = await time_element.get_attribute('datetime') if time_element else ""

            # æå–äº’åŠ¨æ•°æ®
            retweet_element = await element.query_selector(selectors['retweet_count'])
            retweet_count = await self._extract_count_from_element(retweet_element)

            like_element = await element.query_selector(selectors['like_count'])
            like_count = await self._extract_count_from_element(like_element)

            reply_element = await element.query_selector(selectors['reply_count'])
            reply_count = await self._extract_count_from_element(reply_element)

            return {
                'content': tweet_text,
                'user_name': user_name,
                'created_at': tweet_time,
                'retweet_count': retweet_count,
                'like_count': like_count,
                'reply_count': reply_count,
                'extracted_at': asyncio.get_event_loop().time()
            }

        except Exception as e:
            logger.warning(f"Failed to extract tweet data: {str(e)}")
            return None

    async def _extract_count_from_element(self, element) -> int:
        """ä»å…ƒç´ ä¸­æå–æ•°å­—"""
        if not element:
            return 0

        try:
            text = await element.inner_text()
            # å¤„ç†K, Mç­‰å•ä½
            text = text.strip().lower()
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            elif 'm' in text:
                return int(float(text.replace('m', '')) * 1000000)
            else:
                return int(text) if text.isdigit() else 0
        except:
            return 0
```

#### æœç´¢é‡‡é›†å™¨å®ç°
```python
# modules/search_scraper.py
class TwitterSearchScraper(TwitterBaseScraper):
    """åŸºäºæµè§ˆå™¨è‡ªåŠ¨åŒ–çš„Twitteræœç´¢é‡‡é›†å™¨"""

    async def search_tweets(self, keyword: str, count: int = 100, search_type: str = 'latest') -> List[Dict]:
        """æœç´¢æ¨æ–‡"""
        try:
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            success = await self.navigate_to_page(
                'search_page',
                query=keyword,
                search_type=X_SCRAPING_TARGETS['search_page']['search_types'].get(search_type, 'live')
            )

            if not success:
                logger.error(f"Failed to navigate to search page for keyword: {keyword}")
                return []

            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            await asyncio.sleep(3)

            collected_tweets = []
            scroll_attempts = 0
            max_scrolls = min(count // 20 + 1, 10)  # æ¯æ¬¡æ»šåŠ¨å¤§çº¦åŠ è½½20æ¡æ¨æ–‡

            while len(collected_tweets) < count and scroll_attempts < max_scrolls:
                # æå–å½“å‰é¡µé¢çš„æ¨æ–‡
                page_tweets = await self.extract_page_data('search_page')

                # å»é‡å¹¶æ·»åŠ æ–°æ¨æ–‡
                new_tweets = []
                existing_ids = {tweet.get('tweet_id') for tweet in collected_tweets}

                for tweet in page_tweets:
                    tweet_id = self._extract_tweet_id_from_url(tweet)
                    if tweet_id and tweet_id not in existing_ids:
                        tweet['tweet_id'] = tweet_id
                        new_tweets.append(tweet)
                        existing_ids.add(tweet_id)

                collected_tweets.extend(new_tweets)

                # å¦‚æœæ²¡æœ‰æ–°æ¨æ–‡ï¼Œå°è¯•æ»šåŠ¨åŠ è½½æ›´å¤š
                if not new_tweets:
                    scroll_count = await self.scroll_and_load_more(max_scrolls=1)
                    if scroll_count == 0:
                        break  # æ— æ³•ç»§ç»­æ»šåŠ¨ï¼Œå¯èƒ½å·²åˆ°åº•éƒ¨
                else:
                    # æœ‰æ–°æ¨æ–‡ï¼Œç»§ç»­æ»šåŠ¨
                    await self.scroll_and_load_more(max_scrolls=1)

                scroll_attempts += 1
                await asyncio.sleep(2)  # é¿å…è¿‡å¿«è¯·æ±‚

            # å°è¯•ä»æ‹¦æˆªçš„APIæ•°æ®ä¸­è·å–æ›´å®Œæ•´çš„ä¿¡æ¯
            await self._enrich_tweets_from_api_data(collected_tweets)

            logger.info(f"Collected {len(collected_tweets)} tweets for keyword: {keyword}")
            return collected_tweets[:count]

        except Exception as e:
            logger.error(f"Search failed for keyword '{keyword}': {str(e)}")
            return []

    def _extract_tweet_id_from_url(self, tweet_data: Dict) -> Optional[str]:
        """ä»æ¨æ–‡æ•°æ®ä¸­æå–æ¨æ–‡ID"""
        try:
            # å°è¯•ä»é¡µé¢å…ƒç´ ä¸­è·å–æ¨æ–‡é“¾æ¥
            if 'tweet_url' in tweet_data:
                url = tweet_data['tweet_url']
                # Xæ¨æ–‡URLæ ¼å¼: https://x.com/username/status/tweet_id
                import re
                match = re.search(r'/status/(\d+)', url)
                if match:
                    return match.group(1)

            # å¦‚æœæ²¡æœ‰URLï¼Œå°è¯•ä»å…¶ä»–å±æ€§ä¸­æå–
            # è¿™éœ€è¦æ ¹æ®å®é™…çš„é¡µé¢ç»“æ„æ¥è°ƒæ•´
            return None

        except Exception as e:
            logger.warning(f"Failed to extract tweet ID: {str(e)}")
            return None

    async def _enrich_tweets_from_api_data(self, tweets: List[Dict]):
        """ä»æ‹¦æˆªçš„APIæ•°æ®ä¸­ä¸°å¯Œæ¨æ–‡ä¿¡æ¯"""
        try:
            # è·å–æ‹¦æˆªåˆ°çš„æœç´¢APIæ•°æ®
            api_data = await self.get_intercepted_data('graphql_search')

            if not api_data:
                return

            # è§£æAPIæ•°æ®ä¸­çš„æ¨æ–‡ä¿¡æ¯
            api_tweets = self._parse_api_search_response(api_data['data'])

            # å°†APIæ•°æ®ä¸é¡µé¢æ•°æ®åŒ¹é…å¹¶åˆå¹¶
            for tweet in tweets:
                tweet_id = tweet.get('tweet_id')
                if tweet_id:
                    api_tweet = next((t for t in api_tweets if t.get('tweet_id') == tweet_id), None)
                    if api_tweet:
                        # åˆå¹¶APIæ•°æ®
                        tweet.update(api_tweet)

        except Exception as e:
            logger.warning(f"Failed to enrich tweets from API data: {str(e)}")

    def _parse_api_search_response(self, api_response: Dict) -> List[Dict]:
        """è§£ææœç´¢APIå“åº”"""
        tweets = []

        try:
            # å¯¼èˆªåˆ°æ¨æ–‡æ•°æ®
            timeline_data = api_response.get('search_by_raw_query', {}).get('search_timeline', {}).get('timeline', {})
            instructions = timeline_data.get('instructions', [])

            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])

                    for entry in entries:
                        if entry.get('entryId', '').startswith('tweet-'):
                            tweet_data = self._extract_tweet_from_api_entry(entry)
                            if tweet_data:
                                tweets.append(tweet_data)

        except Exception as e:
            logger.warning(f"Failed to parse API search response: {str(e)}")

        return tweets

    def _extract_tweet_from_api_entry(self, entry: Dict) -> Optional[Dict]:
        """ä»APIæ¡ç›®ä¸­æå–æ¨æ–‡æ•°æ®"""
        try:
            tweet_results = entry.get('content', {}).get('itemContent', {}).get('tweet_results', {}).get('result', {})

            if tweet_results.get('__typename') != 'Tweet':
                return None

            legacy = tweet_results.get('legacy', {})
            user_results = tweet_results.get('core', {}).get('user_results', {}).get('result', {})
            user_legacy = user_results.get('legacy', {})

            # æå–åª’ä½“ä¿¡æ¯
            media_list = []
            entities = legacy.get('entities', {})
            if 'media' in entities:
                for media in entities['media']:
                    media_info = {
                        'type': media.get('type'),
                        'url': media.get('media_url_https'),
                        'video_info': media.get('video_info', {})
                    }
                    media_list.append(media_info)

            tweet_data = {
                'tweet_id': legacy.get('id_str'),
                'user_id': user_legacy.get('id_str'),
                'username': user_legacy.get('screen_name'),
                'display_name': user_legacy.get('name'),
                'content': legacy.get('full_text', ''),
                'created_at': legacy.get('created_at'),
                'retweet_count': legacy.get('retweet_count', 0),
                'favorite_count': legacy.get('favorite_count', 0),
                'reply_count': legacy.get('reply_count', 0),
                'quote_count': legacy.get('quote_count', 0),
                'view_count': tweet_results.get('views', {}).get('count', 0),
                'media': media_list,
                'hashtags': [tag['text'] for tag in entities.get('hashtags', [])],
                'urls': [url['expanded_url'] for url in entities.get('urls', [])],
                'user_mentions': [mention['screen_name'] for mention in entities.get('user_mentions', [])],
                'is_retweet': legacy.get('retweeted', False),
                'is_quote': 'quoted_status_id_str' in legacy,
                'lang': legacy.get('lang'),
                'source': legacy.get('source', ''),
                'collected_at': datetime.utcnow().isoformat()
            }

            return tweet_data

        except Exception as e:
            logger.warning(f"Failed to extract tweet from API entry: {str(e)}")
            return None

    async def _extract_search_results(self, selectors: Dict) -> List[Dict]:
        """é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œæå–æœç´¢ç»“æœ"""
        tweets = []

        try:
            # ç­‰å¾…æ¨æ–‡å®¹å™¨åŠ è½½
            await self.page.wait_for_selector(selectors['tweet_container'], timeout=10000)

            # è·å–æ‰€æœ‰æ¨æ–‡å…ƒç´ 
            tweet_elements = await self.page.query_selector_all(selectors['tweet_container'])

            for element in tweet_elements:
                try:
                    tweet_data = await self._extract_tweet_from_element(element, selectors)
                    if tweet_data:
                        # å°è¯•è·å–æ¨æ–‡é“¾æ¥
                        link_element = await element.query_selector('a[href*="/status/"]')
                        if link_element:
                            tweet_url = await link_element.get_attribute('href')
                            if tweet_url:
                                tweet_data['tweet_url'] = f"https://x.com{tweet_url}"

                        tweets.append(tweet_data)

                except Exception as e:
                    logger.warning(f"Failed to extract tweet: {str(e)}")
                    continue

        except Exception as e:
            logger.error(f"Failed to extract search results: {str(e)}")

        return tweets
```

### 3. é«˜çº§åŠŸèƒ½å®ç°

#### è§†é¢‘æ•°æ®é‡‡é›†
```python
# modules/video_scraper.py
class TwitterVideoScraper(TwitterBaseScraper):
    async def get_video_stats(self, tweet_id: str) -> Dict:
        """è·å–è§†é¢‘æ’­æ”¾ç»Ÿè®¡æ•°æ®"""
        params = {
            'variables': json.dumps({
                'focalTweetId': tweet_id,
                'with_rux_injections': False,
                'includePromotedContent': True,
                'withCommunity': True,
                'withQuickPromoteEligibilityTweetFields': True,
                'withBirdwatchNotes': True,
                'withVoice': True,
                'withV2Timeline': True
            })
        }

        try:
            data = await self.make_request('TweetDetail', params)

            # è§£æè§†é¢‘ç»Ÿè®¡æ•°æ®
            instructions = data.get('data', {}).get('threaded_conversation_with_injections_v2', {}).get('instructions', [])

            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])

                    for entry in entries:
                        if entry.get('entryId') == f'tweet-{tweet_id}':
                            tweet_result = entry.get('content', {}).get('itemContent', {}).get('tweet_results', {}).get('result', {})

                            # æå–è§†é¢‘ä¿¡æ¯
                            legacy = tweet_result.get('legacy', {})
                            media_list = legacy.get('entities', {}).get('media', [])

                            video_stats = {
                                'tweet_id': tweet_id,
                                'view_count': tweet_result.get('views', {}).get('count', 0),
                                'videos': []
                            }

                            for media in media_list:
                                if media.get('type') == 'video':
                                    video_info = media.get('video_info', {})
                                    video_data = {
                                        'duration_millis': video_info.get('duration_millis', 0),
                                        'aspect_ratio': video_info.get('aspect_ratio', []),
                                        'variants': video_info.get('variants', []),
                                        'thumbnail': media.get('media_url_https'),
                                        'play_count': self.extract_play_count(media)
                                    }
                                    video_stats['videos'].append(video_data)

                            return video_stats

        except Exception as e:
            logger.error(f"Failed to get video stats for tweet {tweet_id}: {str(e)}")

        return {}

    def extract_play_count(self, media_data: Dict) -> int:
        """ä»åª’ä½“æ•°æ®ä¸­æå–æ’­æ”¾æ¬¡æ•°"""
        # Twitterçš„æ’­æ”¾æ¬¡æ•°é€šå¸¸åœ¨additional_media_infoä¸­
        additional_info = media_data.get('additional_media_info', {})
        return additional_info.get('view_count', 0)
```

#### è¯„è®ºé‡‡é›†å™¨
```python
# modules/reply_scraper.py
class TwitterReplyScraper(TwitterBaseScraper):
    async def get_tweet_replies(self, tweet_id: str, max_replies: int = 100) -> List[Dict]:
        """è·å–æ¨æ–‡çš„è¯„è®º"""
        replies = []
        cursor = None
        collected = 0

        while collected < max_replies:
            params = {
                'variables': json.dumps({
                    'focalTweetId': tweet_id,
                    'cursor': cursor,
                    'referrer': 'tweet',
                    'with_rux_injections': False,
                    'includePromotedContent': True,
                    'withCommunity': True,
                    'withQuickPromoteEligibilityTweetFields': True,
                    'withBirdwatchNotes': True,
                    'withVoice': True,
                    'withV2Timeline': True
                })
            }

            try:
                data = await self.make_request('TweetDetail', params)

                # è§£æè¯„è®ºæ•°æ®
                instructions = data.get('data', {}).get('threaded_conversation_with_injections_v2', {}).get('instructions', [])

                for instruction in instructions:
                    if instruction.get('type') == 'TimelineAddEntries':
                        entries = instruction.get('entries', [])

                        for entry in entries:
                            entry_id = entry.get('entryId', '')

                            if entry_id.startswith('tweet-') and entry_id != f'tweet-{tweet_id}':
                                # è¿™æ˜¯ä¸€ä¸ªå›å¤
                                reply_data = self.extract_reply_data(entry, tweet_id)
                                if reply_data:
                                    replies.append(reply_data)
                                    collected += 1

                            elif entry_id.startswith('cursor-bottom-'):
                                cursor = entry.get('content', {}).get('value')

                if not cursor or collected >= max_replies:
                    break

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"Failed to get replies for tweet {tweet_id}: {str(e)}")
                break

        return replies

    def extract_reply_data(self, entry: Dict, parent_tweet_id: str) -> Optional[Dict]:
        """æå–å›å¤æ•°æ®"""
        try:
            tweet_results = entry.get('content', {}).get('itemContent', {}).get('tweet_results', {}).get('result', {})

            if tweet_results.get('__typename') != 'Tweet':
                return None

            legacy = tweet_results.get('legacy', {})
            user_results = tweet_results.get('core', {}).get('user_results', {}).get('result', {})
            user_legacy = user_results.get('legacy', {})

            reply_data = {
                'reply_id': legacy.get('id_str'),
                'parent_tweet_id': parent_tweet_id,
                'user_id': user_legacy.get('id_str'),
                'username': user_legacy.get('screen_name'),
                'display_name': user_legacy.get('name'),
                'content': legacy.get('full_text', ''),
                'created_at': legacy.get('created_at'),
                'retweet_count': legacy.get('retweet_count', 0),
                'favorite_count': legacy.get('favorite_count', 0),
                'reply_count': legacy.get('reply_count', 0),
                'is_reply_to_reply': legacy.get('in_reply_to_status_id_str') != parent_tweet_id,
                'collected_at': datetime.utcnow().isoformat()
            }

            return reply_data

        except Exception as e:
            logger.error(f"Failed to extract reply data: {str(e)}")
            return None
```

### 4. Cookieæ± å’Œä»£ç†ç®¡ç†è¯¦ç»†å®ç°

#### Cookieç®¡ç†å™¨
```python
# core/cookie_manager.py
import redis
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Optional, List

class CookieManager:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.cookie_prefix = "twitter:cookie:"
        self.account_status_prefix = "twitter:account:status:"
        self.rate_limit_prefix = "twitter:ratelimit:"

    async def add_account(self, account_id: str, cookie_data: Dict) -> bool:
        """æ·»åŠ æ–°è´¦å·åˆ°Cookieæ± """
        try:
            cookie_key = f"{self.cookie_prefix}{account_id}"
            status_key = f"{self.account_status_prefix}{account_id}"

            # ä¿å­˜Cookieæ•°æ®
            cookie_info = {
                'account_id': account_id,
                'auth_token': cookie_data['auth_token'],
                'csrf_token': cookie_data['csrf_token'],
                'cookies': cookie_data['cookies'],
                'user_agent': cookie_data.get('user_agent', ''),
                'proxy_ip': cookie_data.get('proxy_ip', ''),
                'created_at': datetime.utcnow().isoformat(),
                'last_used': datetime.utcnow().isoformat(),
                'use_count': 0
            }

            self.redis.setex(cookie_key, 86400 * 7, json.dumps(cookie_info))  # 7å¤©è¿‡æœŸ

            # è®¾ç½®è´¦å·çŠ¶æ€
            account_status = {
                'status': 'active',
                'last_check': datetime.utcnow().isoformat(),
                'error_count': 0,
                'success_count': 0
            }

            self.redis.setex(status_key, 86400 * 7, json.dumps(account_status))

            return True

        except Exception as e:
            logger.error(f"Failed to add account {account_id}: {str(e)}")
            return False

    async def get_available_cookie(self, preferred_account: str = None) -> Optional[Dict]:
        """è·å–å¯ç”¨çš„Cookie"""
        try:
            # å¦‚æœæŒ‡å®šäº†è´¦å·ï¼Œä¼˜å…ˆä½¿ç”¨
            if preferred_account:
                cookie_data = await self._get_account_cookie(preferred_account)
                if cookie_data and await self._is_account_available(preferred_account):
                    await self._update_usage(preferred_account)
                    return cookie_data

            # è·å–æ‰€æœ‰å¯ç”¨è´¦å·
            available_accounts = await self._get_available_accounts()

            if not available_accounts:
                raise Exception("No available accounts in cookie pool")

            # é€‰æ‹©ä½¿ç”¨æ¬¡æ•°æœ€å°‘çš„è´¦å·
            selected_account = min(available_accounts, key=lambda x: x['use_count'])

            await self._update_usage(selected_account['account_id'])
            return selected_account

        except Exception as e:
            logger.error(f"Failed to get available cookie: {str(e)}")
            return None

    async def _get_account_cookie(self, account_id: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šè´¦å·çš„Cookie"""
        cookie_key = f"{self.cookie_prefix}{account_id}"
        cookie_data = self.redis.get(cookie_key)

        if cookie_data:
            return json.loads(cookie_data)
        return None

    async def _is_account_available(self, account_id: str) -> bool:
        """æ£€æŸ¥è´¦å·æ˜¯å¦å¯ç”¨"""
        status_key = f"{self.account_status_prefix}{account_id}"
        rate_limit_key = f"{self.rate_limit_prefix}{account_id}"

        # æ£€æŸ¥è´¦å·çŠ¶æ€
        status_data = self.redis.get(status_key)
        if status_data:
            status = json.loads(status_data)
            if status['status'] != 'active':
                return False

        # æ£€æŸ¥æ˜¯å¦è¢«é™æµ
        if self.redis.exists(rate_limit_key):
            return False

        return True

    async def _get_available_accounts(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¯ç”¨è´¦å·åˆ—è¡¨"""
        available = []

        # æ‰«ææ‰€æœ‰Cookieé”®
        for key in self.redis.scan_iter(match=f"{self.cookie_prefix}*"):
            account_id = key.decode().replace(self.cookie_prefix, '')

            if await self._is_account_available(account_id):
                cookie_data = await self._get_account_cookie(account_id)
                if cookie_data:
                    available.append(cookie_data)

        return available

    async def _update_usage(self, account_id: str):
        """æ›´æ–°è´¦å·ä½¿ç”¨ç»Ÿè®¡"""
        cookie_key = f"{self.cookie_prefix}{account_id}"
        cookie_data = self.redis.get(cookie_key)

        if cookie_data:
            data = json.loads(cookie_data)
            data['use_count'] += 1
            data['last_used'] = datetime.utcnow().isoformat()

            self.redis.setex(cookie_key, 86400 * 7, json.dumps(data))

    async def mark_rate_limited(self, account_id: str, duration: int = 900):
        """æ ‡è®°è´¦å·è¢«é™æµ"""
        rate_limit_key = f"{self.rate_limit_prefix}{account_id}"
        self.redis.setex(rate_limit_key, duration, "rate_limited")

        logger.warning(f"Account {account_id} marked as rate limited for {duration} seconds")

    async def mark_invalid(self, account_id: str, reason: str):
        """æ ‡è®°è´¦å·æ— æ•ˆ"""
        status_key = f"{self.account_status_prefix}{account_id}"
        status_data = self.redis.get(status_key)

        if status_data:
            status = json.loads(status_data)
            status['status'] = 'invalid'
            status['invalid_reason'] = reason
            status['invalid_at'] = datetime.utcnow().isoformat()

            self.redis.setex(status_key, 86400 * 7, json.dumps(status))

        logger.error(f"Account {account_id} marked as invalid: {reason}")

    async def get_pool_stats(self) -> Dict:
        """è·å–Cookieæ± ç»Ÿè®¡ä¿¡æ¯"""
        total_accounts = 0
        active_accounts = 0
        rate_limited_accounts = 0
        invalid_accounts = 0

        for key in self.redis.scan_iter(match=f"{self.account_status_prefix}*"):
            total_accounts += 1
            status_data = self.redis.get(key)

            if status_data:
                status = json.loads(status_data)
                if status['status'] == 'active':
                    account_id = key.decode().replace(self.account_status_prefix, '')
                    if await self._is_account_available(account_id):
                        active_accounts += 1
                    else:
                        rate_limited_accounts += 1
                elif status['status'] == 'invalid':
                    invalid_accounts += 1

        return {
            'total_accounts': total_accounts,
            'active_accounts': active_accounts,
            'rate_limited_accounts': rate_limited_accounts,
            'invalid_accounts': invalid_accounts,
            'availability_rate': active_accounts / total_accounts if total_accounts > 0 else 0
        }
```

#### ä»£ç†ç®¡ç†å™¨
```python
# core/proxy_manager.py
import redis
import json
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, Optional, List

class ProxyManager:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.proxy_prefix = "twitter:proxy:"
        self.proxy_status_prefix = "twitter:proxy:status:"
        self.account_proxy_binding = "twitter:account:proxy:"

    async def add_proxy(self, proxy_id: str, proxy_config: Dict) -> bool:
        """æ·»åŠ ä»£ç†åˆ°ä»£ç†æ± """
        try:
            proxy_key = f"{self.proxy_prefix}{proxy_id}"
            status_key = f"{self.proxy_status_prefix}{proxy_id}"

            proxy_info = {
                'proxy_id': proxy_id,
                'host': proxy_config['host'],
                'port': proxy_config['port'],
                'username': proxy_config.get('username', ''),
                'password': proxy_config.get('password', ''),
                'protocol': proxy_config.get('protocol', 'http'),
                'country': proxy_config.get('country', ''),
                'city': proxy_config.get('city', ''),
                'created_at': datetime.utcnow().isoformat(),
                'last_used': None,
                'use_count': 0
            }

            self.redis.setex(proxy_key, 86400 * 30, json.dumps(proxy_info))  # 30å¤©è¿‡æœŸ

            # åˆå§‹åŒ–ä»£ç†çŠ¶æ€
            proxy_status = {
                'status': 'unknown',  # unknown, active, failed, banned
                'last_check': None,
                'response_time': 0,
                'success_count': 0,
                'failure_count': 0,
                'banned_until': None
            }

            self.redis.setex(status_key, 86400 * 30, json.dumps(proxy_status))

            return True

        except Exception as e:
            logger.error(f"Failed to add proxy {proxy_id}: {str(e)}")
            return False

    async def get_proxy(self, account_id: str = None, country: str = None) -> Optional[str]:
        """è·å–å¯ç”¨ä»£ç†"""
        try:
            # å¦‚æœæŒ‡å®šäº†è´¦å·ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç»‘å®šçš„ä»£ç†
            if account_id:
                bound_proxy = await self._get_bound_proxy(account_id)
                if bound_proxy and await self._is_proxy_available(bound_proxy):
                    await self._update_proxy_usage(bound_proxy)
                    return await self._format_proxy_url(bound_proxy)

            # è·å–å¯ç”¨ä»£ç†åˆ—è¡¨
            available_proxies = await self._get_available_proxies(country)

            if not available_proxies:
                raise Exception("No available proxies")

            # é€‰æ‹©å“åº”æ—¶é—´æœ€å¿«çš„ä»£ç†
            selected_proxy = min(available_proxies, key=lambda x: x.get('response_time', 999))

            # å¦‚æœæŒ‡å®šäº†è´¦å·ï¼Œå»ºç«‹ç»‘å®šå…³ç³»
            if account_id:
                await self._bind_account_proxy(account_id, selected_proxy['proxy_id'])

            await self._update_proxy_usage(selected_proxy['proxy_id'])
            return await self._format_proxy_url(selected_proxy['proxy_id'])

        except Exception as e:
            logger.error(f"Failed to get proxy: {str(e)}")
            return None

    async def _get_bound_proxy(self, account_id: str) -> Optional[str]:
        """è·å–è´¦å·ç»‘å®šçš„ä»£ç†"""
        binding_key = f"{self.account_proxy_binding}{account_id}"
        proxy_id = self.redis.get(binding_key)

        if proxy_id:
            return proxy_id.decode()
        return None

    async def _bind_account_proxy(self, account_id: str, proxy_id: str):
        """ç»‘å®šè´¦å·å’Œä»£ç†"""
        binding_key = f"{self.account_proxy_binding}{account_id}"
        self.redis.setex(binding_key, 86400 * 7, proxy_id)  # 7å¤©ç»‘å®šæœŸ

    async def _is_proxy_available(self, proxy_id: str) -> bool:
        """æ£€æŸ¥ä»£ç†æ˜¯å¦å¯ç”¨"""
        status_key = f"{self.proxy_status_prefix}{proxy_id}"
        status_data = self.redis.get(status_key)

        if status_data:
            status = json.loads(status_data)

            # æ£€æŸ¥æ˜¯å¦è¢«å°ç¦
            if status.get('banned_until'):
                banned_until = datetime.fromisoformat(status['banned_until'])
                if datetime.utcnow() < banned_until:
                    return False

            # æ£€æŸ¥çŠ¶æ€
            if status['status'] in ['active', 'unknown']:
                return True

        return False

    async def _get_available_proxies(self, country: str = None) -> List[Dict]:
        """è·å–å¯ç”¨ä»£ç†åˆ—è¡¨"""
        available = []

        for key in self.redis.scan_iter(match=f"{self.proxy_prefix}*"):
            proxy_id = key.decode().replace(self.proxy_prefix, '')

            if await self._is_proxy_available(proxy_id):
                proxy_data = self.redis.get(key)
                if proxy_data:
                    proxy_info = json.loads(proxy_data)

                    # å¦‚æœæŒ‡å®šäº†å›½å®¶ï¼Œè¿›è¡Œè¿‡æ»¤
                    if country and proxy_info.get('country', '').lower() != country.lower():
                        continue

                    # è·å–çŠ¶æ€ä¿¡æ¯
                    status_key = f"{self.proxy_status_prefix}{proxy_id}"
                    status_data = self.redis.get(status_key)
                    if status_data:
                        status = json.loads(status_data)
                        proxy_info.update(status)

                    available.append(proxy_info)

        return available

    async def _format_proxy_url(self, proxy_id: str) -> str:
        """æ ¼å¼åŒ–ä»£ç†URL"""
        proxy_key = f"{self.proxy_prefix}{proxy_id}"
        proxy_data = self.redis.get(proxy_key)

        if proxy_data:
            proxy_info = json.loads(proxy_data)

            if proxy_info.get('username') and proxy_info.get('password'):
                return f"{proxy_info['protocol']}://{proxy_info['username']}:{proxy_info['password']}@{proxy_info['host']}:{proxy_info['port']}"
            else:
                return f"{proxy_info['protocol']}://{proxy_info['host']}:{proxy_info['port']}"

        return None

    async def _update_proxy_usage(self, proxy_id: str):
        """æ›´æ–°ä»£ç†ä½¿ç”¨ç»Ÿè®¡"""
        proxy_key = f"{self.proxy_prefix}{proxy_id}"
        proxy_data = self.redis.get(proxy_key)

        if proxy_data:
            data = json.loads(proxy_data)
            data['use_count'] += 1
            data['last_used'] = datetime.utcnow().isoformat()

            self.redis.setex(proxy_key, 86400 * 30, json.dumps(data))

    async def check_proxy_health(self, proxy_id: str) -> bool:
        """æ£€æŸ¥ä»£ç†å¥åº·çŠ¶æ€"""
        try:
            proxy_url = await self._format_proxy_url(proxy_id)
            if not proxy_url:
                return False

            start_time = datetime.utcnow()

            async with aiohttp.ClientSession() as session:
                async with session.get(
                    'https://httpbin.org/ip',
                    proxy=proxy_url,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        response_time = (datetime.utcnow() - start_time).total_seconds()
                        await self._update_proxy_status(proxy_id, 'active', response_time, True)
                        return True
                    else:
                        await self._update_proxy_status(proxy_id, 'failed', 0, False)
                        return False

        except Exception as e:
            logger.error(f"Proxy health check failed for {proxy_id}: {str(e)}")
            await self._update_proxy_status(proxy_id, 'failed', 0, False)
            return False

    async def _update_proxy_status(self, proxy_id: str, status: str, response_time: float, success: bool):
        """æ›´æ–°ä»£ç†çŠ¶æ€"""
        status_key = f"{self.proxy_status_prefix}{proxy_id}"
        status_data = self.redis.get(status_key)

        if status_data:
            current_status = json.loads(status_data)
        else:
            current_status = {
                'status': 'unknown',
                'last_check': None,
                'response_time': 0,
                'success_count': 0,
                'failure_count': 0,
                'banned_until': None
            }

        current_status['status'] = status
        current_status['last_check'] = datetime.utcnow().isoformat()
        current_status['response_time'] = response_time

        if success:
            current_status['success_count'] += 1
        else:
            current_status['failure_count'] += 1

        self.redis.setex(status_key, 86400 * 30, json.dumps(current_status))

    async def mark_proxy_banned(self, proxy_id: str, duration_hours: int = 24):
        """æ ‡è®°ä»£ç†è¢«å°ç¦"""
        status_key = f"{self.proxy_status_prefix}{proxy_id}"
        status_data = self.redis.get(status_key)

        if status_data:
            status = json.loads(status_data)
        else:
            status = {}

        status['status'] = 'banned'
        status['banned_until'] = (datetime.utcnow() + timedelta(hours=duration_hours)).isoformat()

        self.redis.setex(status_key, 86400 * 30, json.dumps(status))

        logger.warning(f"Proxy {proxy_id} marked as banned for {duration_hours} hours")

    async def get_proxy_stats(self) -> Dict:
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        total_proxies = 0
        active_proxies = 0
        failed_proxies = 0
        banned_proxies = 0

        for key in self.redis.scan_iter(match=f"{self.proxy_status_prefix}*"):
            total_proxies += 1
            status_data = self.redis.get(key)

            if status_data:
                status = json.loads(status_data)

                if status['status'] == 'active':
                    active_proxies += 1
                elif status['status'] == 'failed':
                    failed_proxies += 1
                elif status['status'] == 'banned':
                    banned_proxies += 1

        return {
            'total_proxies': total_proxies,
            'active_proxies': active_proxies,
            'failed_proxies': failed_proxies,
            'banned_proxies': banned_proxies,
            'availability_rate': active_proxies / total_proxies if total_proxies > 0 else 0
        }
```

### 5. ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿå®ç°

#### Celeryä»»åŠ¡å®šä¹‰
```python
# core/tasks.py
from celery import Celery
from celery.result import AsyncResult
import json
import asyncio
from datetime import datetime, timedelta

# åˆå§‹åŒ–Celeryåº”ç”¨
app = Celery('weget')
app.config_from_object('config.celery_config')

@app.task(bind=True, max_retries=3)
def scrape_search_task(self, keyword, count=100, search_type='Latest'):
    """æœç´¢é‡‡é›†ä»»åŠ¡"""
    try:
        # åˆ›å»ºå¼‚æ­¥äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # åˆå§‹åŒ–ç®¡ç†å™¨
        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        # æ‰§è¡Œæœç´¢é‡‡é›†
        async def run_search():
            async with TwitterSearchScraper(cookie_mgr, proxy_mgr) as scraper:
                tweets = await scraper.search_tweets(keyword, count, search_type)

                # ä¿å­˜æ•°æ®
                data_mgr = DataManager()
                for tweet in tweets:
                    await data_mgr.save_tweet(tweet)

                return len(tweets)

        result = loop.run_until_complete(run_search())
        loop.close()

        return {
            'status': 'success',
            'keyword': keyword,
            'collected_count': result,
            'completed_at': datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Search task failed for keyword '{keyword}': {str(e)}")

        # é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (self.request.retries + 1))

        return {
            'status': 'failed',
            'keyword': keyword,
            'error': str(e),
            'failed_at': datetime.utcnow().isoformat()
        }

@app.task(bind=True, max_retries=3)
def scrape_profile_task(self, username, include_tweets=True, tweet_count=200):
    """ç”¨æˆ·ä¸»é¡µé‡‡é›†ä»»åŠ¡"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        async def run_profile_scrape():
            async with TwitterProfileScraper(cookie_mgr, proxy_mgr) as scraper:
                # è·å–ç”¨æˆ·åŸºç¡€ä¿¡æ¯
                user_info = await scraper.get_user_info(username)
                if not user_info:
                    raise Exception(f"User {username} not found")

                data_mgr = DataManager()
                await data_mgr.save_user(user_info)

                collected_tweets = 0
                if include_tweets:
                    # è·å–ç”¨æˆ·æ¨æ–‡
                    tweets = await scraper.get_user_tweets(user_info['user_id'], tweet_count)
                    for tweet in tweets:
                        await data_mgr.save_tweet(tweet)
                    collected_tweets = len(tweets)

                return {
                    'user_info': user_info,
                    'tweet_count': collected_tweets
                }

        result = loop.run_until_complete(run_profile_scrape())
        loop.close()

        return {
            'status': 'success',
            'username': username,
            'user_id': result['user_info']['user_id'],
            'collected_tweets': result['tweet_count'],
            'completed_at': datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Profile task failed for user '{username}': {str(e)}")

        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (self.request.retries + 1))

        return {
            'status': 'failed',
            'username': username,
            'error': str(e),
            'failed_at': datetime.utcnow().isoformat()
        }

@app.task(bind=True, max_retries=3)
def scrape_tweet_task(self, tweet_id, include_replies=True, max_replies=100):
    """å•æ¡æ¨æ–‡é‡‡é›†ä»»åŠ¡"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        async def run_tweet_scrape():
            # è·å–æ¨æ–‡è¯¦æƒ…
            async with TwitterTweetScraper(cookie_mgr, proxy_mgr) as tweet_scraper:
                tweet_detail = await tweet_scraper.get_tweet_detail(tweet_id)
                if not tweet_detail:
                    raise Exception(f"Tweet {tweet_id} not found")

                data_mgr = DataManager()
                await data_mgr.save_tweet(tweet_detail)

                collected_replies = 0
                if include_replies:
                    # è·å–æ¨æ–‡å›å¤
                    async with TwitterReplyScraper(cookie_mgr, proxy_mgr) as reply_scraper:
                        replies = await reply_scraper.get_tweet_replies(tweet_id, max_replies)
                        for reply in replies:
                            await data_mgr.save_reply(reply)
                        collected_replies = len(replies)

                # è·å–è§†é¢‘ç»Ÿè®¡æ•°æ®
                video_stats = {}
                if tweet_detail.get('media'):
                    for media in tweet_detail['media']:
                        if media.get('type') == 'video':
                            async with TwitterVideoScraper(cookie_mgr, proxy_mgr) as video_scraper:
                                video_stats = await video_scraper.get_video_stats(tweet_id)
                                break

                return {
                    'tweet_detail': tweet_detail,
                    'reply_count': collected_replies,
                    'video_stats': video_stats
                }

        result = loop.run_until_complete(run_tweet_scrape())
        loop.close()

        return {
            'status': 'success',
            'tweet_id': tweet_id,
            'collected_replies': result['reply_count'],
            'has_video_stats': bool(result['video_stats']),
            'completed_at': datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Tweet task failed for tweet '{tweet_id}': {str(e)}")

        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (self.request.retries + 1))

        return {
            'status': 'failed',
            'tweet_id': tweet_id,
            'error': str(e),
            'failed_at': datetime.utcnow().isoformat()
        }

@app.task
def health_check_task():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        # æ£€æŸ¥Cookieæ± çŠ¶æ€
        cookie_stats = asyncio.run(cookie_mgr.get_pool_stats())

        # æ£€æŸ¥ä»£ç†æ± çŠ¶æ€
        proxy_stats = asyncio.run(proxy_mgr.get_proxy_stats())

        # æ£€æŸ¥Redisè¿æ¥
        redis_status = redis_client.ping()

        # æ£€æŸ¥MongoDBè¿æ¥
        mongo_status = True
        try:
            mongo_client.admin.command('ping')
        except:
            mongo_status = False

        health_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'cookie_pool': cookie_stats,
            'proxy_pool': proxy_stats,
            'redis_status': redis_status,
            'mongodb_status': mongo_status,
            'overall_health': all([
                cookie_stats['availability_rate'] > 0.1,  # è‡³å°‘10%è´¦å·å¯ç”¨
                proxy_stats['availability_rate'] > 0.1,   # è‡³å°‘10%ä»£ç†å¯ç”¨
                redis_status,
                mongo_status
            ])
        }

        # ä¿å­˜å¥åº·æŠ¥å‘Š
        redis_client.setex('system:health', 300, json.dumps(health_report))  # 5åˆ†é’Ÿè¿‡æœŸ

        return health_report

    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'status': 'error',
            'error': str(e)
        }
```

#### ä»»åŠ¡è°ƒåº¦å™¨
```python
# core/scheduler.py
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from celery import group, chain, chord
from .tasks import app, scrape_search_task, scrape_profile_task, scrape_tweet_task

class TaskScheduler:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.celery_app = app

    def submit_search_jobs(self, keywords: List[str], count: int = 100, priority: str = 'normal') -> List[str]:
        """æ‰¹é‡æäº¤æœç´¢ä»»åŠ¡"""
        task_ids = []

        for keyword in keywords:
            # åˆ›å»ºä»»åŠ¡
            task = scrape_search_task.apply_async(
                args=[keyword, count],
                priority=self._get_priority_value(priority),
                expires=datetime.utcnow() + timedelta(hours=6)  # 6å°æ—¶è¿‡æœŸ
            )

            task_ids.append(task.id)

            # è®°å½•ä»»åŠ¡ä¿¡æ¯
            task_info = {
                'task_id': task.id,
                'task_type': 'search',
                'keyword': keyword,
                'count': count,
                'priority': priority,
                'created_at': datetime.utcnow().isoformat(),
                'status': 'pending'
            }

            self.redis.setex(f"task:info:{task.id}", 86400, json.dumps(task_info))

        logger.info(f"Submitted {len(task_ids)} search tasks")
        return task_ids

    def submit_profile_jobs(self, usernames: List[str], include_tweets: bool = True, priority: str = 'normal') -> List[str]:
        """æ‰¹é‡æäº¤ç”¨æˆ·ä¸»é¡µé‡‡é›†ä»»åŠ¡"""
        task_ids = []

        for username in usernames:
            task = scrape_profile_task.apply_async(
                args=[username, include_tweets],
                priority=self._get_priority_value(priority),
                expires=datetime.utcnow() + timedelta(hours=6)
            )

            task_ids.append(task.id)

            task_info = {
                'task_id': task.id,
                'task_type': 'profile',
                'username': username,
                'include_tweets': include_tweets,
                'priority': priority,
                'created_at': datetime.utcnow().isoformat(),
                'status': 'pending'
            }

            self.redis.setex(f"task:info:{task.id}", 86400, json.dumps(task_info))

        logger.info(f"Submitted {len(task_ids)} profile tasks")
        return task_ids

    def submit_tweet_jobs(self, tweet_ids: List[str], include_replies: bool = True, priority: str = 'normal') -> List[str]:
        """æ‰¹é‡æäº¤æ¨æ–‡é‡‡é›†ä»»åŠ¡"""
        task_ids = []

        for tweet_id in tweet_ids:
            task = scrape_tweet_task.apply_async(
                args=[tweet_id, include_replies],
                priority=self._get_priority_value(priority),
                expires=datetime.utcnow() + timedelta(hours=6)
            )

            task_ids.append(task.id)

            task_info = {
                'task_id': task.id,
                'task_type': 'tweet',
                'tweet_id': tweet_id,
                'include_replies': include_replies,
                'priority': priority,
                'created_at': datetime.utcnow().isoformat(),
                'status': 'pending'
            }

            self.redis.setex(f"task:info:{task.id}", 86400, json.dumps(task_info))

        logger.info(f"Submitted {len(task_ids)} tweet tasks")
        return task_ids

    def get_task_status(self, task_id: str) -> Dict:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # ä»Redisè·å–ä»»åŠ¡ä¿¡æ¯
        task_info_key = f"task:info:{task_id}"
        task_info = self.redis.get(task_info_key)

        if task_info:
            info = json.loads(task_info)
        else:
            info = {'task_id': task_id}

        # ä»Celeryè·å–ä»»åŠ¡çŠ¶æ€
        result = AsyncResult(task_id, app=self.celery_app)

        info.update({
            'celery_status': result.status,
            'result': result.result if result.ready() else None,
            'traceback': result.traceback if result.failed() else None
        })

        return info

    def get_tasks_by_type(self, task_type: str, limit: int = 100) -> List[Dict]:
        """æŒ‰ç±»å‹è·å–ä»»åŠ¡åˆ—è¡¨"""
        tasks = []

        for key in self.redis.scan_iter(match="task:info:*", count=limit):
            task_info = self.redis.get(key)
            if task_info:
                info = json.loads(task_info)
                if info.get('task_type') == task_type:
                    task_id = info['task_id']

                    # è·å–CeleryçŠ¶æ€
                    result = AsyncResult(task_id, app=self.celery_app)
                    info['celery_status'] = result.status

                    tasks.append(info)

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        tasks.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        return tasks[:limit]

    def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        try:
            self.celery_app.control.revoke(task_id, terminate=True)

            # æ›´æ–°ä»»åŠ¡ä¿¡æ¯
            task_info_key = f"task:info:{task_id}"
            task_info = self.redis.get(task_info_key)

            if task_info:
                info = json.loads(task_info)
                info['status'] = 'cancelled'
                info['cancelled_at'] = datetime.utcnow().isoformat()
                self.redis.setex(task_info_key, 86400, json.dumps(info))

            return True

        except Exception as e:
            logger.error(f"Failed to cancel task {task_id}: {str(e)}")
            return False

    def get_queue_stats(self) -> Dict:
        """è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        inspect = self.celery_app.control.inspect()

        # è·å–æ´»è·ƒä»»åŠ¡
        active_tasks = inspect.active()

        # è·å–ç­‰å¾…ä»»åŠ¡
        reserved_tasks = inspect.reserved()

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'active_tasks': sum(len(tasks) for tasks in (active_tasks or {}).values()),
            'reserved_tasks': sum(len(tasks) for tasks in (reserved_tasks or {}).values()),
            'workers': list((active_tasks or {}).keys()),
            'timestamp': datetime.utcnow().isoformat()
        }

        return stats

    def _get_priority_value(self, priority: str) -> int:
        """è·å–ä¼˜å…ˆçº§æ•°å€¼"""
        priority_map = {
            'low': 3,
            'normal': 6,
            'high': 9
        }
        return priority_map.get(priority, 6)

    def schedule_periodic_health_check(self):
        """è°ƒåº¦å®šæœŸå¥åº·æ£€æŸ¥"""
        from celery.schedules import crontab

        self.celery_app.conf.beat_schedule = {
            'health-check': {
                'task': 'core.tasks.health_check_task',
                'schedule': crontab(minute='*/5'),  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
            },
        }
```

### 6. ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

#### ç³»ç»Ÿç›‘æ§å™¨
```python
# core/monitor.py
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart

class SystemMonitor:
    def __init__(self, redis_client, config):
        self.redis = redis_client
        self.config = config
        self.alert_thresholds = {
            'cookie_availability_min': 0.1,  # æœ€ä½10%è´¦å·å¯ç”¨
            'proxy_availability_min': 0.1,   # æœ€ä½10%ä»£ç†å¯ç”¨
            'task_failure_rate_max': 0.3,    # æœ€é«˜30%ä»»åŠ¡å¤±è´¥ç‡
            'response_time_max': 30,          # æœ€å¤§å“åº”æ—¶é—´30ç§’
        }

    def check_system_health(self) -> Dict:
        """æ£€æŸ¥ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        health_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'components': {},
            'alerts': [],
            'overall_status': 'healthy'
        }

        # æ£€æŸ¥Cookieæ± 
        cookie_health = self._check_cookie_pool()
        health_report['components']['cookie_pool'] = cookie_health

        # æ£€æŸ¥ä»£ç†æ± 
        proxy_health = self._check_proxy_pool()
        health_report['components']['proxy_pool'] = proxy_health

        # æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œæƒ…å†µ
        task_health = self._check_task_performance()
        health_report['components']['task_performance'] = task_health

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_health = self._check_database_connections()
        health_report['components']['databases'] = db_health

        # ç”Ÿæˆå‘Šè­¦
        alerts = self._generate_alerts(health_report['components'])
        health_report['alerts'] = alerts

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if alerts:
            critical_alerts = [a for a in alerts if a['level'] == 'critical']
            if critical_alerts:
                health_report['overall_status'] = 'critical'
            else:
                health_report['overall_status'] = 'warning'

        # ä¿å­˜å¥åº·æŠ¥å‘Š
        self.redis.setex('monitor:health_report', 300, json.dumps(health_report))

        # å‘é€å‘Šè­¦
        if alerts:
            self._send_alerts(alerts)

        return health_report

    def _check_cookie_pool(self) -> Dict:
        """æ£€æŸ¥Cookieæ± çŠ¶æ€"""
        try:
            cookie_mgr = CookieManager(self.redis)
            stats = asyncio.run(cookie_mgr.get_pool_stats())

            status = 'healthy'
            if stats['availability_rate'] < self.alert_thresholds['cookie_availability_min']:
                status = 'critical'
            elif stats['availability_rate'] < 0.3:
                status = 'warning'

            return {
                'status': status,
                'stats': stats,
                'last_check': datetime.utcnow().isoformat()
            }

        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'last_check': datetime.utcnow().isoformat()
            }

    def _check_proxy_pool(self) -> Dict:
        """æ£€æŸ¥ä»£ç†æ± çŠ¶æ€"""
        try:
            proxy_mgr = ProxyManager(self.redis)
            stats = asyncio.run(proxy_mgr.get_proxy_stats())

            status = 'healthy'
            if stats['availability_rate'] < self.alert_thresholds['proxy_availability_min']:
                status = 'critical'
            elif stats['availability_rate'] < 0.3:
                status = 'warning'

            return {
                'status': status,
                'stats': stats,
                'last_check': datetime.utcnow().isoformat()
            }

        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'last_check': datetime.utcnow().isoformat()
            }

    def _check_task_performance(self) -> Dict:
        """æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œæ€§èƒ½"""
        try:
            # è·å–æœ€è¿‘1å°æ—¶çš„ä»»åŠ¡ç»Ÿè®¡
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=1)

            total_tasks = 0
            failed_tasks = 0
            avg_response_time = 0

            # æ‰«æä»»åŠ¡ä¿¡æ¯
            for key in self.redis.scan_iter(match="task:info:*"):
                task_info = self.redis.get(key)
                if task_info:
                    info = json.loads(task_info)
                    created_at = datetime.fromisoformat(info.get('created_at', ''))

                    if start_time <= created_at <= end_time:
                        total_tasks += 1

                        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                        result = AsyncResult(info['task_id'], app=app)
                        if result.failed():
                            failed_tasks += 1

            failure_rate = failed_tasks / total_tasks if total_tasks > 0 else 0

            status = 'healthy'
            if failure_rate > self.alert_thresholds['task_failure_rate_max']:
                status = 'critical'
            elif failure_rate > 0.1:
                status = 'warning'

            return {
                'status': status,
                'total_tasks_1h': total_tasks,
                'failed_tasks_1h': failed_tasks,
                'failure_rate': failure_rate,
                'avg_response_time': avg_response_time,
                'last_check': datetime.utcnow().isoformat()
            }

        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'last_check': datetime.utcnow().isoformat()
            }

    def _check_database_connections(self) -> Dict:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€"""
        db_status = {}

        # æ£€æŸ¥Redis
        try:
            self.redis.ping()
            db_status['redis'] = {'status': 'healthy', 'response_time': 0}
        except Exception as e:
            db_status['redis'] = {'status': 'error', 'error': str(e)}

        # æ£€æŸ¥MongoDB
        try:
            start_time = time.time()
            mongo_client.admin.command('ping')
            response_time = time.time() - start_time
            db_status['mongodb'] = {'status': 'healthy', 'response_time': response_time}
        except Exception as e:
            db_status['mongodb'] = {'status': 'error', 'error': str(e)}

        return db_status

    def _generate_alerts(self, components: Dict) -> List[Dict]:
        """ç”Ÿæˆå‘Šè­¦ä¿¡æ¯"""
        alerts = []

        for component_name, component_data in components.items():
            if component_data['status'] == 'critical':
                alerts.append({
                    'level': 'critical',
                    'component': component_name,
                    'message': f"{component_name} is in critical state",
                    'details': component_data,
                    'timestamp': datetime.utcnow().isoformat()
                })
            elif component_data['status'] == 'warning':
                alerts.append({
                    'level': 'warning',
                    'component': component_name,
                    'message': f"{component_name} needs attention",
                    'details': component_data,
                    'timestamp': datetime.utcnow().isoformat()
                })
            elif component_data['status'] == 'error':
                alerts.append({
                    'level': 'error',
                    'component': component_name,
                    'message': f"{component_name} encountered an error",
                    'details': component_data,
                    'timestamp': datetime.utcnow().isoformat()
                })

        return alerts

    def _send_alerts(self, alerts: List[Dict]):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        if not self.config.get('alerts', {}).get('email_enabled', False):
            return

        try:
            # æ„å»ºé‚®ä»¶å†…å®¹
            subject = f"WeGet System Alert - {len(alerts)} issues detected"

            body = "WeGet Twitteré‡‡é›†ç³»ç»Ÿå‘Šè­¦æŠ¥å‘Š\n\n"
            body += f"æ£€æµ‹æ—¶é—´: {datetime.utcnow().isoformat()}\n"
            body += f"å‘Šè­¦æ•°é‡: {len(alerts)}\n\n"

            for alert in alerts:
                body += f"çº§åˆ«: {alert['level'].upper()}\n"
                body += f"ç»„ä»¶: {alert['component']}\n"
                body += f"æ¶ˆæ¯: {alert['message']}\n"
                body += f"æ—¶é—´: {alert['timestamp']}\n"
                body += "-" * 50 + "\n"

            # å‘é€é‚®ä»¶
            self._send_email(subject, body)

        except Exception as e:
            logger.error(f"Failed to send alert email: {str(e)}")

    def _send_email(self, subject: str, body: str):
        """å‘é€é‚®ä»¶"""
        email_config = self.config['alerts']['email']

        msg = MimeMultipart()
        msg['From'] = email_config['from']
        msg['To'] = ', '.join(email_config['to'])
        msg['Subject'] = subject

        msg.attach(MimeText(body, 'plain', 'utf-8'))

        server = smtplib.SMTP(email_config['smtp_host'], email_config['smtp_port'])
        if email_config.get('use_tls', True):
            server.starttls()
        if email_config.get('username') and email_config.get('password'):
            server.login(email_config['username'], email_config['password'])

        server.send_message(msg)
        server.quit()

    def get_historical_metrics(self, hours: int = 24) -> Dict:
        """è·å–å†å²ç›‘æ§æŒ‡æ ‡"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        metrics = {
            'timeframe': f"{hours} hours",
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'data_points': []
        }

        # è·å–å†å²å¥åº·æŠ¥å‘Š
        for i in range(hours * 12):  # æ¯5åˆ†é’Ÿä¸€ä¸ªæ•°æ®ç‚¹
            check_time = end_time - timedelta(minutes=i * 5)

            # è¿™é‡Œå¯ä»¥ä»æ—¶åºæ•°æ®åº“æˆ–æ—¥å¿—ä¸­è·å–å†å²æ•°æ®
            # ç®€åŒ–å®ç°ï¼Œä»…è¿”å›å½“å‰çŠ¶æ€

        return metrics

#### PrometheusæŒ‡æ ‡å¯¼å‡ºå™¨
```python
# core/prometheus_exporter.py
import asyncio
import psutil
from datetime import datetime
from typing import Dict, List, Optional
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST
from prometheus_client.exposition import MetricsHandler
from fastapi import FastAPI, Response
from celery import Celery
import redis

class PrometheusMetrics:
    """PrometheusæŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.registry = CollectorRegistry()

        # è´¦å·ç›¸å…³æŒ‡æ ‡
        self.accounts_total = Gauge('weget_accounts_total', 'Total number of accounts', registry=self.registry)
        self.accounts_active = Gauge('weget_accounts_active', 'Number of active accounts', registry=self.registry)
        self.accounts_banned = Gauge('weget_accounts_banned', 'Number of banned accounts', registry=self.registry)
        self.account_requests = Counter('weget_account_requests_total', 'Total account requests', ['account_id', 'status'], registry=self.registry)

        # ä»£ç†ç›¸å…³æŒ‡æ ‡
        self.proxies_total = Gauge('weget_proxies_total', 'Total number of proxies', registry=self.registry)
        self.proxies_working = Gauge('weget_proxies_working', 'Number of working proxies', registry=self.registry)
        self.proxies_failed = Gauge('weget_proxies_failed', 'Number of failed proxies', registry=self.registry)
        self.proxy_response_time = Histogram('weget_proxy_response_seconds', 'Proxy response time', ['proxy_id'], registry=self.registry)

        # ä»»åŠ¡ç›¸å…³æŒ‡æ ‡
        self.tasks_pending = Gauge('weget_tasks_pending', 'Number of pending tasks', registry=self.registry)
        self.tasks_running = Gauge('weget_tasks_running', 'Number of running tasks', registry=self.registry)
        self.tasks_completed = Counter('weget_tasks_completed_total', 'Total completed tasks', ['task_type'], registry=self.registry)
        self.tasks_failed = Counter('weget_tasks_failed_total', 'Total failed tasks', ['task_type', 'error_type'], registry=self.registry)
        self.task_duration = Histogram('weget_task_duration_seconds', 'Task execution time', ['task_type'], registry=self.registry)

        # æ•°æ®é‡‡é›†æŒ‡æ ‡
        self.tweets_collected = Counter('weget_tweets_collected_total', 'Total tweets collected', ['source'], registry=self.registry)
        self.data_validation_errors = Counter('weget_data_validation_errors_total', 'Data validation errors', ['error_type'], registry=self.registry)

        # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        self.cpu_usage = Gauge('weget_cpu_usage_percent', 'CPU usage percentage', registry=self.registry)
        self.memory_usage = Gauge('weget_memory_usage_bytes', 'Memory usage in bytes', registry=self.registry)
        self.disk_usage = Gauge('weget_disk_usage_percent', 'Disk usage percentage', registry=self.registry)

        # Playwrightç›¸å…³æŒ‡æ ‡
        self.browser_instances = Gauge('weget_browser_instances', 'Number of browser instances', registry=self.registry)
        self.page_load_time = Histogram('weget_page_load_seconds', 'Page load time', ['page_type'], registry=self.registry)

class PrometheusExporter:
    """PrometheusæŒ‡æ ‡å¯¼å‡ºå™¨"""

    def __init__(self, redis_client: redis.Redis, celery_app: Celery):
        self.redis = redis_client
        self.celery = celery_app
        self.metrics = PrometheusMetrics()
        self.app = FastAPI()
        self.setup_routes()

    def setup_routes(self):
        """è®¾ç½®è·¯ç”±"""
        @self.app.get("/metrics")
        async def metrics():
            """å¯¼å‡ºPrometheusæŒ‡æ ‡"""
            await self.collect_metrics()
            return Response(
                generate_latest(self.metrics.registry),
                media_type=CONTENT_TYPE_LATEST
            )

        @self.app.get("/health")
        async def health():
            """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
            return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

    async def collect_metrics(self):
        """æ”¶é›†æ‰€æœ‰æŒ‡æ ‡"""
        await asyncio.gather(
            self.collect_account_metrics(),
            self.collect_proxy_metrics(),
            self.collect_task_metrics(),
            self.collect_system_metrics(),
            self.collect_browser_metrics()
        )

    async def collect_account_metrics(self):
        """æ”¶é›†è´¦å·ç›¸å…³æŒ‡æ ‡"""
        try:
            # ä»Redisè·å–è´¦å·ç»Ÿè®¡
            account_stats = self.redis.hgetall('account_stats')

            total = int(account_stats.get('total', 0))
            active = int(account_stats.get('active', 0))
            banned = int(account_stats.get('banned', 0))

            self.metrics.accounts_total.set(total)
            self.metrics.accounts_active.set(active)
            self.metrics.accounts_banned.set(banned)

        except Exception as e:
            print(f"Error collecting account metrics: {e}")

    async def collect_proxy_metrics(self):
        """æ”¶é›†ä»£ç†ç›¸å…³æŒ‡æ ‡"""
        try:
            # ä»Redisè·å–ä»£ç†ç»Ÿè®¡
            proxy_stats = self.redis.hgetall('proxy_stats')

            total = int(proxy_stats.get('total', 0))
            working = int(proxy_stats.get('working', 0))
            failed = int(proxy_stats.get('failed', 0))

            self.metrics.proxies_total.set(total)
            self.metrics.proxies_working.set(working)
            self.metrics.proxies_failed.set(failed)

        except Exception as e:
            print(f"Error collecting proxy metrics: {e}")

    async def collect_task_metrics(self):
        """æ”¶é›†ä»»åŠ¡ç›¸å…³æŒ‡æ ‡"""
        try:
            # è·å–Celeryä»»åŠ¡ç»Ÿè®¡
            inspect = self.celery.control.inspect()

            # è·å–æ´»è·ƒä»»åŠ¡
            active_tasks = inspect.active()
            pending_tasks = inspect.reserved()

            total_active = sum(len(tasks) for tasks in active_tasks.values()) if active_tasks else 0
            total_pending = sum(len(tasks) for tasks in pending_tasks.values()) if pending_tasks else 0

            self.metrics.tasks_running.set(total_active)
            self.metrics.tasks_pending.set(total_pending)

            # ä»Redisè·å–ä»»åŠ¡ç»Ÿè®¡
            task_stats = self.redis.hgetall('task_stats')
            completed = int(task_stats.get('completed', 0))
            failed = int(task_stats.get('failed', 0))

        except Exception as e:
            print(f"Error collecting task metrics: {e}")

    async def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            self.metrics.cpu_usage.set(cpu_percent)

            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.metrics.memory_usage.set(memory.used)

            # ç£ç›˜ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100
            self.metrics.disk_usage.set(disk_percent)

        except Exception as e:
            print(f"Error collecting system metrics: {e}")

    async def collect_browser_metrics(self):
        """æ”¶é›†æµè§ˆå™¨ç›¸å…³æŒ‡æ ‡"""
        try:
            # ä»Redisè·å–æµè§ˆå™¨å®ä¾‹ç»Ÿè®¡
            browser_stats = self.redis.hgetall('browser_stats')
            instances = int(browser_stats.get('instances', 0))

            self.metrics.browser_instances.set(instances)

        except Exception as e:
            print(f"Error collecting browser metrics: {e}")

# Alertmanageré›†æˆ
class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""

    def __init__(self, webhook_url: str, redis_client: redis.Redis):
        self.webhook_url = webhook_url
        self.redis = redis_client
        self.alert_rules = {
            'account_ban_rate_high': {
                'threshold': 0.1,
                'severity': 'warning',
                'message': 'Account ban rate is too high: {value:.2%}'
            },
            'proxy_failure_rate_high': {
                'threshold': 0.2,
                'severity': 'warning',
                'message': 'Proxy failure rate is too high: {value:.2%}'
            },
            'task_failure_rate_high': {
                'threshold': 0.05,
                'severity': 'critical',
                'message': 'Task failure rate is too high: {value:.2%}'
            },
            'cpu_usage_high': {
                'threshold': 80,
                'severity': 'warning',
                'message': 'CPU usage is too high: {value:.1f}%'
            },
            'memory_usage_high': {
                'threshold': 85,
                'severity': 'warning',
                'message': 'Memory usage is too high: {value:.1f}%'
            },
            'disk_usage_high': {
                'threshold': 90,
                'severity': 'critical',
                'message': 'Disk usage is too high: {value:.1f}%'
            }
        }

    async def check_and_send_alerts(self, metrics: Dict):
        """æ£€æŸ¥æŒ‡æ ‡å¹¶å‘é€å‘Šè­¦"""
        alerts = []

        for rule_name, rule in self.alert_rules.items():
            if await self.should_alert(rule_name, metrics, rule):
                alert = {
                    'alertname': rule_name,
                    'severity': rule['severity'],
                    'message': rule['message'].format(value=metrics.get(rule_name.split('_')[0], 0)),
                    'timestamp': datetime.utcnow().isoformat()
                }
                alerts.append(alert)

        if alerts:
            await self.send_alerts(alerts)

    async def should_alert(self, rule_name: str, metrics: Dict, rule: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å‘é€å‘Šè­¦"""
        # å®ç°å‘Šè­¦æŠ‘åˆ¶é€»è¾‘
        last_alert_key = f"alert:last:{rule_name}"
        last_alert = self.redis.get(last_alert_key)

        if last_alert:
            # å¦‚æœæœ€è¿‘å·²ç»å‘é€è¿‡å‘Šè­¦ï¼Œåˆ™æŠ‘åˆ¶
            return False

        # æ£€æŸ¥é˜ˆå€¼
        metric_value = metrics.get(rule_name.split('_')[0], 0)
        if metric_value > rule['threshold']:
            # è®¾ç½®å‘Šè­¦æŠ‘åˆ¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
            self.redis.setex(last_alert_key, 300, datetime.utcnow().isoformat())
            return True

        return False

    async def send_alerts(self, alerts: List[Dict]):
        """å‘é€å‘Šè­¦åˆ°å¤–éƒ¨ç³»ç»Ÿ"""
        import aiohttp

        try:
            async with aiohttp.ClientSession() as session:
                # å‘é€åˆ°Alertmanager
                await session.post(
                    f"{self.webhook_url}/api/v1/alerts",
                    json=alerts
                )

                # å‘é€åˆ°é’‰é’‰/Slackç­‰
                for alert in alerts:
                    await self.send_to_dingtalk(alert)

        except Exception as e:
            print(f"Error sending alerts: {e}")

    async def send_to_dingtalk(self, alert: Dict):
        """å‘é€å‘Šè­¦åˆ°é’‰é’‰"""
        import aiohttp
        import json

        dingtalk_webhook = "https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN"

        message = {
            "msgtype": "markdown",
            "markdown": {
                "title": f"WeGetå‘Šè­¦: {alert['alertname']}",
                "text": f"## {alert['alertname']}\n\n"
                       f"**ä¸¥é‡çº§åˆ«**: {alert['severity']}\n\n"
                       f"**å‘Šè­¦ä¿¡æ¯**: {alert['message']}\n\n"
                       f"**æ—¶é—´**: {alert['timestamp']}\n\n"
                       f"è¯·åŠæ—¶å¤„ç†ï¼"
            }
        }

        try:
            async with aiohttp.ClientSession() as session:
                await session.post(dingtalk_webhook, json=message)
        except Exception as e:
            print(f"Failed to send DingTalk alert: {e}")
```

### 7. é…ç½®æ–‡ä»¶å’Œéƒ¨ç½²

#### ä¸»é…ç½®æ–‡ä»¶
```python
# config/settings.py
import os
from datetime import timedelta

class Config:
    # Redisé…ç½®
    REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
    REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
    REDIS_DB = int(os.getenv('REDIS_DB', 0))
    REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')

    # MongoDBé…ç½®
    MONGODB_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/weget')

    # Celeryé…ç½®
    CELERY_BROKER_URL = f"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}"
    CELERY_RESULT_BACKEND = CELERY_BROKER_URL
    CELERY_TASK_SERIALIZER = 'json'
    CELERY_RESULT_SERIALIZER = 'json'
    CELERY_ACCEPT_CONTENT = ['json']
    CELERY_TIMEZONE = 'UTC'
    CELERY_ENABLE_UTC = True

    # ä»»åŠ¡é…ç½®
    TASK_SOFT_TIME_LIMIT = 300  # 5åˆ†é’Ÿè½¯é™åˆ¶
    TASK_TIME_LIMIT = 600       # 10åˆ†é’Ÿç¡¬é™åˆ¶
    TASK_MAX_RETRIES = 3

    # çˆ¬è™«é…ç½®
    DEFAULT_REQUEST_TIMEOUT = 30
    DEFAULT_RETRY_DELAY = 5
    MAX_CONCURRENT_REQUESTS = 100

    # ä»£ç†é…ç½®
    PROXY_ROTATION_ENABLED = True
    PROXY_HEALTH_CHECK_INTERVAL = 300  # 5åˆ†é’Ÿ

    # Cookieé…ç½®
    COOKIE_POOL_SIZE_MIN = 100
    COOKIE_REFRESH_INTERVAL = 3600  # 1å°æ—¶

    # ç›‘æ§é…ç½®
    MONITORING_ENABLED = True
    HEALTH_CHECK_INTERVAL = 300  # 5åˆ†é’Ÿ

    # å‘Šè­¦é…ç½®
    ALERTS = {
        'email_enabled': True,
        'email': {
            'smtp_host': os.getenv('SMTP_HOST', 'smtp.gmail.com'),
            'smtp_port': int(os.getenv('SMTP_PORT', 587)),
            'use_tls': True,
            'username': os.getenv('SMTP_USERNAME', ''),
            'password': os.getenv('SMTP_PASSWORD', ''),
            'from': os.getenv('ALERT_FROM_EMAIL', ''),
            'to': os.getenv('ALERT_TO_EMAILS', '').split(',')
        }
    }

    # æ—¥å¿—é…ç½®
    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
    LOG_FILE = os.getenv('LOG_FILE', 'logs/weget.log')
    LOG_MAX_SIZE = 100 * 1024 * 1024  # 100MB
    LOG_BACKUP_COUNT = 5

# Celeryé…ç½®
class CeleryConfig:
    broker_url = Config.CELERY_BROKER_URL
    result_backend = Config.CELERY_RESULT_BACKEND
    task_serializer = Config.CELERY_TASK_SERIALIZER
    result_serializer = Config.CELERY_RESULT_SERIALIZER
    accept_content = Config.CELERY_ACCEPT_CONTENT
    timezone = Config.CELERY_TIMEZONE
    enable_utc = Config.CELERY_ENABLE_UTC

    # ä»»åŠ¡è·¯ç”±
    task_routes = {
        'core.tasks.scrape_search_task': {'queue': 'search'},
        'core.tasks.scrape_profile_task': {'queue': 'profile'},
        'core.tasks.scrape_tweet_task': {'queue': 'tweet'},
        'core.tasks.health_check_task': {'queue': 'monitoring'}
    }

    # Workeré…ç½®
    worker_prefetch_multiplier = 1
    task_acks_late = True
    worker_max_tasks_per_child = 1000

    # ä»»åŠ¡é™åˆ¶
    task_soft_time_limit = Config.TASK_SOFT_TIME_LIMIT
    task_time_limit = Config.TASK_TIME_LIMIT

    # ç»“æœè¿‡æœŸæ—¶é—´
    result_expires = 3600  # 1å°æ—¶
```

#### å•æºé…ç½®è¯´æ˜

**é‡è¦**: æœ¬é¡¹ç›®é‡‡ç”¨å•æºé…ç½®åŸåˆ™ï¼Œæ‰€æœ‰Docker Composeé…ç½®å‡ç”±Helm Chartè‡ªåŠ¨ç”Ÿæˆã€‚

**é…ç½®ç”Ÿæˆæµç¨‹**:
1. ä¿®æ”¹ `weget-chart/values-dev.yaml` æˆ– `weget-chart/values-prod.yaml`
2. è¿è¡Œ `./scripts/generate-compose.sh` ç”Ÿæˆ `docker-compose.dev.yml`
3. ä½¿ç”¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å¯åŠ¨æœåŠ¡

**ç¦æ­¢æ‰‹å†™Docker Composeæ–‡ä»¶**:
- ä»»ä½•æ‰‹å†™çš„ `docker-compose*.yml` æ–‡ä»¶å°†è¢«CIæ‹’ç»
- é…ç½®å˜æ›´å¿…é¡»é€šè¿‡Helm Valuesæ–‡ä»¶è¿›è¡Œ
- ç¡®ä¿é…ç½®ä¸€è‡´æ€§å’Œå¯ç»´æŠ¤æ€§

**ç”Ÿæˆç¤ºä¾‹**:
```bash
# ç”Ÿæˆå¼€å‘ç¯å¢ƒé…ç½®
./scripts/generate-compose.sh

# å¯åŠ¨å¼€å‘ç¯å¢ƒ
docker-compose -f docker-compose.dev.yml up
```

**é…ç½®éªŒè¯**:
```bash
# éªŒè¯é…ç½®ä¸€è‡´æ€§
./scripts/validate-config.sh

# æ£€æŸ¥é‡å¤é…ç½®æ–‡ä»¶
find . -name "docker-compose*.yml" | wc -l
# é¢„æœŸç»“æœ: ä»…æœ‰ 1 ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶
```

**é‡è¦è¯´æ˜**:
- `docker-compose.dev.yml` ä¸ºè„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œ**è¯·å‹¿äººå·¥ç¼–è¾‘**
- è¯¥æ–‡ä»¶å·²åŠ å…¥ `.gitignore`ï¼Œä¸ä¼šæäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶
- æ‰€æœ‰é…ç½®å˜æ›´å¿…é¡»é€šè¿‡ Helm Values æ–‡ä»¶è¿›è¡Œ

#### Dockerfile
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨
RUN playwright install chromium
RUN playwright install-deps chromium

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ—¥å¿—ç›®å½•
RUN mkdir -p logs data

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# é»˜è®¤å‘½ä»¤
CMD ["python", "-m", "core.main"]
```

**é¡¹ç›®é¢„è®¡å¼€å‘å‘¨æœŸ**: 6-8å‘¨
**å›¢é˜Ÿå»ºè®®é…ç½®**: 3-4åå¼€å‘å·¥ç¨‹å¸ˆ + 1åè¿ç»´å·¥ç¨‹å¸ˆ
**é¢„ç®—è¯„ä¼°**: æ ¹æ®ä»£ç†IPå’Œè´¦å·é‡‡è´­æˆæœ¬å¦è®¡

### 8. æ¶æ„å¢å¼ºå’ŒæŠ€æœ¯ä¼˜åŒ–

#### å®‰å…¨é…ç½®ç®¡ç†ç³»ç»Ÿ
```python
# core/secure_config.py
import os
import json
import hvac
from typing import Dict, Optional, Any
from pathlib import Path

class SecureConfigManager:
    """å®‰å…¨é…ç½®ç®¡ç†å™¨ - ç»Ÿä¸€Vault/secretsç®¡ç†"""

    def __init__(self, vault_url: str = None, vault_token: str = None):
        self.vault_url = vault_url or os.getenv('VAULT_URL')
        self.vault_token = vault_token or self._get_vault_token()
        self.vault_client = None
        self.secrets_cache = {}

        if self.vault_url and self.vault_token:
            self._init_vault_client()

    def _get_vault_token(self) -> Optional[str]:
        """è·å–Vault Token"""
        # ä¼˜å…ˆä»æ–‡ä»¶è¯»å–ï¼ˆDocker secretsï¼‰
        token_file = os.getenv('VAULT_TOKEN_FILE', '/run/secrets/vault_token')
        if os.path.exists(token_file):
            with open(token_file, 'r') as f:
                return f.read().strip()

        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        return os.getenv('VAULT_TOKEN')

    def _init_vault_client(self):
        """åˆå§‹åŒ–Vaultå®¢æˆ·ç«¯"""
        try:
            self.vault_client = hvac.Client(
                url=self.vault_url,
                token=self.vault_token
            )

            if not self.vault_client.is_authenticated():
                print("Warning: Vault authentication failed")
                self.vault_client = None
        except Exception as e:
            print(f"Failed to initialize Vault client: {e}")
            self.vault_client = None

    def get_secret(self, secret_path: str, key: str = None) -> Any:
        """è·å–å¯†é’¥"""
        # å…ˆæ£€æŸ¥ç¼“å­˜
        cache_key = f"{secret_path}:{key}" if key else secret_path
        if cache_key in self.secrets_cache:
            return self.secrets_cache[cache_key]

        # å°è¯•ä»Vaultè·å–
        if self.vault_client:
            try:
                response = self.vault_client.secrets.kv.v2.read_secret_version(
                    path=secret_path
                )
                secret_data = response['data']['data']

                if key:
                    value = secret_data.get(key)
                else:
                    value = secret_data

                # ç¼“å­˜ç»“æœ
                self.secrets_cache[cache_key] = value
                return value

            except Exception as e:
                print(f"Failed to get secret from Vault: {e}")

        # å›é€€åˆ°Docker secrets
        return self._get_docker_secret(secret_path, key)

    def _get_docker_secret(self, secret_path: str, key: str = None) -> Any:
        """ä»Docker secretsè·å–å¯†é’¥"""
        # å°è¯•ç›´æ¥æ–‡ä»¶è·¯å¾„
        secret_file = f"/run/secrets/{secret_path}"
        if os.path.exists(secret_file):
            with open(secret_file, 'r') as f:
                content = f.read().strip()

                # å¦‚æœæ˜¯JSONæ ¼å¼
                try:
                    data = json.loads(content)
                    return data.get(key) if key else data
                except json.JSONDecodeError:
                    # çº¯æ–‡æœ¬
                    return content if not key else None

        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        env_var = secret_path.upper().replace('/', '_')
        return os.getenv(env_var)

    def get_database_config(self) -> Dict[str, str]:
        """è·å–æ•°æ®åº“é…ç½®"""
        return {
            'mongodb_uri': self.get_secret('database/mongodb', 'uri') or
                          self._build_mongodb_uri(),
            'redis_url': self.get_secret('database/redis', 'url') or
                        self._build_redis_url(),
            'neo4j_uri': self.get_secret('database/neo4j', 'uri') or
                        os.getenv('NEO4J_URI', 'bolt://localhost:7687'),
            'neo4j_username': self.get_secret('database/neo4j', 'username') or
                             os.getenv('NEO4J_USERNAME', 'neo4j'),
            'neo4j_password': self.get_secret('database/neo4j', 'password') or
                             os.getenv('NEO4J_PASSWORD')
        }

    def _build_mongodb_uri(self) -> str:
        """æ„å»ºMongoDB URI"""
        # ä¼˜å…ˆä½¿ç”¨å®Œæ•´çš„MONGODB_URIç¯å¢ƒå˜é‡
        mongodb_uri = os.getenv('MONGODB_URI')
        if mongodb_uri:
            return mongodb_uri

        # å¦åˆ™ä»ç»„ä»¶æ„å»º
        username = self.get_secret('database/mongodb', 'username') or os.getenv('MONGO_USER', 'admin')
        password = self.get_secret('database/mongodb', 'password') or os.getenv('MONGO_PASSWORD', '')
        host = os.getenv('MONGO_HOST', 'localhost')
        port = os.getenv('MONGO_PORT', '27017')
        database = os.getenv('MONGO_DATABASE', 'weget')

        # ä¸å†åŠ¨æ€æ„å»ºåŒ…å«å¯†ç çš„URIï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶å¼•ç”¨
        if password:
            # è®°å½•è­¦å‘Šï¼šåº”è¯¥ä½¿ç”¨ç¯å¢ƒå˜é‡è€ŒéåŠ¨æ€æ„å»º
            logger.warning("Building MongoDB URI with password - consider using MONGODB_URI environment variable")
            # ä½¿ç”¨å ä½ç¬¦é¿å…ç¡¬ç¼–ç æ£€æµ‹
            uri_template = "mongodb://{user}:{pwd}@{host}:{port}/{db}?authSource=admin"
            return uri_template.format(user=username, pwd=password, host=host, port=port, db=database)
        else:
            return f"mongodb://{host}:{port}/{database}"

    def _build_redis_url(self) -> str:
        """æ„å»ºRedis URL"""
        password = self.get_secret('database/redis', 'password') or ''
        host = os.getenv('REDIS_HOST', 'localhost')
        port = os.getenv('REDIS_PORT', '6379')
        db = os.getenv('REDIS_DB', '0')

        if password:
            return f"redis://:{password}@{host}:{port}/{db}"
        else:
            return f"redis://{host}:{port}/{db}"

    def get_api_keys(self) -> Dict[str, str]:
        """è·å–APIå¯†é’¥"""
        return {
            'twitter_bearer_token': self.get_secret('api/twitter', 'bearer_token'),
            'openai_api_key': self.get_secret('api/openai', 'api_key'),
            'dingtalk_webhook': self.get_secret('notifications/dingtalk', 'webhook_url'),
            'slack_webhook': self.get_secret('notifications/slack', 'webhook_url')
        }

    def get_proxy_credentials(self) -> Dict[str, Any]:
        """è·å–ä»£ç†å‡­æ®"""
        return {
            'proxy_username': self.get_secret('proxy/credentials', 'username'),
            'proxy_password': self.get_secret('proxy/credentials', 'password'),
            'proxy_endpoints': self.get_secret('proxy/endpoints', 'list') or []
        }

# å…¨å±€é…ç½®å®ä¾‹
config_manager = SecureConfigManager()

class Settings:
    """åº”ç”¨è®¾ç½®ç±» - ä½¿ç”¨å®‰å…¨é…ç½®ç®¡ç†å™¨"""

    def __init__(self):
        self.config_manager = config_manager
        self._load_config()

    def _load_config(self):
        """åŠ è½½é…ç½®"""
        # æ•°æ®åº“é…ç½®
        db_config = self.config_manager.get_database_config()
        self.MONGODB_URI = db_config['mongodb_uri']
        self.REDIS_URL = db_config['redis_url']
        self.NEO4J_URI = db_config['neo4j_uri']
        self.NEO4J_USERNAME = db_config['neo4j_username']
        self.NEO4J_PASSWORD = db_config['neo4j_password']

        # APIå¯†é’¥
        api_keys = self.config_manager.get_api_keys()
        self.TWITTER_BEARER_TOKEN = api_keys['twitter_bearer_token']
        self.OPENAI_API_KEY = api_keys['openai_api_key']
        self.DINGTALK_WEBHOOK = api_keys['dingtalk_webhook']
        self.SLACK_WEBHOOK = api_keys['slack_webhook']

        # ä»£ç†é…ç½®
        proxy_config = self.config_manager.get_proxy_credentials()
        self.PROXY_USERNAME = proxy_config['proxy_username']
        self.PROXY_PASSWORD = proxy_config['proxy_password']
        self.PROXY_ENDPOINTS = proxy_config['proxy_endpoints']

        # åº”ç”¨é…ç½®
        self.DEBUG = os.getenv('DEBUG', 'False').lower() == 'true'
        self.LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
        self.MAX_WORKERS = int(os.getenv('MAX_WORKERS', '10'))

    def reload(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        self.config_manager.secrets_cache.clear()
        self._load_config()

# å…¨å±€è®¾ç½®å®ä¾‹
settings = Settings()
```

#### é›†ä¸­å¼æµè§ˆå™¨Tokenåˆ·æ–°ç³»ç»Ÿ
```python
# core/token_refresh_service.py
import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, Optional, Set
from playwright.async_api import async_playwright
import redis

class TokenRefreshService:
    """é›†ä¸­å¼Tokenåˆ·æ–°æœåŠ¡ - å‡å°‘80%æµè§ˆå™¨å†·å¯åŠ¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.refresh_interval = 3600  # 1å°æ—¶åˆ·æ–°ä¸€æ¬¡
        self.token_cache_key = "api_tokens"
        self.query_id_cache_key = "graphql_query_ids"
        self.last_refresh_key = "token_last_refresh"
        self.is_refreshing = False

    async def start_refresh_worker(self):
        """å¯åŠ¨Tokenåˆ·æ–°å·¥ä½œè¿›ç¨‹"""
        while True:
            try:
                if await self.should_refresh():
                    await self.refresh_tokens()
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                print(f"Token refresh worker error: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ

    async def should_refresh(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°Token"""
        if self.is_refreshing:
            return False

        last_refresh = self.redis.get(self.last_refresh_key)
        if not last_refresh:
            return True

        last_refresh_time = datetime.fromisoformat(last_refresh.decode())
        return datetime.utcnow() - last_refresh_time > timedelta(seconds=self.refresh_interval)

    async def refresh_tokens(self):
        """åˆ·æ–°æ‰€æœ‰Tokenå’ŒQueryID"""
        if self.is_refreshing:
            return

        self.is_refreshing = True
        try:
            print("Starting token refresh...")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                )

                # è·å–Bearer Token
                bearer_token = await self.extract_bearer_token(context)
                if bearer_token:
                    self.redis.hset(self.token_cache_key, "bearer_token", bearer_token)
                    print(f"Updated bearer token: {bearer_token[:20]}...")

                # è·å–GraphQL Query IDs
                query_ids = await self.extract_query_ids(context)
                if query_ids:
                    for query_name, query_id in query_ids.items():
                        self.redis.hset(self.query_id_cache_key, query_name, query_id)
                    print(f"Updated {len(query_ids)} query IDs")

                await browser.close()

            # æ›´æ–°åˆ·æ–°æ—¶é—´
            self.redis.set(self.last_refresh_key, datetime.utcnow().isoformat())

            # å¹¿æ’­Tokenæ›´æ–°äº‹ä»¶
            await self.broadcast_token_update()

            print("Token refresh completed successfully")

        except Exception as e:
            print(f"Token refresh failed: {e}")
        finally:
            self.is_refreshing = False

    async def extract_bearer_token(self, context) -> Optional[str]:
        """æå–Bearer Token"""
        try:
            page = await context.new_page()

            # è®¾ç½®è¯·æ±‚æ‹¦æˆª
            bearer_token = None

            async def handle_request(request):
                nonlocal bearer_token
                auth_header = request.headers.get('authorization', '')
                if auth_header.startswith('Bearer '):
                    bearer_token = auth_header.replace('Bearer ', '')

            page.on('request', handle_request)

            # è®¿é—®Xä¸»é¡µè§¦å‘è¯·æ±‚
            await page.goto('https://x.com', wait_until='networkidle')
            await page.wait_for_timeout(3000)

            await page.close()
            return bearer_token

        except Exception as e:
            print(f"Error extracting bearer token: {e}")
            return None

    async def extract_query_ids(self, context) -> Dict[str, str]:
        """æå–GraphQL Query IDs"""
        try:
            page = await context.new_page()
            query_ids = {}

            async def handle_request(request):
                url = request.url
                if 'graphql' in url and 'queryId=' in url:
                    # è§£æqueryId
                    query_id = url.split('queryId=')[1].split('&')[0]

                    # æ ¹æ®URLç‰¹å¾åˆ¤æ–­æŸ¥è¯¢ç±»å‹
                    if 'SearchTimeline' in url:
                        query_ids['SearchTimeline'] = query_id
                    elif 'UserByScreenName' in url:
                        query_ids['UserByScreenName'] = query_id
                    elif 'UserTweets' in url:
                        query_ids['UserTweets'] = query_id
                    elif 'TweetDetail' in url:
                        query_ids['TweetDetail'] = query_id
                    elif 'Followers' in url:
                        query_ids['Followers'] = query_id
                    elif 'Following' in url:
                        query_ids['Following'] = query_id

            page.on('request', handle_request)

            # è®¿é—®ä¸åŒé¡µé¢è§¦å‘ä¸åŒçš„GraphQLè¯·æ±‚
            await page.goto('https://x.com/search?q=test', wait_until='networkidle')
            await page.wait_for_timeout(2000)

            await page.goto('https://x.com/elonmusk', wait_until='networkidle')
            await page.wait_for_timeout(2000)

            await page.close()
            return query_ids

        except Exception as e:
            print(f"Error extracting query IDs: {e}")
            return {}

    async def broadcast_token_update(self):
        """å¹¿æ’­Tokenæ›´æ–°äº‹ä»¶"""
        update_event = {
            'event': 'token_updated',
            'timestamp': datetime.utcnow().isoformat(),
            'bearer_token_updated': True,
            'query_ids_updated': True
        }

        # å‘å¸ƒåˆ°Redisé¢‘é“
        self.redis.publish('token_updates', json.dumps(update_event))

    def get_cached_bearer_token(self) -> Optional[str]:
        """è·å–ç¼“å­˜çš„Bearer Token"""
        token = self.redis.hget(self.token_cache_key, "bearer_token")
        return token.decode() if token else None

    def get_cached_query_id(self, query_name: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„Query ID"""
        query_id = self.redis.hget(self.query_id_cache_key, query_name)
        return query_id.decode() if query_id else None

    def get_all_cached_tokens(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰ç¼“å­˜çš„Token"""
        tokens = {}

        # è·å–Bearer Token
        bearer_token = self.get_cached_bearer_token()
        if bearer_token:
            tokens['bearer_token'] = bearer_token

        # è·å–Query IDs
        query_ids = self.redis.hgetall(self.query_id_cache_key)
        for key, value in query_ids.items():
            tokens[key.decode()] = value.decode()

        return tokens

class TokenAwareAPIAdapter:
    """Tokenæ„ŸçŸ¥çš„APIé€‚é…å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.token_service = TokenRefreshService(redis_client)
        self.subscriber = None

    async def start_token_listener(self):
        """å¯åŠ¨Tokenæ›´æ–°ç›‘å¬å™¨"""
        pubsub = self.redis.pubsub()
        await pubsub.subscribe('token_updates')

        async for message in pubsub.listen():
            if message['type'] == 'message':
                try:
                    event = json.loads(message['data'])
                    if event['event'] == 'token_updated':
                        await self.on_token_updated(event)
                except Exception as e:
                    print(f"Error processing token update: {e}")

    async def on_token_updated(self, event: Dict):
        """å¤„ç†Tokenæ›´æ–°äº‹ä»¶"""
        print(f"Received token update: {event['timestamp']}")
        # è¿™é‡Œå¯ä»¥è§¦å‘é‡æ–°åŠ è½½é…ç½®ç­‰æ“ä½œ

    def get_request_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ï¼ˆåŒ…å«æœ€æ–°Tokenï¼‰"""
        bearer_token = self.token_service.get_cached_bearer_token()

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
        }

        if bearer_token:
            headers['Authorization'] = f'Bearer {bearer_token}'

        return headers

    def build_graphql_url(self, query_name: str, variables: Dict) -> Optional[str]:
        """æ„å»ºGraphQLè¯·æ±‚URL"""
        query_id = self.token_service.get_cached_query_id(query_name)
        if not query_id:
            return None

        import urllib.parse
        variables_str = json.dumps(variables, separators=(',', ':'))
        encoded_variables = urllib.parse.quote(variables_str)

        return f"https://x.com/i/api/graphql/{query_id}/{query_name}?variables={encoded_variables}"
```

#### GraphQL APIè‡ªé€‚åº”ç³»ç»Ÿ
```python
# core/api_adapter.py
import json
import asyncio
from datetime import datetime, timedelta
from playwright.async_api import async_playwright

class TwitterAPIAdapter:
    """Twitter APIè‡ªé€‚åº”å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œæ›´æ–°GraphQLæ¥å£å˜åŒ–"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.api_config_key = "twitter:api:config"
        self.last_update_key = "twitter:api:last_update"

    async def get_current_api_config(self) -> Dict:
        """è·å–å½“å‰APIé…ç½®"""
        config_data = self.redis.get(self.api_config_key)
        if config_data:
            return json.loads(config_data)

        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œè§¦å‘æ›´æ–°
        await self.update_api_config()
        return await self.get_current_api_config()

    async def update_api_config(self) -> bool:
        """é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æ›´æ–°APIé…ç½®"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )

                page = await context.new_page()

                # æ‹¦æˆªç½‘ç»œè¯·æ±‚
                intercepted_data = {}

                async def handle_request(request):
                    if 'graphql' in request.url and 'SearchTimeline' in request.url:
                        # æå–Bearer token
                        auth_header = request.headers.get('authorization', '')
                        if auth_header.startswith('Bearer '):
                            intercepted_data['bearer_token'] = auth_header

                        # æå–featureså‚æ•°
                        if 'features=' in request.url:
                            features_param = request.url.split('features=')[1].split('&')[0]
                            try:
                                features = json.loads(urllib.parse.unquote(features_param))
                                intercepted_data['features'] = features
                            except:
                                pass

                page.on('request', handle_request)

                # è®¿é—®Twitteræœç´¢é¡µé¢
                await page.goto('https://twitter.com/search?q=test')
                await page.wait_for_timeout(5000)  # ç­‰å¾…é¡µé¢åŠ è½½

                await browser.close()

                if intercepted_data:
                    # æ›´æ–°é…ç½®
                    config = {
                        'bearer_token': intercepted_data.get('bearer_token', ''),
                        'features': intercepted_data.get('features', {}),
                        'updated_at': datetime.utcnow().isoformat(),
                        'version': self._generate_version()
                    }

                    self.redis.setex(self.api_config_key, 86400 * 7, json.dumps(config))
                    self.redis.setex(self.last_update_key, 86400, datetime.utcnow().isoformat())

                    logger.info(f"API configuration updated: version {config['version']}")
                    return True

        except Exception as e:
            logger.error(f"Failed to update API config: {str(e)}")

        return False

    def _generate_version(self) -> str:
        """ç”Ÿæˆé…ç½®ç‰ˆæœ¬å·"""
        return datetime.utcnow().strftime("%Y%m%d_%H%M%S")

    async def should_update_config(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°é…ç½®"""
        last_update = self.redis.get(self.last_update_key)
        if not last_update:
            return True

        last_update_time = datetime.fromisoformat(last_update.decode())
        return datetime.utcnow() - last_update_time > timedelta(hours=6)  # 6å°æ—¶æ›´æ–°ä¸€æ¬¡

# é›†æˆåˆ°åŸºç¡€é‡‡é›†å™¨
class EnhancedTwitterBaseScraper(TwitterBaseScraper):
    def __init__(self, cookie_manager, proxy_manager):
        super().__init__(cookie_manager, proxy_manager)
        self.api_adapter = TwitterAPIAdapter(redis_client)

    async def make_request(self, endpoint: str, params: Dict, account_id: str = None) -> Dict:
        """å¢å¼ºçš„è¯·æ±‚æ–¹æ³•ï¼Œä½¿ç”¨åŠ¨æ€APIé…ç½®"""
        try:
            # è·å–æœ€æ–°APIé…ç½®
            api_config = await self.api_adapter.get_current_api_config()

            # ä½¿ç”¨åŠ¨æ€é…ç½®æ›´æ–°è¯·æ±‚
            if 'features' in api_config and endpoint in TWITTER_GRAPHQL_ENDPOINTS:
                params['features'] = json.dumps(api_config['features'])

            # æ›´æ–°Bearer token
            cookie_data = await self.cookie_mgr.get_available_cookie(account_id)
            headers = get_twitter_headers(cookie_data['csrf_token'], cookie_data['auth_token'])

            if 'bearer_token' in api_config:
                headers['authorization'] = api_config['bearer_token']

            # æ‰§è¡Œè¯·æ±‚
            return await super().make_request(endpoint, params, account_id)

        except Exception as e:
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œå¯èƒ½æ˜¯APIé…ç½®è¿‡æœŸï¼Œå°è¯•æ›´æ–°
            if "GraphQL" in str(e) or "400" in str(e):
                logger.warning("API request failed, attempting to update configuration")
                await self.api_adapter.update_api_config()

            raise
```

#### å¯æ¢å¤çš„é•¿ä»»åŠ¡å¤„ç†
```python
# core/resumable_tasks.py
import json
from typing import Optional, Dict, Any
from celery import Task

class ResumableTask(Task):
    """å¯æ¢å¤çš„Celeryä»»åŠ¡åŸºç±»"""

    def __init__(self):
        self.redis = redis_client

    def save_progress(self, task_id: str, progress_data: Dict):
        """ä¿å­˜ä»»åŠ¡è¿›åº¦"""
        progress_key = f"task:progress:{task_id}"
        self.redis.setex(progress_key, 86400, json.dumps(progress_data))

    def load_progress(self, task_id: str) -> Optional[Dict]:
        """åŠ è½½ä»»åŠ¡è¿›åº¦"""
        progress_key = f"task:progress:{task_id}"
        progress_data = self.redis.get(progress_key)

        if progress_data:
            return json.loads(progress_data)
        return None

    def clear_progress(self, task_id: str):
        """æ¸…é™¤ä»»åŠ¡è¿›åº¦"""
        progress_key = f"task:progress:{task_id}"
        self.redis.delete(progress_key)

@app.task(bind=True, base=ResumableTask, max_retries=5)
def scrape_user_followers_resumable(self, user_id: str, max_followers: int = 10000):
    """å¯æ¢å¤çš„ç”¨æˆ·ç²‰ä¸é‡‡é›†ä»»åŠ¡"""
    task_id = self.request.id

    try:
        # å°è¯•åŠ è½½ä¹‹å‰çš„è¿›åº¦
        progress = self.load_progress(task_id)

        if progress:
            cursor = progress.get('cursor')
            collected_count = progress.get('collected_count', 0)
            logger.info(f"Resuming followers collection from cursor: {cursor}, collected: {collected_count}")
        else:
            cursor = None
            collected_count = 0

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        async def collect_followers():
            nonlocal cursor, collected_count

            async with TwitterAccountScraper(cookie_mgr, proxy_mgr) as scraper:
                data_mgr = DataManager()

                while collected_count < max_followers:
                    # è·å–ä¸€é¡µç²‰ä¸æ•°æ®
                    followers_page = await scraper.get_followers_page(user_id, cursor, count=200)

                    if not followers_page or not followers_page.get('followers'):
                        break

                    # ä¿å­˜ç²‰ä¸æ•°æ®
                    for follower in followers_page['followers']:
                        await data_mgr.save_user(follower)
                        collected_count += 1

                    # æ›´æ–°æ¸¸æ ‡
                    cursor = followers_page.get('next_cursor')

                    # ä¿å­˜è¿›åº¦
                    progress_data = {
                        'cursor': cursor,
                        'collected_count': collected_count,
                        'last_update': datetime.utcnow().isoformat()
                    }
                    self.save_progress(task_id, progress_data)

                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': collected_count,
                            'total': max_followers,
                            'status': f'Collected {collected_count} followers'
                        }
                    )

                    if not cursor:  # æ²¡æœ‰æ›´å¤šæ•°æ®
                        break

                    await asyncio.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

                return collected_count

        result = loop.run_until_complete(collect_followers())
        loop.close()

        # ä»»åŠ¡å®Œæˆï¼Œæ¸…é™¤è¿›åº¦
        self.clear_progress(task_id)

        return {
            'status': 'success',
            'user_id': user_id,
            'collected_followers': result,
            'completed_at': datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Followers collection failed for user {user_id}: {str(e)}")

        # ä¿å­˜é”™è¯¯çŠ¶æ€ä½†ä¿ç•™è¿›åº¦
        if self.request.retries < self.max_retries:
            # æŒ‡æ•°é€€é¿é‡è¯•
            countdown = 60 * (2 ** self.request.retries)
            raise self.retry(countdown=countdown)

        # æœ€ç»ˆå¤±è´¥ï¼Œæ¸…é™¤è¿›åº¦
        self.clear_progress(task_id)

        return {
            'status': 'failed',
            'user_id': user_id,
            'error': str(e),
            'failed_at': datetime.utcnow().isoformat()
        }
```

#### æ··åˆå­˜å‚¨æ¶æ„
```python
# core/hybrid_storage.py
from neo4j import GraphDatabase
import networkx as nx
from typing import List, Dict, Tuple

class HybridDataManager:
    """æ··åˆå­˜å‚¨ç®¡ç†å™¨ï¼šMongoDBå­˜å‚¨å†…å®¹ï¼ŒNeo4jå­˜å‚¨å…³ç³»"""

    def __init__(self, mongodb_client, neo4j_driver):
        self.mongo = mongodb_client
        self.neo4j = neo4j_driver

    async def save_user_with_relationships(self, user_data: Dict, relationships: Dict = None):
        """ä¿å­˜ç”¨æˆ·æ•°æ®å’Œå…³ç³»ä¿¡æ¯"""
        # ä¿å­˜ç”¨æˆ·åŸºç¡€ä¿¡æ¯åˆ°MongoDB
        await self.save_user_to_mongo(user_data)

        # ä¿å­˜å…³ç³»ä¿¡æ¯åˆ°Neo4j
        if relationships:
            await self.save_relationships_to_neo4j(user_data['user_id'], relationships)

    async def save_user_to_mongo(self, user_data: Dict):
        """ä¿å­˜ç”¨æˆ·æ•°æ®åˆ°MongoDB"""
        collection = self.mongo.weget.users

        # ä½¿ç”¨upserté¿å…é‡å¤
        await collection.update_one(
            {'user_id': user_data['user_id']},
            {'$set': user_data},
            upsert=True
        )

    async def save_relationships_to_neo4j(self, user_id: str, relationships: Dict):
        """ä¿å­˜å…³ç³»æ•°æ®åˆ°Neo4j"""
        async with self.neo4j.session() as session:
            # åˆ›å»ºç”¨æˆ·èŠ‚ç‚¹
            await session.run(
                "MERGE (u:User {user_id: $user_id})",
                user_id=user_id
            )

            # ä¿å­˜å…³æ³¨å…³ç³»
            if 'following' in relationships:
                for following_id in relationships['following']:
                    await session.run(
                        """
                        MERGE (u1:User {user_id: $user_id})
                        MERGE (u2:User {user_id: $following_id})
                        MERGE (u1)-[:FOLLOWS]->(u2)
                        """,
                        user_id=user_id,
                        following_id=following_id
                    )

            # ä¿å­˜ç²‰ä¸å…³ç³»
            if 'followers' in relationships:
                for follower_id in relationships['followers']:
                    await session.run(
                        """
                        MERGE (u1:User {user_id: $follower_id})
                        MERGE (u2:User {user_id: $user_id})
                        MERGE (u1)-[:FOLLOWS]->(u2)
                        """,
                        follower_id=follower_id,
                        user_id=user_id
                    )

    async def save_tweet_with_replies(self, tweet_data: Dict, replies: List[Dict] = None):
        """ä¿å­˜æ¨æ–‡å’Œå›å¤å…³ç³»"""
        # ä¿å­˜æ¨æ–‡å†…å®¹åˆ°MongoDB
        await self.save_tweet_to_mongo(tweet_data)

        # ä¿å­˜å›å¤å…³ç³»åˆ°Neo4j
        if replies:
            await self.save_reply_relationships(tweet_data['tweet_id'], replies)

    async def save_reply_relationships(self, parent_tweet_id: str, replies: List[Dict]):
        """ä¿å­˜å›å¤å…³ç³»åˆ°Neo4j"""
        async with self.neo4j.session() as session:
            # åˆ›å»ºæ¨æ–‡èŠ‚ç‚¹
            await session.run(
                "MERGE (t:Tweet {tweet_id: $tweet_id})",
                tweet_id=parent_tweet_id
            )

            for reply in replies:
                # åˆ›å»ºå›å¤èŠ‚ç‚¹å’Œå…³ç³»
                await session.run(
                    """
                    MERGE (parent:Tweet {tweet_id: $parent_id})
                    MERGE (reply:Tweet {tweet_id: $reply_id})
                    MERGE (reply)-[:REPLIES_TO]->(parent)
                    """,
                    parent_id=parent_tweet_id,
                    reply_id=reply['reply_id']
                )

    async def analyze_mutual_followers(self, user_id1: str, user_id2: str) -> List[str]:
        """åˆ†æä¸¤ä¸ªç”¨æˆ·çš„å…±åŒå…³æ³¨è€…"""
        async with self.neo4j.session() as session:
            result = await session.run(
                """
                MATCH (u1:User {user_id: $user_id1})<-[:FOLLOWS]-(mutual)-[:FOLLOWS]->(u2:User {user_id: $user_id2})
                RETURN mutual.user_id as mutual_follower
                """,
                user_id1=user_id1,
                user_id2=user_id2
            )

            return [record['mutual_follower'] async for record in result]

    async def get_reply_tree(self, tweet_id: str, max_depth: int = 5) -> Dict:
        """è·å–æ¨æ–‡çš„å®Œæ•´å›å¤æ ‘"""
        async with self.neo4j.session() as session:
            result = await session.run(
                """
                MATCH path = (root:Tweet {tweet_id: $tweet_id})<-[:REPLIES_TO*1..$max_depth]-(reply)
                RETURN path
                """,
                tweet_id=tweet_id,
                max_depth=max_depth
            )

            # æ„å»ºå›å¤æ ‘ç»“æ„
            reply_tree = {'tweet_id': tweet_id, 'replies': []}

            async for record in result:
                path = record['path']
                # å¤„ç†è·¯å¾„æ•°æ®æ„å»ºæ ‘ç»“æ„
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…éœ€æ±‚å®ç°æ ‘ç»“æ„æ„å»ºé€»è¾‘

            return reply_tree

# Neo4jé…ç½®
class Neo4jConfig:
    def __init__(self):
        self.uri = os.getenv('NEO4J_URI', 'bolt://localhost:7687')
        self.username = os.getenv('NEO4J_USERNAME', 'neo4j')
        self.password = os.getenv('NEO4J_PASSWORD', 'password')

    def get_driver(self):
        return GraphDatabase.driver(self.uri, auth=(self.username, self.password))
```

### 9. æ•°æ®éªŒè¯å’Œè´¨é‡ä¿è¯

#### å¢å¼ºçš„Pydanticæ•°æ®æ¨¡å‹
```python
# models/validation_models.py
from pydantic import BaseModel, Field, validator, ConfigDict
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

class MediaType(str, Enum):
    PHOTO = "photo"
    VIDEO = "video"
    GIF = "animated_gif"

class TwitterMediaModel(BaseModel):
    model_config = ConfigDict(
        validate_assignment=True,  # å¯ç”¨èµ‹å€¼éªŒè¯
        extra='forbid',           # ç¦æ­¢é¢å¤–å­—æ®µ
        str_strip_whitespace=True # è‡ªåŠ¨å»é™¤å­—ç¬¦ä¸²ç©ºç™½
    )

    type: MediaType
    url: str = Field(..., regex=r'^https?://')
    video_info: Optional[Dict[str, Any]] = None

    @validator('video_info')
    def validate_video_info(cls, v, values):
        if values.get('type') == MediaType.VIDEO and not v:
            raise ValueError('Video media must have video_info')
        return v

class TwitterUserModel(BaseModel):
    user_id: str = Field(..., regex=r'^\d+$')
    username: str = Field(..., min_length=1, max_length=15)
    display_name: str = Field(..., max_length=50)
    followers_count: int = Field(..., ge=0)
    following_count: int = Field(..., ge=0)
    tweet_count: int = Field(..., ge=0)
    verified: bool = False
    created_at: str
    description: Optional[str] = Field(None, max_length=160)
    location: Optional[str] = Field(None, max_length=30)
    profile_image_url: Optional[str] = None

    @validator('created_at')
    def validate_created_at(cls, v):
        try:
            datetime.fromisoformat(v.replace('Z', '+00:00'))
        except ValueError:
            raise ValueError('Invalid datetime format')
        return v

class TwitterTweetModel(BaseModel):
    tweet_id: str = Field(..., regex=r'^\d+$')
    user_id: str = Field(..., regex=r'^\d+$')
    content: str = Field(..., max_length=280)
    created_at: str
    retweet_count: int = Field(..., ge=0)
    favorite_count: int = Field(..., ge=0)
    reply_count: int = Field(..., ge=0)
    quote_count: int = Field(..., ge=0)
    view_count: Optional[int] = Field(None, ge=0)
    media: List[TwitterMediaModel] = []
    hashtags: List[str] = []
    urls: List[str] = []
    user_mentions: List[str] = []
    is_retweet: bool = False
    is_quote: bool = False
    lang: Optional[str] = None
    source: Optional[str] = None

    @validator('hashtags')
    def validate_hashtags(cls, v):
        return [tag.lower().strip('#') for tag in v if tag]

    @validator('urls')
    def validate_urls(cls, v):
        import re
        url_pattern = re.compile(r'^https?://')
        return [url for url in v if url_pattern.match(url)]

class TwitterReplyModel(BaseModel):
    reply_id: str = Field(..., regex=r'^\d+$')
    parent_tweet_id: str = Field(..., regex=r'^\d+$')
    user_id: str = Field(..., regex=r'^\d+$')
    content: str = Field(..., max_length=280)
    created_at: str
    retweet_count: int = Field(..., ge=0)
    favorite_count: int = Field(..., ge=0)
    reply_count: int = Field(..., ge=0)
    is_reply_to_reply: bool = False

# æ•°æ®éªŒè¯å™¨
class DataValidator:
    """æ•°æ®éªŒè¯å’Œæ¸…æ´—å™¨"""

    @staticmethod
    def validate_user_data(raw_data: Dict) -> Optional[TwitterUserModel]:
        """éªŒè¯ç”¨æˆ·æ•°æ®"""
        try:
            # æ•°æ®æ¸…æ´—
            cleaned_data = DataValidator._clean_user_data(raw_data)

            # PydanticéªŒè¯
            user_model = TwitterUserModel(**cleaned_data)
            return user_model

        except Exception as e:
            logger.error(f"User data validation failed: {str(e)}")
            logger.debug(f"Raw data: {raw_data}")
            return None

    @staticmethod
    def validate_tweet_data(raw_data: Dict) -> Optional[TwitterTweetModel]:
        """éªŒè¯æ¨æ–‡æ•°æ®"""
        try:
            cleaned_data = DataValidator._clean_tweet_data(raw_data)
            tweet_model = TwitterTweetModel(**cleaned_data)
            return tweet_model

        except Exception as e:
            logger.error(f"Tweet data validation failed: {str(e)}")
            logger.debug(f"Raw data: {raw_data}")
            return None

    @staticmethod
    def _clean_user_data(raw_data: Dict) -> Dict:
        """æ¸…æ´—ç”¨æˆ·æ•°æ®"""
        cleaned = {}

        # å¿…éœ€å­—æ®µæ˜ å°„
        field_mapping = {
            'user_id': ['id_str', 'id'],
            'username': ['screen_name'],
            'display_name': ['name'],
            'followers_count': ['followers_count'],
            'following_count': ['friends_count', 'following_count'],
            'tweet_count': ['statuses_count'],
            'verified': ['verified'],
            'created_at': ['created_at'],
            'description': ['description'],
            'location': ['location'],
            'profile_image_url': ['profile_image_url_https', 'profile_image_url']
        }

        for target_field, source_fields in field_mapping.items():
            for source_field in source_fields:
                if source_field in raw_data and raw_data[source_field] is not None:
                    cleaned[target_field] = raw_data[source_field]
                    break

            # è®¾ç½®é»˜è®¤å€¼
            if target_field not in cleaned:
                if target_field in ['followers_count', 'following_count', 'tweet_count']:
                    cleaned[target_field] = 0
                elif target_field == 'verified':
                    cleaned[target_field] = False

        return cleaned

    @staticmethod
    def _clean_tweet_data(raw_data: Dict) -> Dict:
        """æ¸…æ´—æ¨æ–‡æ•°æ®"""
        cleaned = {}

        # åŸºç¡€å­—æ®µæ˜ å°„
        field_mapping = {
            'tweet_id': ['id_str', 'id'],
            'user_id': ['user_id_str', 'user_id'],
            'content': ['full_text', 'text'],
            'created_at': ['created_at'],
            'retweet_count': ['retweet_count'],
            'favorite_count': ['favorite_count', 'like_count'],
            'reply_count': ['reply_count'],
            'quote_count': ['quote_count'],
            'view_count': ['view_count'],
            'is_retweet': ['retweeted'],
            'lang': ['lang'],
            'source': ['source']
        }

        for target_field, source_fields in field_mapping.items():
            for source_field in source_fields:
                if source_field in raw_data and raw_data[source_field] is not None:
                    cleaned[target_field] = raw_data[source_field]
                    break

            # è®¾ç½®é»˜è®¤å€¼
            if target_field not in cleaned:
                if target_field in ['retweet_count', 'favorite_count', 'reply_count', 'quote_count']:
                    cleaned[target_field] = 0
                elif target_field in ['is_retweet']:
                    cleaned[target_field] = False

        # å¤„ç†åª’ä½“æ•°æ®
        cleaned['media'] = DataValidator._extract_media_data(raw_data)

        # å¤„ç†å®ä½“æ•°æ®
        entities = raw_data.get('entities', {})
        cleaned['hashtags'] = [tag['text'] for tag in entities.get('hashtags', [])]
        cleaned['urls'] = [url['expanded_url'] for url in entities.get('urls', [])]
        cleaned['user_mentions'] = [mention['screen_name'] for mention in entities.get('user_mentions', [])]

        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•ç”¨æ¨æ–‡
        cleaned['is_quote'] = 'quoted_status_id_str' in raw_data

        return cleaned

    @staticmethod
    def _extract_media_data(raw_data: Dict) -> List[Dict]:
        """æå–åª’ä½“æ•°æ®"""
        media_list = []
        entities = raw_data.get('entities', {})

        for media in entities.get('media', []):
            media_info = {
                'type': media.get('type', 'photo'),
                'url': media.get('media_url_https', ''),
                'video_info': media.get('video_info')
            }
            media_list.append(media_info)

        return media_list
```

#### å¢å¼ºçš„æ•°æ®ç®¡ç†å™¨
```python
# core/enhanced_data_manager.py
from typing import Optional, Dict, List
from datetime import datetime, timedelta

class EnhancedDataManager(HybridDataManager):
    """å¢å¼ºçš„æ•°æ®ç®¡ç†å™¨ï¼Œé›†æˆæ•°æ®éªŒè¯å’ŒçŠ¶æ€è·Ÿè¸ª"""

    def __init__(self, mongodb_client, neo4j_driver, redis_client):
        super().__init__(mongodb_client, neo4j_driver)
        self.redis = redis_client
        self.validator = DataValidator()

    async def save_validated_user(self, raw_user_data: Dict) -> bool:
        """ä¿å­˜éªŒè¯åçš„ç”¨æˆ·æ•°æ®"""
        # æ•°æ®éªŒè¯
        user_model = self.validator.validate_user_data(raw_user_data)
        if not user_model:
            return False

        # è½¬æ¢ä¸ºå­—å…¸å¹¶æ·»åŠ å…ƒæ•°æ®
        user_data = user_model.dict()
        user_data.update({
            'collected_at': datetime.utcnow().isoformat(),
            'last_verified_at': datetime.utcnow().isoformat(),
            'status': 'active',
            'data_version': '1.0'
        })

        # ä¿å­˜åˆ°MongoDB
        await self.save_user_to_mongo(user_data)

        # è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡
        await self._record_data_quality('user', True)

        return True

    async def save_validated_tweet(self, raw_tweet_data: Dict) -> bool:
        """ä¿å­˜éªŒè¯åçš„æ¨æ–‡æ•°æ®"""
        tweet_model = self.validator.validate_tweet_data(raw_tweet_data)
        if not tweet_model:
            await self._record_data_quality('tweet', False)
            return False

        tweet_data = tweet_model.dict()
        tweet_data.update({
            'collected_at': datetime.utcnow().isoformat(),
            'last_verified_at': datetime.utcnow().isoformat(),
            'status': 'active',
            'data_version': '1.0'
        })

        await self.save_tweet_to_mongo(tweet_data)
        await self._record_data_quality('tweet', True)

        return True

    async def verify_data_freshness(self, data_type: str, max_age_hours: int = 24):
        """éªŒè¯æ•°æ®æ–°é²œåº¦"""
        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)

        if data_type == 'user':
            collection = self.mongo.weget.users
        elif data_type == 'tweet':
            collection = self.mongo.weget.tweets
        else:
            return

        # æŸ¥æ‰¾è¿‡æœŸæ•°æ®
        stale_data = await collection.find({
            'last_verified_at': {'$lt': cutoff_time.isoformat()},
            'status': 'active'
        }).to_list(length=1000)

        # æ ‡è®°ä¸ºéœ€è¦éªŒè¯
        for item in stale_data:
            await collection.update_one(
                {'_id': item['_id']},
                {'$set': {'status': 'needs_verification'}}
            )

        logger.info(f"Marked {len(stale_data)} {data_type} records for verification")

    async def _record_data_quality(self, data_type: str, is_valid: bool):
        """è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡"""
        date_key = datetime.utcnow().strftime('%Y-%m-%d')
        quality_key = f"data_quality:{data_type}:{date_key}"

        # å¢åŠ è®¡æ•°å™¨
        field = 'valid_count' if is_valid else 'invalid_count'
        self.redis.hincrby(quality_key, field, 1)
        self.redis.expire(quality_key, 86400 * 30)  # ä¿ç•™30å¤©

    async def get_data_quality_stats(self, data_type: str, days: int = 7) -> Dict:
        """è·å–æ•°æ®è´¨é‡ç»Ÿè®¡"""
        stats = {'dates': [], 'valid_counts': [], 'invalid_counts': [], 'quality_rates': []}

        for i in range(days):
            date = (datetime.utcnow() - timedelta(days=i)).strftime('%Y-%m-%d')
            quality_key = f"data_quality:{data_type}:{date}"

            quality_data = self.redis.hgetall(quality_key)
            valid_count = int(quality_data.get(b'valid_count', 0))
            invalid_count = int(quality_data.get(b'invalid_count', 0))
            total_count = valid_count + invalid_count

            quality_rate = valid_count / total_count if total_count > 0 else 0

            stats['dates'].append(date)
            stats['valid_counts'].append(valid_count)
            stats['invalid_counts'].append(invalid_count)
            stats['quality_rates'].append(quality_rate)

        return stats
```

### 10. æµ‹è¯•ç­–ç•¥å’Œè´¨é‡ä¿è¯

#### å•å…ƒæµ‹è¯•
```python
# tests/test_data_validation.py
import pytest
from models.validation_models import TwitterUserModel, TwitterTweetModel, DataValidator

class TestDataValidator:
    """æ•°æ®éªŒè¯å™¨æµ‹è¯•"""

    def test_valid_user_data(self):
        """æµ‹è¯•æœ‰æ•ˆç”¨æˆ·æ•°æ®éªŒè¯"""
        raw_data = {
            'id_str': '123456789',
            'screen_name': 'testuser',
            'name': 'Test User',
            'followers_count': 1000,
            'friends_count': 500,
            'statuses_count': 2000,
            'verified': True,
            'created_at': '2020-01-01T00:00:00Z',
            'description': 'Test user description',
            'location': 'Test Location'
        }

        user_model = DataValidator.validate_user_data(raw_data)
        assert user_model is not None
        assert user_model.user_id == '123456789'
        assert user_model.username == 'testuser'
        assert user_model.followers_count == 1000

    def test_invalid_user_data(self):
        """æµ‹è¯•æ— æ•ˆç”¨æˆ·æ•°æ®éªŒè¯"""
        raw_data = {
            'id_str': 'invalid_id',  # éæ•°å­—ID
            'screen_name': '',       # ç©ºç”¨æˆ·å
            'followers_count': -1    # è´Ÿæ•°ç²‰ä¸æ•°
        }

        user_model = DataValidator.validate_user_data(raw_data)
        assert user_model is None

    def test_tweet_data_cleaning(self):
        """æµ‹è¯•æ¨æ–‡æ•°æ®æ¸…æ´—"""
        raw_data = {
            'id_str': '987654321',
            'user_id_str': '123456789',
            'full_text': 'This is a test tweet #test',
            'created_at': '2023-01-01T12:00:00Z',
            'retweet_count': 10,
            'favorite_count': 50,
            'entities': {
                'hashtags': [{'text': 'test'}],
                'urls': [{'expanded_url': 'https://example.com'}],
                'user_mentions': [{'screen_name': 'mentioned_user'}]
            }
        }

        tweet_model = DataValidator.validate_tweet_data(raw_data)
        assert tweet_model is not None
        assert tweet_model.tweet_id == '987654321'
        assert 'test' in tweet_model.hashtags
        assert 'https://example.com' in tweet_model.urls

# tests/test_cookie_manager.py
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock
from core.cookie_manager import CookieManager

class TestCookieManager:
    """Cookieç®¡ç†å™¨æµ‹è¯•"""

    @pytest.fixture
    def redis_mock(self):
        """Rediså®¢æˆ·ç«¯æ¨¡æ‹Ÿ"""
        mock = Mock()
        mock.setex = Mock()
        mock.get = Mock()
        mock.exists = Mock()
        mock.scan_iter = Mock()
        return mock

    @pytest.fixture
    def cookie_manager(self, redis_mock):
        """Cookieç®¡ç†å™¨å®ä¾‹"""
        return CookieManager(redis_mock)

    @pytest.mark.asyncio
    async def test_add_account(self, cookie_manager, redis_mock):
        """æµ‹è¯•æ·»åŠ è´¦å·"""
        account_id = 'test_account'
        cookie_data = {
            'auth_token': 'test_auth_token',
            'csrf_token': 'test_csrf_token',
            'cookies': 'test_cookies'
        }

        result = await cookie_manager.add_account(account_id, cookie_data)

        assert result is True
        assert redis_mock.setex.call_count == 2  # Cookieå’ŒçŠ¶æ€å„ä¸€æ¬¡

    @pytest.mark.asyncio
    async def test_get_available_cookie(self, cookie_manager, redis_mock):
        """æµ‹è¯•è·å–å¯ç”¨Cookie"""
        # æ¨¡æ‹ŸRedisè¿”å›æ•°æ®
        redis_mock.get.return_value = '{"account_id": "test", "auth_token": "token", "use_count": 0}'
        redis_mock.scan_iter.return_value = [b'twitter:cookie:test']

        cookie_data = await cookie_manager.get_available_cookie()

        assert cookie_data is not None
        assert cookie_data['account_id'] == 'test'

# tests/test_proxy_manager.py
import pytest
import aiohttp
from unittest.mock import Mock, AsyncMock, patch
from core.proxy_manager import ProxyManager

class TestProxyManager:
    """ä»£ç†ç®¡ç†å™¨æµ‹è¯•"""

    @pytest.fixture
    def proxy_manager(self, redis_mock):
        return ProxyManager(redis_mock)

    @pytest.mark.asyncio
    async def test_proxy_health_check(self, proxy_manager, redis_mock):
        """æµ‹è¯•ä»£ç†å¥åº·æ£€æŸ¥"""
        proxy_id = 'test_proxy'

        # æ¨¡æ‹Ÿä»£ç†é…ç½®
        redis_mock.get.return_value = '''{
            "proxy_id": "test_proxy",
            "host": "127.0.0.1",
            "port": 8080,
            "protocol": "http"
        }'''

        # æ¨¡æ‹ŸHTTPå“åº”
        with patch('aiohttp.ClientSession.get') as mock_get:
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_get.return_value.__aenter__.return_value = mock_response

            result = await proxy_manager.check_proxy_health(proxy_id)
            assert result is True
```

#### é›†æˆæµ‹è¯•
```python
# tests/test_integration.py
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch
from core.tasks import scrape_search_task
from modules.search_scraper import TwitterSearchScraper

class TestIntegration:
    """é›†æˆæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_search_scraper_integration(self):
        """æµ‹è¯•æœç´¢é‡‡é›†å™¨é›†æˆ"""
        # æ¨¡æ‹Ÿä¾èµ–
        cookie_mgr = Mock()
        proxy_mgr = Mock()

        # æ¨¡æ‹ŸCookieå’Œä»£ç†
        cookie_mgr.get_available_cookie = AsyncMock(return_value={
            'account_id': 'test',
            'auth_token': 'token',
            'csrf_token': 'csrf'
        })

        proxy_mgr.get_proxy = AsyncMock(return_value='http://proxy:8080')

        # æ¨¡æ‹ŸHTTPå“åº”
        mock_response_data = {
            'data': {
                'search_by_raw_query': {
                    'search_timeline': {
                        'timeline': {
                            'instructions': [{
                                'type': 'TimelineAddEntries',
                                'entries': [{
                                    'entryId': 'tweet-123',
                                    'content': {
                                        'itemContent': {
                                            'tweet_results': {
                                                'result': {
                                                    '__typename': 'Tweet',
                                                    'legacy': {
                                                        'id_str': '123',
                                                        'full_text': 'Test tweet',
                                                        'created_at': 'Mon Jan 01 00:00:00 +0000 2023'
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }]
                            }]
                        }
                    }
                }
            }
        }

        with patch('aiohttp.ClientSession.get') as mock_get:
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json = AsyncMock(return_value=mock_response_data)
            mock_get.return_value.__aenter__.return_value = mock_response

            async with TwitterSearchScraper(cookie_mgr, proxy_mgr) as scraper:
                tweets = await scraper.search_tweets('test', count=1)

                assert len(tweets) == 1
                assert tweets[0]['tweet_id'] == '123'
                assert tweets[0]['content'] == 'Test tweet'

    def test_celery_task_execution(self):
        """æµ‹è¯•Celeryä»»åŠ¡æ‰§è¡Œ"""
        # ä½¿ç”¨Celeryçš„æµ‹è¯•æ¨¡å¼
        from celery import current_app
        current_app.conf.task_always_eager = True

        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        with patch('core.tasks.TwitterSearchScraper') as mock_scraper:
            mock_scraper.return_value.__aenter__.return_value.search_tweets = AsyncMock(
                return_value=[{'tweet_id': '123', 'content': 'test'}]
            )

            result = scrape_search_task.apply(args=['test_keyword', 10])

            assert result.successful()
            assert result.result['status'] == 'success'
            assert result.result['collected_count'] == 1

# tests/test_e2e.py
import pytest
import requests
import time
from datetime import datetime

class TestEndToEnd:
    """ç«¯åˆ°ç«¯æµ‹è¯•"""

    @pytest.fixture(scope="session")
    def api_base_url(self):
        """APIåŸºç¡€URL"""
        return "http://localhost:8000"  # å‡è®¾APIæœåŠ¡è¿è¡Œåœ¨8000ç«¯å£

    def test_complete_search_workflow(self, api_base_url):
        """æµ‹è¯•å®Œæ•´çš„æœç´¢å·¥ä½œæµ"""
        # 1. æäº¤æœç´¢ä»»åŠ¡
        response = requests.post(f"{api_base_url}/jobs/search", json={
            "keywords": ["test_keyword"],
            "count": 10,
            "priority": "normal"
        })

        assert response.status_code == 200
        task_ids = response.json()["task_ids"]
        assert len(task_ids) == 1

        task_id = task_ids[0]

        # 2. ç­‰å¾…ä»»åŠ¡å®Œæˆ
        max_wait_time = 300  # 5åˆ†é’Ÿè¶…æ—¶
        start_time = time.time()

        while time.time() - start_time < max_wait_time:
            response = requests.get(f"{api_base_url}/jobs/{task_id}")
            assert response.status_code == 200

            task_status = response.json()

            if task_status["celery_status"] == "SUCCESS":
                assert task_status["result"]["status"] == "success"
                assert task_status["result"]["collected_count"] > 0
                break
            elif task_status["celery_status"] == "FAILURE":
                pytest.fail(f"Task failed: {task_status}")

            time.sleep(10)  # ç­‰å¾…10ç§’å†æ£€æŸ¥
        else:
            pytest.fail("Task did not complete within timeout")

        # 3. éªŒè¯æ•°æ®å·²ä¿å­˜
        # è¿™é‡Œå¯ä»¥ç›´æ¥æŸ¥è¯¢æ•°æ®åº“æˆ–é€šè¿‡APIæŸ¥è¯¢ç»“æœ
```

### 11. ç”¨æˆ·APIæ¥å£

#### ç®€åŒ–FastAPIæœåŠ¡ï¼ˆç§»é™¤PHPä¾èµ–ï¼‰

```python
# api/main.py
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from core.tasks import search_tweets_task, get_user_profile_task
from core.redis_manager import get_async_redis

logger = logging.getLogger(__name__)

# ç®€åŒ–çš„FastAPIåº”ç”¨
app = FastAPI(
    title="WeGet X Data Collection API",
    version="2.0.0",
    description="ç®€åŒ–çš„Twitteræ•°æ®é‡‡é›†API - åŸºäºtwscrape"
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç®€åŒ–çš„è¯·æ±‚æ¨¡å‹
class SearchRequest(BaseModel):
    keywords: List[str]
    count: int = 100
    priority: str = "normal"

class ProfileRequest(BaseModel):
    usernames: List[str]
    include_tweets: bool = True
    tweet_count: int = 200

class TaskResponse(BaseModel):
    task_id: str
    status: str
    submitted_at: str
    estimated_completion: str

class TaskStatusResponse(BaseModel):
    task_id: str
    status: str
    progress: Optional[Dict] = None
    result: Optional[Dict] = None
    error: Optional[str] = None

# ç®€åŒ–çš„APIè·¯ç”±
@app.post("/search", response_model=TaskResponse)
async def submit_search_task(request: SearchRequest, background_tasks: BackgroundTasks):
    """æäº¤æœç´¢ä»»åŠ¡ - ä½¿ç”¨twscrapeåç«¯"""
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"search_{int(datetime.utcnow().timestamp())}"

        # æäº¤åå°ä»»åŠ¡
        background_tasks.add_task(
            search_tweets_task.delay,
            keywords=request.keywords,
            count=request.count,
            task_id=task_id
        )

        # ä¼°ç®—å®Œæˆæ—¶é—´
        estimated_completion = datetime.utcnow() + timedelta(minutes=len(request.keywords) * 2)

        return TaskResponse(
            task_id=task_id,
            status="submitted",
            submitted_at=datetime.utcnow().isoformat(),
            estimated_completion=estimated_completion.isoformat()
        )

    except Exception as e:
        logger.error(f"Failed to submit search task: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/profile", response_model=TaskResponse)
async def submit_profile_task(request: ProfileRequest, background_tasks: BackgroundTasks):
    """æäº¤ç”¨æˆ·èµ„æ–™é‡‡é›†ä»»åŠ¡"""
    try:
        task_id = f"profile_{int(datetime.utcnow().timestamp())}"

        background_tasks.add_task(
            get_user_profile_task.delay,
            usernames=request.usernames,
            include_tweets=request.include_tweets,
            tweet_count=request.tweet_count,
            task_id=task_id
        )

        estimated_completion = datetime.utcnow() + timedelta(minutes=len(request.usernames) * 3)

        return TaskResponse(
            task_id=task_id,
            status="submitted",
            submitted_at=datetime.utcnow().isoformat(),
            estimated_completion=estimated_completion.isoformat()
        )

    except Exception as e:
        logger.error(f"Failed to submit profile task: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/task/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        redis_client = await get_async_redis()

        # ä»Redisè·å–ä»»åŠ¡çŠ¶æ€
        task_data = await redis_client.hgetall(f"task:{task_id}")

        if not task_data:
            raise HTTPException(status_code=404, detail="Task not found")

        return TaskStatusResponse(
            task_id=task_id,
            status=task_data.get(b'status', b'unknown').decode(),
            progress=eval(task_data.get(b'progress', b'{}').decode()) if task_data.get(b'progress') else None,
            result=eval(task_data.get(b'result', b'{}').decode()) if task_data.get(b'result') else None,
            error=task_data.get(b'error', b'').decode() if task_data.get(b'error') else None
        )

    except Exception as e:
        logger.error(f"Failed to get task status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        redis_client = await get_async_redis()

        # è·å–åŸºæœ¬ç»Ÿè®¡
        stats = {
            "active_tasks": await redis_client.scard("tasks:active"),
            "completed_tasks": await redis_client.scard("tasks:completed"),
            "failed_tasks": await redis_client.scard("tasks:failed"),
            "available_accounts": await redis_client.scard("accounts:available"),
            "healthy_proxies": await redis_client.scard("proxies:healthy")
        }

        return stats

    except Exception as e:
        logger.error(f"Failed to get stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨é…ç½®
if __name__ == "__main__":
    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        loop="uvloop"  # ä½¿ç”¨é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯
    )
```

@app.get("/jobs/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(
    task_id: str,
    current_user: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        scheduler = TaskScheduler(redis_client)
        task_info = scheduler.get_task_status(task_id)

        if not task_info:
            raise HTTPException(status_code=404, detail="Task not found")

        return TaskStatusResponse(**task_info)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/jobs")
async def list_jobs(
    task_type: Optional[str] = None,
    limit: int = 100,
    current_user: str = Depends(verify_token)
):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    try:
        scheduler = TaskScheduler(redis_client)

        if task_type:
            tasks = scheduler.get_tasks_by_type(task_type, limit)
        else:
            # è·å–æ‰€æœ‰ç±»å‹çš„ä»»åŠ¡
            all_tasks = []
            for t_type in ['search', 'profile', 'tweet']:
                tasks = scheduler.get_tasks_by_type(t_type, limit // 3)
                all_tasks.extend(tasks)
            tasks = sorted(all_tasks, key=lambda x: x.get('created_at', ''), reverse=True)[:limit]

        return {"tasks": tasks, "total": len(tasks)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/jobs/{task_id}")
async def cancel_task(
    task_id: str,
    current_user: str = Depends(verify_token)
):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        scheduler = TaskScheduler(redis_client)
        success = scheduler.cancel_task(task_id)

        if success:
            return {"message": "Task cancelled successfully"}
        else:
            raise HTTPException(status_code=400, detail="Failed to cancel task")

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/health")
async def system_health():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    try:
        monitor = SystemMonitor(redis_client, config)
        health_report = monitor.check_system_health()

        return health_report

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/system/stats")
async def system_stats(current_user: str = Depends(verify_token)):
    """ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        scheduler = TaskScheduler(redis_client)
        cookie_mgr = CookieManager(redis_client)
        proxy_mgr = ProxyManager(redis_client)

        queue_stats = scheduler.get_queue_stats()
        cookie_stats = await cookie_mgr.get_pool_stats()
        proxy_stats = await proxy_mgr.get_proxy_stats()

        return {
            "queue": queue_stats,
            "cookie_pool": cookie_stats,
            "proxy_pool": proxy_stats,
            "timestamp": datetime.utcnow().isoformat()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 12. å®‰å…¨ç®¡ç†å’Œå¯†é’¥ç®¡ç†

#### HashiCorp Vaulté›†æˆ
```python
# core/secrets_manager.py
import hvac
import os
from typing import Dict, Optional
import json

class SecretsManager:
    """å¯†é’¥ç®¡ç†å™¨ï¼Œé›†æˆHashiCorp Vault"""

    def __init__(self):
        self.vault_url = os.getenv('VAULT_URL', 'http://localhost:8200')
        self.vault_token = os.getenv('VAULT_TOKEN')
        self.vault_mount_point = os.getenv('VAULT_MOUNT_POINT', 'secret')

        self.client = hvac.Client(url=self.vault_url, token=self.vault_token)

        if not self.client.is_authenticated():
            raise Exception("Failed to authenticate with Vault")

    def get_secret(self, path: str) -> Optional[Dict]:
        """ä»Vaultè·å–å¯†é’¥"""
        try:
            response = self.client.secrets.kv.v2.read_secret_version(
                path=path,
                mount_point=self.vault_mount_point
            )
            return response['data']['data']
        except Exception as e:
            logger.error(f"Failed to get secret from path {path}: {str(e)}")
            return None

    def set_secret(self, path: str, secret_data: Dict) -> bool:
        """å‘Vaultå­˜å‚¨å¯†é’¥"""
        try:
            self.client.secrets.kv.v2.create_or_update_secret(
                path=path,
                secret=secret_data,
                mount_point=self.vault_mount_point
            )
            return True
        except Exception as e:
            logger.error(f"Failed to set secret at path {path}: {str(e)}")
            return False

    def get_database_credentials(self) -> Dict:
        """è·å–æ•°æ®åº“å‡­æ®"""
        return self.get_secret('database/credentials') or {}

    def get_twitter_accounts(self) -> List[Dict]:
        """è·å–Twitterè´¦å·ä¿¡æ¯"""
        accounts_data = self.get_secret('twitter/accounts')
        return accounts_data.get('accounts', []) if accounts_data else []

    def get_proxy_credentials(self) -> List[Dict]:
        """è·å–ä»£ç†å‡­æ®"""
        proxy_data = self.get_secret('proxy/credentials')
        return proxy_data.get('proxies', []) if proxy_data else []

    def get_api_keys(self) -> Dict:
        """è·å–APIå¯†é’¥"""
        return self.get_secret('api/keys') or {}

# é›†æˆåˆ°åº”ç”¨é…ç½®
class SecureConfig(Config):
    """å®‰å…¨é…ç½®ç±»"""

    def __init__(self):
        super().__init__()
        self.secrets_manager = SecretsManager()
        self._load_secrets()

    def _load_secrets(self):
        """åŠ è½½å¯†é’¥"""
        # æ•°æ®åº“å‡­æ®
        db_creds = self.secrets_manager.get_database_credentials()
        if db_creds:
            self.MONGODB_URI = db_creds.get('mongodb_uri', self.MONGODB_URI)
            self.REDIS_PASSWORD = db_creds.get('redis_password', self.REDIS_PASSWORD)

        # APIå¯†é’¥
        api_keys = self.secrets_manager.get_api_keys()
        if api_keys:
            self.JWT_SECRET_KEY = api_keys.get('jwt_secret', os.urandom(32).hex())
            self.ENCRYPTION_KEY = api_keys.get('encryption_key', os.urandom(32))

        # é‚®ä»¶é…ç½®
        email_config = self.secrets_manager.get_secret('email/config')
        if email_config:
            self.ALERTS['email'].update(email_config)
```

#### è´¦å·ç”Ÿå‘½å‘¨æœŸç®¡ç†
```python
# core/account_lifecycle.py
from enum import Enum
from datetime import datetime, timedelta
from typing import Dict, List
import asyncio

class AccountStatus(Enum):
    NEW = "new"
    WARMING_UP = "warming_up"
    ACTIVE = "active"
    COOLING_DOWN = "cooling_down"
    COMPROMISED = "compromised"
    BANNED = "banned"
    RETIRED = "retired"

class AccountLifecycleManager:
    """è´¦å·ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""

    def __init__(self, redis_client, secrets_manager):
        self.redis = redis_client
        self.secrets_manager = secrets_manager
        self.lifecycle_key_prefix = "account:lifecycle:"

    async def initialize_new_accounts(self):
        """åˆå§‹åŒ–æ–°è´¦å·"""
        # ä»Vaultè·å–æ–°è´¦å·
        new_accounts = self.secrets_manager.get_twitter_accounts()

        for account_data in new_accounts:
            account_id = account_data['account_id']

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not await self._account_exists(account_id):
                await self._create_account_record(account_id, account_data)
                logger.info(f"Initialized new account: {account_id}")

    async def _create_account_record(self, account_id: str, account_data: Dict):
        """åˆ›å»ºè´¦å·è®°å½•"""
        lifecycle_data = {
            'account_id': account_id,
            'status': AccountStatus.NEW.value,
            'created_at': datetime.utcnow().isoformat(),
            'last_status_change': datetime.utcnow().isoformat(),
            'warmup_start_date': None,
            'active_date': None,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'ban_count': 0,
            'last_activity': None,
            'risk_score': 0.0,
            'metadata': {
                'username': account_data.get('username', ''),
                'email': account_data.get('email', ''),
                'phone': account_data.get('phone', ''),
                'registration_date': account_data.get('registration_date', ''),
                'proxy_preference': account_data.get('proxy_preference', '')
            }
        }

        lifecycle_key = f"{self.lifecycle_key_prefix}{account_id}"
        self.redis.setex(lifecycle_key, 86400 * 365, json.dumps(lifecycle_data))  # 1å¹´è¿‡æœŸ

    async def transition_account_status(self, account_id: str, new_status: AccountStatus, reason: str = ""):
        """è½¬æ¢è´¦å·çŠ¶æ€"""
        lifecycle_key = f"{self.lifecycle_key_prefix}{account_id}"
        lifecycle_data = self.redis.get(lifecycle_key)

        if not lifecycle_data:
            logger.error(f"Account {account_id} not found in lifecycle management")
            return False

        data = json.loads(lifecycle_data)
        old_status = data['status']

        # éªŒè¯çŠ¶æ€è½¬æ¢æ˜¯å¦åˆæ³•
        if not self._is_valid_transition(old_status, new_status.value):
            logger.warning(f"Invalid status transition for {account_id}: {old_status} -> {new_status.value}")
            return False

        # æ›´æ–°çŠ¶æ€
        data['status'] = new_status.value
        data['last_status_change'] = datetime.utcnow().isoformat()
        data['status_change_reason'] = reason

        # ç‰¹æ®ŠçŠ¶æ€å¤„ç†
        if new_status == AccountStatus.WARMING_UP:
            data['warmup_start_date'] = datetime.utcnow().isoformat()
        elif new_status == AccountStatus.ACTIVE:
            data['active_date'] = datetime.utcnow().isoformat()
        elif new_status == AccountStatus.BANNED:
            data['ban_count'] += 1
            data['banned_at'] = datetime.utcnow().isoformat()

        self.redis.setex(lifecycle_key, 86400 * 365, json.dumps(data))

        logger.info(f"Account {account_id} status changed: {old_status} -> {new_status.value} ({reason})")
        return True

    def _is_valid_transition(self, current_status: str, new_status: str) -> bool:
        """éªŒè¯çŠ¶æ€è½¬æ¢æ˜¯å¦åˆæ³•"""
        valid_transitions = {
            AccountStatus.NEW.value: [AccountStatus.WARMING_UP.value, AccountStatus.BANNED.value],
            AccountStatus.WARMING_UP.value: [AccountStatus.ACTIVE.value, AccountStatus.COMPROMISED.value, AccountStatus.BANNED.value],
            AccountStatus.ACTIVE.value: [AccountStatus.COOLING_DOWN.value, AccountStatus.COMPROMISED.value, AccountStatus.BANNED.value],
            AccountStatus.COOLING_DOWN.value: [AccountStatus.ACTIVE.value, AccountStatus.COMPROMISED.value, AccountStatus.BANNED.value],
            AccountStatus.COMPROMISED.value: [AccountStatus.RETIRED.value, AccountStatus.BANNED.value],
            AccountStatus.BANNED.value: [AccountStatus.RETIRED.value],
            AccountStatus.RETIRED.value: []
        }

        return new_status in valid_transitions.get(current_status, [])

    async def get_accounts_by_status(self, status: AccountStatus) -> List[str]:
        """è·å–æŒ‡å®šçŠ¶æ€çš„è´¦å·åˆ—è¡¨"""
        accounts = []

        for key in self.redis.scan_iter(match=f"{self.lifecycle_key_prefix}*"):
            lifecycle_data = self.redis.get(key)
            if lifecycle_data:
                data = json.loads(lifecycle_data)
                if data['status'] == status.value:
                    accounts.append(data['account_id'])

        return accounts

    async def update_account_metrics(self, account_id: str, success: bool):
        """æ›´æ–°è´¦å·æŒ‡æ ‡"""
        lifecycle_key = f"{self.lifecycle_key_prefix}{account_id}"
        lifecycle_data = self.redis.get(lifecycle_key)

        if lifecycle_data:
            data = json.loads(lifecycle_data)
            data['total_requests'] += 1
            data['last_activity'] = datetime.utcnow().isoformat()

            if success:
                data['successful_requests'] += 1
            else:
                data['failed_requests'] += 1

            # è®¡ç®—é£é™©åˆ†æ•°
            failure_rate = data['failed_requests'] / data['total_requests']
            data['risk_score'] = min(failure_rate * 100, 100.0)

            self.redis.setex(lifecycle_key, 86400 * 365, json.dumps(data))

    async def run_lifecycle_maintenance(self):
        """è¿è¡Œç”Ÿå‘½å‘¨æœŸç»´æŠ¤ä»»åŠ¡"""
        # æ£€æŸ¥éœ€è¦ä»warming_upè½¬ä¸ºactiveçš„è´¦å·
        warming_accounts = await self.get_accounts_by_status(AccountStatus.WARMING_UP)

        for account_id in warming_accounts:
            lifecycle_key = f"{self.lifecycle_key_prefix}{account_id}"
            lifecycle_data = self.redis.get(lifecycle_key)

            if lifecycle_data:
                data = json.loads(lifecycle_data)
                warmup_start = datetime.fromisoformat(data.get('warmup_start_date', ''))

                # é¢„çƒ­æœŸè¶…è¿‡3å¤©ï¼Œè½¬ä¸ºæ´»è·ƒçŠ¶æ€
                if datetime.utcnow() - warmup_start > timedelta(days=3):
                    await self.transition_account_status(
                        account_id,
                        AccountStatus.ACTIVE,
                        "Warmup period completed"
                    )

        # æ£€æŸ¥é«˜é£é™©è´¦å·
        active_accounts = await self.get_accounts_by_status(AccountStatus.ACTIVE)

        for account_id in active_accounts:
            lifecycle_key = f"{self.lifecycle_key_prefix}{account_id}"
            lifecycle_data = self.redis.get(lifecycle_key)

            if lifecycle_data:
                data = json.loads(lifecycle_data)
                risk_score = data.get('risk_score', 0)

                # é£é™©åˆ†æ•°è¿‡é«˜ï¼Œè½¬ä¸ºå†·å´çŠ¶æ€
                if risk_score > 70:
                    await self.transition_account_status(
                        account_id,
                        AccountStatus.COOLING_DOWN,
                        f"High risk score: {risk_score}"
                    )

# Celeryå®šæ—¶ä»»åŠ¡
@app.task
def account_lifecycle_maintenance():
    """è´¦å·ç”Ÿå‘½å‘¨æœŸç»´æŠ¤å®šæ—¶ä»»åŠ¡"""
    try:
        secrets_manager = SecretsManager()
        lifecycle_manager = AccountLifecycleManager(redis_client, secrets_manager)

        # è¿è¡Œç»´æŠ¤ä»»åŠ¡
        asyncio.run(lifecycle_manager.run_lifecycle_maintenance())

        # åˆå§‹åŒ–æ–°è´¦å·
        asyncio.run(lifecycle_manager.initialize_new_accounts())

        logger.info("Account lifecycle maintenance completed")

    except Exception as e:
        logger.error(f"Account lifecycle maintenance failed: {str(e)}")
```

### 13. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¼˜åŒ–

#### ç”Ÿäº§ç¯å¢ƒé…ç½®è¯´æ˜

**é‡è¦**: ç”Ÿäº§ç¯å¢ƒé…ç½®å·²è¿ç§»åˆ° Helm Chart ç»Ÿä¸€ç®¡ç†ï¼Œä¸å†ç»´æŠ¤æ‰‹å†™ Docker Compose æ–‡ä»¶ã€‚

**ç”Ÿäº§éƒ¨ç½²æµç¨‹**:
```bash
# 1. ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒ Values
helm template weget-prod ./weget-chart \
  --values ./weget-chart/values-prod.yaml \
  --output-dir ./generated

# 2. éƒ¨ç½²åˆ° Kubernetes
helm upgrade --install weget-prod ./weget-chart \
  --values ./weget-chart/values-prod.yaml \
  --namespace weget-prod \
  --create-namespace

# 3. å¦‚éœ€æœ¬åœ°æµ‹è¯•ï¼Œç”Ÿæˆ Compose æ–‡ä»¶
helm template weget-prod ./weget-chart \
  --values ./weget-chart/values-prod.yaml | \
  yq eval 'select(.kind == "Deployment" or .kind == "Service")' > docker-compose.prod.yml
```

**ç”Ÿäº§ç¯å¢ƒç‰¹æ€§**:
- **é«˜å¯ç”¨**: å¤šå‰¯æœ¬éƒ¨ç½²ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§»
- **èµ„æºé™åˆ¶**: å†…å­˜/CPU é™åˆ¶ï¼Œé˜²æ­¢èµ„æºè€—å°½
- **å¥åº·æ£€æŸ¥**: è‡ªåŠ¨é‡å¯ä¸å¥åº·å®¹å™¨
- **ç›‘æ§é›†æˆ**: Prometheus + Grafana å®Œæ•´ç›‘æ§æ ˆ
- **å®‰å…¨åŠ å›º**: Vault å¯†é’¥ç®¡ç†ï¼Œç½‘ç»œéš”ç¦»

### 14. æç«¯å¯æ‰©å±•æ€§æ¶æ„æ¨¡å¼

#### Rediså¼‚æ­¥å®¢æˆ·ç«¯ç®¡ç†
```python
# core/async_redis.py
import asyncio
import redis.asyncio as redis
from typing import Dict, List, Optional, Any, Union
import json
from datetime import datetime

class AsyncRedisManager:
    """å¼‚æ­¥Redisç®¡ç†å™¨ - è§£å†³é˜»å¡äº‹ä»¶å¾ªç¯é—®é¢˜"""

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.pool = None
        self.client = None
        self._lock = asyncio.Lock()

    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥æ± """
        try:
            # åˆ›å»ºè¿æ¥æ± 
            self.pool = redis.ConnectionPool.from_url(
                self.redis_url,
                max_connections=20,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={},
                health_check_interval=30
            )

            # åˆ›å»ºå®¢æˆ·ç«¯
            self.client = redis.Redis(connection_pool=self.pool)

            # æµ‹è¯•è¿æ¥
            await self.client.ping()
            print("Redis async client initialized successfully")

        except Exception as e:
            print(f"Failed to initialize Redis async client: {e}")
            raise

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            await self.client.close()
        if self.pool:
            await self.pool.disconnect()

    async def get(self, key: str) -> Optional[str]:
        """è·å–å€¼"""
        try:
            value = await self.client.get(key)
            return value.decode() if value else None
        except Exception as e:
            print(f"Redis GET error for key {key}: {e}")
            return None

    async def set(self, key: str, value: str, ex: int = None) -> bool:
        """è®¾ç½®å€¼"""
        try:
            return await self.client.set(key, value, ex=ex)
        except Exception as e:
            print(f"Redis SET error for key {key}: {e}")
            return False

    async def hget(self, name: str, key: str) -> Optional[str]:
        """è·å–å“ˆå¸Œå­—æ®µå€¼"""
        try:
            value = await self.client.hget(name, key)
            return value.decode() if value else None
        except Exception as e:
            print(f"Redis HGET error for {name}:{key}: {e}")
            return None

    async def hset(self, name: str, key: str, value: str) -> bool:
        """è®¾ç½®å“ˆå¸Œå­—æ®µå€¼"""
        try:
            return await self.client.hset(name, key, value)
        except Exception as e:
            print(f"Redis HSET error for {name}:{key}: {e}")
            return False

    async def hgetall(self, name: str) -> Dict[str, str]:
        """è·å–æ‰€æœ‰å“ˆå¸Œå­—æ®µ"""
        try:
            result = await self.client.hgetall(name)
            return {k.decode(): v.decode() for k, v in result.items()}
        except Exception as e:
            print(f"Redis HGETALL error for {name}: {e}")
            return {}

    async def lpush(self, name: str, *values) -> int:
        """å·¦æ¨å…¥åˆ—è¡¨"""
        try:
            return await self.client.lpush(name, *values)
        except Exception as e:
            print(f"Redis LPUSH error for {name}: {e}")
            return 0

    async def rpop(self, name: str) -> Optional[str]:
        """å³å¼¹å‡ºåˆ—è¡¨"""
        try:
            value = await self.client.rpop(name)
            return value.decode() if value else None
        except Exception as e:
            print(f"Redis RPOP error for {name}: {e}")
            return None

    async def brpop(self, keys: Union[str, List[str]], timeout: int = 0) -> Optional[tuple]:
        """é˜»å¡å³å¼¹å‡º"""
        try:
            result = await self.client.brpop(keys, timeout=timeout)
            if result:
                key, value = result
                return key.decode(), value.decode()
            return None
        except Exception as e:
            print(f"Redis BRPOP error: {e}")
            return None

    async def sadd(self, name: str, *values) -> int:
        """æ·»åŠ åˆ°é›†åˆ"""
        try:
            return await self.client.sadd(name, *values)
        except Exception as e:
            print(f"Redis SADD error for {name}: {e}")
            return 0

    async def srem(self, name: str, *values) -> int:
        """ä»é›†åˆç§»é™¤"""
        try:
            return await self.client.srem(name, *values)
        except Exception as e:
            print(f"Redis SREM error for {name}: {e}")
            return 0

    async def smembers(self, name: str) -> set:
        """è·å–é›†åˆæˆå‘˜"""
        try:
            result = await self.client.smembers(name)
            return {v.decode() for v in result}
        except Exception as e:
            print(f"Redis SMEMBERS error for {name}: {e}")
            return set()

    async def exists(self, *keys) -> int:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        try:
            return await self.client.exists(*keys)
        except Exception as e:
            print(f"Redis EXISTS error: {e}")
            return 0

    async def delete(self, *keys) -> int:
        """åˆ é™¤é”®"""
        try:
            return await self.client.delete(*keys)
        except Exception as e:
            print(f"Redis DELETE error: {e}")
            return 0

    async def publish(self, channel: str, message: str) -> int:
        """å‘å¸ƒæ¶ˆæ¯"""
        try:
            return await self.client.publish(channel, message)
        except Exception as e:
            print(f"Redis PUBLISH error for channel {channel}: {e}")
            return 0

    async def subscribe(self, *channels) -> 'redis.client.PubSub':
        """è®¢é˜…é¢‘é“"""
        try:
            pubsub = self.client.pubsub()
            await pubsub.subscribe(*channels)
            return pubsub
        except Exception as e:
            print(f"Redis SUBSCRIBE error: {e}")
            return None

    async def xadd(self, name: str, fields: Dict[str, Any], id: str = "*") -> str:
        """æ·»åŠ åˆ°æµ"""
        try:
            # åºåˆ—åŒ–å¤æ‚å¯¹è±¡
            serialized_fields = {}
            for k, v in fields.items():
                if isinstance(v, (dict, list)):
                    serialized_fields[k] = json.dumps(v)
                else:
                    serialized_fields[k] = str(v)

            return await self.client.xadd(name, serialized_fields, id=id)
        except Exception as e:
            print(f"Redis XADD error for stream {name}: {e}")
            return None

    async def xread(self, streams: Dict[str, str], count: int = None, block: int = None) -> List:
        """è¯»å–æµ"""
        try:
            return await self.client.xread(streams, count=count, block=block)
        except Exception as e:
            print(f"Redis XREAD error: {e}")
            return []

    async def xgroup_create(self, name: str, groupname: str, id: str = "0", mkstream: bool = False) -> bool:
        """åˆ›å»ºæ¶ˆè´¹è€…ç»„"""
        try:
            await self.client.xgroup_create(name, groupname, id=id, mkstream=mkstream)
            return True
        except Exception as e:
            if "BUSYGROUP" not in str(e):  # ç»„å·²å­˜åœ¨
                print(f"Redis XGROUP CREATE error: {e}")
            return False

    async def xreadgroup(self, groupname: str, consumername: str, streams: Dict[str, str],
                        count: int = None, block: int = None) -> List:
        """æ¶ˆè´¹è€…ç»„è¯»å–"""
        try:
            return await self.client.xreadgroup(
                groupname, consumername, streams, count=count, block=block
            )
        except Exception as e:
            print(f"Redis XREADGROUP error: {e}")
            return []

    async def xack(self, name: str, groupname: str, *ids) -> int:
        """ç¡®è®¤æ¶ˆæ¯"""
        try:
            return await self.client.xack(name, groupname, *ids)
        except Exception as e:
            print(f"Redis XACK error: {e}")
            return 0

# AsyncRedisClient å·²è¢«ç‰©ç†åˆ é™¤ - è¯·ä½¿ç”¨ get_async_redis() æ›¿ä»£

# å…¨å±€å¼‚æ­¥Rediså®ä¾‹
async_redis_manager = None

async def get_async_redis() -> AsyncRedisManager:
    """è·å–å¼‚æ­¥Rediså®ä¾‹"""
    global async_redis_manager
    if async_redis_manager is None:
        from core.secure_config import settings
        async_redis_manager = AsyncRedisManager(settings.REDIS_URL)
        await async_redis_manager.initialize()
    return async_redis_manager

# å…¼å®¹æ€§åŒ…è£…å™¨ - ä¸ºCeleryä¿ç•™åŒæ­¥å®¢æˆ·ç«¯
class SyncRedisWrapper:
    """åŒæ­¥RedisåŒ…è£…å™¨ - ä»…ç”¨äºCelery backend"""

    def __init__(self, redis_url: str):
        import redis as sync_redis
        self.client = sync_redis.from_url(redis_url)

    def get(self, key: str):
        return self.client.get(key)

    def set(self, key: str, value: str, ex: int = None):
        return self.client.set(key, value, ex=ex)

    def hget(self, name: str, key: str):
        return self.client.hget(name, key)

    def hset(self, name: str, key: str, value: str):
        return self.client.hset(name, key, value)

    # å…¶ä»–åŒæ­¥æ–¹æ³•...

def get_sync_redis() -> SyncRedisWrapper:
    """è·å–åŒæ­¥Rediså®ä¾‹ï¼ˆä»…ç”¨äºCeleryï¼‰"""
    from core.secure_config import settings
    return SyncRedisWrapper(settings.REDIS_URL)
```

#### Playwrightæµè§ˆå™¨æ± åŒ–ç®¡ç†
```python
# core/browser_pool.py
import asyncio
import time
from typing import Dict, List, Optional, Tuple
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from dataclasses import dataclass
from enum import Enum
import redis.asyncio as redis

class BrowserStatus(Enum):
    IDLE = "idle"
    BUSY = "busy"
    ERROR = "error"
    CLOSED = "closed"

@dataclass
class BrowserInstance:
    browser: Browser
    context: BrowserContext
    status: BrowserStatus
    created_at: float
    last_used: float
    usage_count: int
    max_usage: int = 100  # æœ€å¤§ä½¿ç”¨æ¬¡æ•°åé‡å¯

class BrowserPool:
    """æµè§ˆå™¨è¿›ç¨‹æ± ç®¡ç†å™¨ - è§£å†³5000 IP Ã— å¤šè´¦å·å¹¶å‘é—®é¢˜"""

    def __init__(self,
                 min_browsers: int = 5,
                 max_browsers: int = 50,
                 max_pages_per_browser: int = 10,
                 browser_timeout: int = 300,
                 redis_client: Optional['redis.asyncio.Redis'] = None):
        self.min_browsers = min_browsers
        self.max_browsers = max_browsers
        self.max_pages_per_browser = max_pages_per_browser
        self.browser_timeout = browser_timeout
        self.redis = redis_client  # ç°åœ¨æ˜¯å¼‚æ­¥Rediså®¢æˆ·ç«¯

        self.browsers: Dict[str, BrowserInstance] = {}
        self.page_assignments: Dict[str, str] = {}  # page_id -> browser_id
        self.playwright = None
        self.lock = asyncio.Lock()

    async def start(self):
        """å¯åŠ¨æµè§ˆå™¨æ± """
        self.playwright = await async_playwright().start()

        # é¢„åˆ›å»ºæœ€å°æ•°é‡çš„æµè§ˆå™¨
        for i in range(self.min_browsers):
            await self.create_browser()

        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        asyncio.create_task(self.cleanup_task())

    async def stop(self):
        """åœæ­¢æµè§ˆå™¨æ± """
        for browser_instance in self.browsers.values():
            try:
                await browser_instance.browser.close()
            except:
                pass

        if self.playwright:
            await self.playwright.stop()

    async def get_page(self, user_agent: str = None, proxy: Dict = None) -> Tuple[Page, str]:
        """è·å–å¯ç”¨é¡µé¢"""
        async with self.lock:
            # æŸ¥æ‰¾å¯ç”¨çš„æµè§ˆå™¨
            browser_id = await self.find_available_browser()

            if not browser_id:
                # åˆ›å»ºæ–°æµè§ˆå™¨
                browser_id = await self.create_browser()

            browser_instance = self.browsers[browser_id]

            # åˆ›å»ºæ–°é¡µé¢
            page = await browser_instance.context.new_page()
            page_id = f"page_{int(time.time() * 1000)}_{id(page)}"

            # è®¾ç½®ç”¨æˆ·ä»£ç†
            if user_agent:
                await page.set_extra_http_headers({'User-Agent': user_agent})

            # è®°å½•é¡µé¢åˆ†é…
            self.page_assignments[page_id] = browser_id
            browser_instance.status = BrowserStatus.BUSY
            browser_instance.last_used = time.time()
            browser_instance.usage_count += 1

            # æ›´æ–°Redisç»Ÿè®¡ - ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
            if self.redis:
                await self.redis.hincrby('browser_stats', 'pages_created', 1)
                await self.redis.hset('browser_stats', 'active_browsers', len(self.browsers))

            return page, page_id

    async def release_page(self, page: Page, page_id: str):
        """é‡Šæ”¾é¡µé¢"""
        try:
            await page.close()
        except:
            pass

        async with self.lock:
            browser_id = self.page_assignments.pop(page_id, None)
            if browser_id and browser_id in self.browsers:
                browser_instance = self.browsers[browser_id]

                # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦è¿˜æœ‰å…¶ä»–é¡µé¢
                remaining_pages = sum(1 for pid, bid in self.page_assignments.items() if bid == browser_id)

                if remaining_pages == 0:
                    browser_instance.status = BrowserStatus.IDLE

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯æµè§ˆå™¨
                if browser_instance.usage_count >= browser_instance.max_usage:
                    await self.restart_browser(browser_id)

    async def find_available_browser(self) -> Optional[str]:
        """æŸ¥æ‰¾å¯ç”¨çš„æµè§ˆå™¨"""
        for browser_id, browser_instance in self.browsers.items():
            if browser_instance.status == BrowserStatus.IDLE:
                return browser_id

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å…±äº«ï¼ˆæœªè¾¾åˆ°æœ€å¤§é¡µé¢æ•°ï¼‰
            if browser_instance.status == BrowserStatus.BUSY:
                current_pages = sum(1 for pid, bid in self.page_assignments.items() if bid == browser_id)
                if current_pages < self.max_pages_per_browser:
                    return browser_id

        return None

    async def create_browser(self) -> str:
        """åˆ›å»ºæ–°æµè§ˆå™¨å®ä¾‹"""
        if len(self.browsers) >= self.max_browsers:
            raise Exception("Browser pool is full")

        browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--no-first-run',
                '--no-zygote',
                '--disable-gpu',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding'
            ]
        )

        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )

        browser_id = f"browser_{int(time.time() * 1000)}_{id(browser)}"

        self.browsers[browser_id] = BrowserInstance(
            browser=browser,
            context=context,
            status=BrowserStatus.IDLE,
            created_at=time.time(),
            last_used=time.time(),
            usage_count=0
        )

        return browser_id

    async def restart_browser(self, browser_id: str):
        """é‡å¯æµè§ˆå™¨å®ä¾‹"""
        if browser_id not in self.browsers:
            return

        browser_instance = self.browsers[browser_id]

        try:
            await browser_instance.browser.close()
        except:
            pass

        # ç§»é™¤æ—§å®ä¾‹
        del self.browsers[browser_id]

        # åˆ›å»ºæ–°å®ä¾‹
        await self.create_browser()

    async def cleanup_task(self):
        """æ¸…ç†ä»»åŠ¡ - å®šæœŸæ¸…ç†è¶…æ—¶å’Œè¿‡æœŸçš„æµè§ˆå™¨"""
        while True:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

                current_time = time.time()
                browsers_to_remove = []

                async with self.lock:
                    for browser_id, browser_instance in self.browsers.items():
                        # æ£€æŸ¥è¶…æ—¶
                        if (current_time - browser_instance.last_used) > self.browser_timeout:
                            browsers_to_remove.append(browser_id)

                        # æ£€æŸ¥é”™è¯¯çŠ¶æ€
                        elif browser_instance.status == BrowserStatus.ERROR:
                            browsers_to_remove.append(browser_id)

                    # ç§»é™¤è¶…æ—¶çš„æµè§ˆå™¨
                    for browser_id in browsers_to_remove:
                        await self.restart_browser(browser_id)

                    # ç¡®ä¿æœ€å°æ•°é‡
                    while len(self.browsers) < self.min_browsers:
                        await self.create_browser()

            except Exception as e:
                print(f"Browser cleanup error: {e}")

class BrowserlessPool:
    """BrowserlessæœåŠ¡æ± åŒ–ç®¡ç† - ä¼ä¸šçº§æµè§ˆå™¨æœåŠ¡"""

    def __init__(self, browserless_endpoints: List[str], max_concurrent: int = 50):
        self.endpoints = browserless_endpoints
        self.max_concurrent = max_concurrent
        self.current_endpoint_index = 0
        self.endpoint_stats = {endpoint: {"active": 0, "total": 0, "errors": 0}
                              for endpoint in browserless_endpoints}
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def get_page(self, options: Dict = None) -> tuple:
        """è·å–é¡µé¢å’Œä¼šè¯ä¿¡æ¯"""
        async with self.semaphore:
            endpoint = self._get_best_endpoint()

            try:
                self.endpoint_stats[endpoint]["active"] += 1
                self.endpoint_stats[endpoint]["total"] += 1

                # è¿æ¥åˆ°BrowserlessæœåŠ¡
                from playwright.async_api import async_playwright

                playwright = await async_playwright().start()

                # ä½¿ç”¨CDPè¿æ¥
                browser = await playwright.chromium.connect_over_cdp(endpoint)
                context = await browser.new_context(**(options or {}))
                page = await context.new_page()

                session_info = {
                    "endpoint": endpoint,
                    "browser": browser,
                    "context": context,
                    "playwright": playwright
                }

                return page, session_info

            except Exception as e:
                self.endpoint_stats[endpoint]["errors"] += 1
                self.endpoint_stats[endpoint]["active"] -= 1
                print(f"Failed to get page from {endpoint}: {e}")
                raise

    async def release_page(self, page, session_info: Dict):
        """é‡Šæ”¾é¡µé¢å’Œä¼šè¯"""
        try:
            endpoint = session_info["endpoint"]

            # å…³é—­é¡µé¢å’Œä¸Šä¸‹æ–‡
            await page.close()
            await session_info["context"].close()
            await session_info["browser"].close()
            await session_info["playwright"].stop()

            # æ›´æ–°ç»Ÿè®¡
            self.endpoint_stats[endpoint]["active"] -= 1

        except Exception as e:
            print(f"Error releasing page: {e}")

    def _get_best_endpoint(self) -> str:
        """è·å–æœ€ä½³ç«¯ç‚¹ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰"""
        # ç®€å•è½®è¯¢ç­–ç•¥
        endpoint = self.endpoints[self.current_endpoint_index]
        self.current_endpoint_index = (self.current_endpoint_index + 1) % len(self.endpoints)

        # æ£€æŸ¥ç«¯ç‚¹å¥åº·çŠ¶æ€
        stats = self.endpoint_stats[endpoint]
        error_rate = stats["errors"] / max(stats["total"], 1)

        # å¦‚æœé”™è¯¯ç‡è¿‡é«˜ï¼Œè·³è¿‡æ­¤ç«¯ç‚¹
        if error_rate > 0.5 and stats["total"] > 10:
            return self._get_best_endpoint()

        return endpoint

    def get_stats(self) -> Dict:
        """è·å–æ± åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        total_active = sum(stats["active"] for stats in self.endpoint_stats.values())
        total_requests = sum(stats["total"] for stats in self.endpoint_stats.values())
        total_errors = sum(stats["errors"] for stats in self.endpoint_stats.values())

        return {
            "total_active_sessions": total_active,
            "total_requests": total_requests,
            "total_errors": total_errors,
            "error_rate": total_errors / max(total_requests, 1),
            "endpoint_stats": self.endpoint_stats
        }

class HybridBrowserManager:
    """æ··åˆæµè§ˆå™¨ç®¡ç†å™¨ - æœ¬åœ°æ±  + BrowserlessæœåŠ¡"""

    def __init__(self, local_pool: BrowserPool = None, browserless_pool: BrowserlessPool = None):
        self.local_pool = local_pool
        self.browserless_pool = browserless_pool
        self.prefer_browserless = True  # ä¼˜å…ˆä½¿ç”¨BrowserlessæœåŠ¡

    async def get_page(self, user_agent: str = None, proxy: Dict = None,
                      force_local: bool = False) -> tuple:
        """æ™ºèƒ½è·å–é¡µé¢ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æœåŠ¡"""

        # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ± 
        if force_local and self.local_pool:
            page, page_id = await self.local_pool.get_page(user_agent, proxy)
            return page, {"type": "local", "page_id": page_id}

        # ä¼˜å…ˆä½¿ç”¨BrowserlessæœåŠ¡
        if self.prefer_browserless and self.browserless_pool:
            try:
                options = {}
                if user_agent:
                    options["user_agent"] = user_agent
                if proxy:
                    options["proxy"] = proxy

                page, session_info = await self.browserless_pool.get_page(options)
                session_info["type"] = "browserless"
                return page, session_info

            except Exception as e:
                print(f"Browserless failed, falling back to local pool: {e}")

        # å›é€€åˆ°æœ¬åœ°æ± 
        if self.local_pool:
            page, page_id = await self.local_pool.get_page(user_agent, proxy)
            return page, {"type": "local", "page_id": page_id}

        raise Exception("No browser service available")

    async def release_page(self, page, session_info: Dict):
        """é‡Šæ”¾é¡µé¢"""
        if session_info["type"] == "browserless":
            await self.browserless_pool.release_page(page, session_info)
        elif session_info["type"] == "local":
            await self.local_pool.release_page(page, session_info["page_id"])

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {"local_pool": None, "browserless_pool": None}

        if self.local_pool:
            stats["local_pool"] = {
                "active_browsers": len(self.local_pool.browsers),
                "active_pages": len(self.local_pool.page_assignments)
            }

        if self.browserless_pool:
            stats["browserless_pool"] = self.browserless_pool.get_stats()

        return stats

# é›†æˆåˆ°åŸºç¡€é‡‡é›†å™¨
class OptimizedTwitterBaseScraper:
    """ä¼˜åŒ–çš„åŸºç¡€é‡‡é›†å™¨ - ä½¿ç”¨æµè§ˆå™¨æ± """

    def __init__(self, browser_pool: BrowserPool, redis_client: 'redis.asyncio.Redis'):
        self.browser_pool = browser_pool
        self.redis = redis_client  # ç°åœ¨æ˜¯å¼‚æ­¥Rediså®¢æˆ·ç«¯
        self.current_page = None
        self.current_page_id = None

    async def get_page(self, user_agent: str = None, proxy: Dict = None) -> Page:
        """è·å–é¡µé¢å®ä¾‹"""
        if self.current_page:
            await self.release_page()

        self.current_page, self.current_page_id = await self.browser_pool.get_page(user_agent, proxy)
        return self.current_page

    async def release_page(self):
        """é‡Šæ”¾é¡µé¢å®ä¾‹"""
        if self.current_page and self.current_page_id:
            await self.browser_pool.release_page(self.current_page, self.current_page_id)
            self.current_page = None
            self.current_page_id = None

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.release_page()
```

#### ç»Ÿä¸€å¼‚æ­¥Redisç®¡ç†å™¨
```python
# core/redis_manager.py
import redis.asyncio as redis
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
import asyncio
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class AsyncRedisManager:
    """ç»Ÿä¸€çš„å¼‚æ­¥Redisç®¡ç†å™¨ - åˆå¹¶æ‰€æœ‰Redisæ“ä½œ"""

    def __init__(self,
                 host: str = 'localhost',
                 port: int = 6379,
                 db: int = 0,
                 password: Optional[str] = None,
                 max_connections: int = 20,
                 retry_on_timeout: bool = True,
                 health_check_interval: int = 30):

        self.connection_params = {
            'host': host,
            'port': port,
            'db': db,
            'password': password,
            'decode_responses': True,
            'max_connections': max_connections,
            'retry_on_timeout': retry_on_timeout,
            'health_check_interval': health_check_interval,
            'socket_connect_timeout': 5,
            'socket_timeout': 5
        }

        self._client: Optional[redis.Redis] = None
        self._connection_pool: Optional[redis.ConnectionPool] = None
        self._lock = asyncio.Lock()

    async def connect(self) -> redis.Redis:
        """å»ºç«‹Redisè¿æ¥"""
        if self._client is None:
            async with self._lock:
                if self._client is None:
                    self._connection_pool = redis.ConnectionPool(**self.connection_params)
                    self._client = redis.Redis(connection_pool=self._connection_pool)

                    # æµ‹è¯•è¿æ¥
                    await self._client.ping()
                    logger.info("Redis connection established successfully")

        return self._client

    async def disconnect(self):
        """å…³é—­Redisè¿æ¥"""
        if self._client:
            await self._client.close()
            self._client = None
            self._connection_pool = None
            logger.info("Redis connection closed")

    @asynccontextmanager
    async def get_client(self):
        """è·å–Rediså®¢æˆ·ç«¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        client = await self.connect()
        try:
            yield client
        except Exception as e:
            logger.error(f"Redis operation error: {str(e)}")
            raise

    # ==================== åŸºç¡€æ“ä½œ ====================

    async def get(self, key: str) -> Optional[str]:
        """è·å–é”®å€¼"""
        async with self.get_client() as client:
            return await client.get(key)

    async def set(self, key: str, value: str, ex: Optional[int] = None) -> bool:
        """è®¾ç½®é”®å€¼"""
        async with self.get_client() as client:
            return await client.set(key, value, ex=ex)

    async def setex(self, key: str, time: int, value: str) -> bool:
        """è®¾ç½®å¸¦è¿‡æœŸæ—¶é—´çš„é”®å€¼"""
        async with self.get_client() as client:
            return await client.setex(key, time, value)

    async def delete(self, *keys: str) -> int:
        """åˆ é™¤é”®"""
        async with self.get_client() as client:
            return await client.delete(*keys)

    async def exists(self, *keys: str) -> int:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        async with self.get_client() as client:
            return await client.exists(*keys)

    async def expire(self, key: str, time: int) -> bool:
        """è®¾ç½®é”®è¿‡æœŸæ—¶é—´"""
        async with self.get_client() as client:
            return await client.expire(key, time)

    # ==================== å“ˆå¸Œæ“ä½œ ====================

    async def hget(self, name: str, key: str) -> Optional[str]:
        """è·å–å“ˆå¸Œå­—æ®µå€¼"""
        async with self.get_client() as client:
            return await client.hget(name, key)

    async def hset(self, name: str, key: str, value: str) -> int:
        """è®¾ç½®å“ˆå¸Œå­—æ®µå€¼"""
        async with self.get_client() as client:
            return await client.hset(name, key, value)

    async def hgetall(self, name: str) -> Dict[str, str]:
        """è·å–æ‰€æœ‰å“ˆå¸Œå­—æ®µ"""
        async with self.get_client() as client:
            return await client.hgetall(name)

    async def hincrby(self, name: str, key: str, amount: int = 1) -> int:
        """å“ˆå¸Œå­—æ®µè‡ªå¢"""
        async with self.get_client() as client:
            return await client.hincrby(name, key, amount)

    async def hdel(self, name: str, *keys: str) -> int:
        """åˆ é™¤å“ˆå¸Œå­—æ®µ"""
        async with self.get_client() as client:
            return await client.hdel(name, *keys)

    # ==================== é›†åˆæ“ä½œ ====================

    async def sadd(self, name: str, *values: str) -> int:
        """æ·»åŠ é›†åˆæˆå‘˜"""
        async with self.get_client() as client:
            return await client.sadd(name, *values)

    async def srem(self, name: str, *values: str) -> int:
        """ç§»é™¤é›†åˆæˆå‘˜"""
        async with self.get_client() as client:
            return await client.srem(name, *values)

    async def smembers(self, name: str) -> set:
        """è·å–é›†åˆæ‰€æœ‰æˆå‘˜"""
        async with self.get_client() as client:
            return await client.smembers(name)

    async def sismember(self, name: str, value: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé›†åˆæˆå‘˜"""
        async with self.get_client() as client:
            return await client.sismember(name, value)

    # ==================== åˆ—è¡¨æ“ä½œ ====================

    async def lpush(self, name: str, *values: str) -> int:
        """å·¦ä¾§æ¨å…¥åˆ—è¡¨"""
        async with self.get_client() as client:
            return await client.lpush(name, *values)

    async def rpush(self, name: str, *values: str) -> int:
        """å³ä¾§æ¨å…¥åˆ—è¡¨"""
        async with self.get_client() as client:
            return await client.rpush(name, *values)

    async def lpop(self, name: str) -> Optional[str]:
        """å·¦ä¾§å¼¹å‡ºåˆ—è¡¨å…ƒç´ """
        async with self.get_client() as client:
            return await client.lpop(name)

    async def rpop(self, name: str) -> Optional[str]:
        """å³ä¾§å¼¹å‡ºåˆ—è¡¨å…ƒç´ """
        async with self.get_client() as client:
            return await client.rpop(name)

    async def llen(self, name: str) -> int:
        """è·å–åˆ—è¡¨é•¿åº¦"""
        async with self.get_client() as client:
            return await client.llen(name)

    # ==================== æµæ“ä½œ ====================

    async def xadd(self, name: str, fields: Dict[str, str], id: str = "*") -> str:
        """æ·»åŠ æµæ¶ˆæ¯"""
        async with self.get_client() as client:
            return await client.xadd(name, fields, id=id)

    async def xread(self, streams: Dict[str, str], count: Optional[int] = None,
                   block: Optional[int] = None) -> List:
        """è¯»å–æµæ¶ˆæ¯"""
        async with self.get_client() as client:
            return await client.xread(streams, count=count, block=block)

    async def xlen(self, name: str) -> int:
        """è·å–æµé•¿åº¦"""
        async with self.get_client() as client:
            return await client.xlen(name)

    # ==================== æ‰«ææ“ä½œ ====================

    async def scan_iter(self, match: Optional[str] = None, count: int = 1000):
        """å¼‚æ­¥æ‰«æé”®"""
        async with self.get_client() as client:
            async for key in client.scan_iter(match=match, count=count):
                yield key

    # ==================== ç®¡é“æ“ä½œ ====================

    @asynccontextmanager
    async def pipeline(self):
        """è·å–ç®¡é“ä¸Šä¸‹æ–‡"""
        async with self.get_client() as client:
            async with client.pipeline() as pipe:
                yield pipe

    # ==================== é«˜çº§æ“ä½œ ====================

    async def get_json(self, key: str) -> Optional[Dict]:
        """è·å–JSONæ ¼å¼çš„å€¼"""
        value = await self.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                logger.warning(f"Failed to decode JSON for key: {key}")
                return None
        return None

    async def set_json(self, key: str, value: Dict, ex: Optional[int] = None) -> bool:
        """è®¾ç½®JSONæ ¼å¼çš„å€¼"""
        try:
            json_str = json.dumps(value, ensure_ascii=False)
            return await self.set(key, json_str, ex=ex)
        except (TypeError, ValueError) as e:
            logger.error(f"Failed to encode JSON for key {key}: {str(e)}")
            return False

    async def increment_with_expiry(self, key: str, amount: int = 1,
                                  expiry: int = 3600) -> int:
        """å¸¦è¿‡æœŸæ—¶é—´çš„è®¡æ•°å™¨è‡ªå¢"""
        async with self.pipeline() as pipe:
            await pipe.incr(key, amount)
            await pipe.expire(key, expiry)
            results = await pipe.execute()
            return results[0]

    async def get_or_set(self, key: str, factory_func, ex: Optional[int] = None):
        """è·å–æˆ–è®¾ç½®ï¼ˆç¼“å­˜æ¨¡å¼ï¼‰"""
        value = await self.get(key)
        if value is None:
            if asyncio.iscoroutinefunction(factory_func):
                value = await factory_func()
            else:
                value = factory_func()

            if value is not None:
                if isinstance(value, (dict, list)):
                    await self.set_json(key, value, ex=ex)
                else:
                    await self.set(key, str(value), ex=ex)

        return value

    # ==================== å¥åº·æ£€æŸ¥ ====================

    async def health_check(self) -> Dict[str, Any]:
        """Rediså¥åº·æ£€æŸ¥"""
        try:
            async with self.get_client() as client:
                start_time = datetime.utcnow()
                await client.ping()
                latency = (datetime.utcnow() - start_time).total_seconds() * 1000

                info = await client.info()

                return {
                    'status': 'healthy',
                    'latency_ms': round(latency, 2),
                    'connected_clients': info.get('connected_clients', 0),
                    'used_memory_human': info.get('used_memory_human', 'unknown'),
                    'redis_version': info.get('redis_version', 'unknown')
                }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }

# å…¨å±€Redisç®¡ç†å™¨å®ä¾‹
_redis_manager: Optional[AsyncRedisManager] = None

async def get_async_redis() -> AsyncRedisManager:
    """è·å–å…¨å±€å¼‚æ­¥Redisç®¡ç†å™¨"""
    global _redis_manager
    if _redis_manager is None:
        import os
        _redis_manager = AsyncRedisManager(
            host=os.getenv('REDIS_HOST', 'localhost'),
            port=int(os.getenv('REDIS_PORT', 6379)),
            password=os.getenv('REDIS_PASSWORD'),
            max_connections=int(os.getenv('REDIS_MAX_CONNECTIONS', 20))
        )
    return _redis_manager

async def close_redis():
    """å…³é—­å…¨å±€Redisè¿æ¥"""
    global _redis_manager
    if _redis_manager:
        await _redis_manager.disconnect()
        _redis_manager = None
```

#### è§£è€¦æ•°æ®æ‘„å–ç®¡é“
```python
# core/data_pipeline.py
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Optional
from redis.asyncio import Redis
import uuid

class DataIngestionPipeline:
    """è§£è€¦çš„æ•°æ®æ‘„å–ç®¡é“ - å¸¦å†™å…¥é™é€Ÿå’Œæ‰¹é‡å¤„ç†"""

    def __init__(self, redis_client: Redis, mongodb_client=None):
        self.redis = redis_client
        self.mongodb = mongodb_client
        self.ingestion_stream = "weget:ingestion:stream"
        self.processing_stream = "weget:processing:stream"
        self.dlq_stream = "weget:dlq:stream"

        # å†™å…¥é™é€Ÿé…ç½®
        self.batch_size = 1000  # æ‰¹é‡å†™å…¥å¤§å°
        self.batch_timeout = 5  # æ‰¹é‡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.max_write_rate = 10000  # æ¯ç§’æœ€å¤§å†™å…¥æ•°
        self.write_concern = "majority"  # MongoDBå†™å…¥å…³æ³¨çº§åˆ«

        # æ‰¹é‡ç¼“å†²åŒº
        self.batch_buffer = []
        self.last_flush_time = datetime.utcnow()
        self.write_semaphore = asyncio.Semaphore(100)  # é™åˆ¶å¹¶å‘å†™å…¥

    async def enqueue_raw_data(self, data_type: str, raw_data: Dict, source_info: Dict) -> str:
        """å°†åŸå§‹æ•°æ®åŠ å…¥æ‘„å–é˜Ÿåˆ—"""
        message_id = str(uuid.uuid4())

        ingestion_message = {
            'message_id': message_id,
            'data_type': data_type,  # 'user', 'tweet', 'reply'
            'raw_data': json.dumps(raw_data),
            'source_info': json.dumps(source_info),
            'ingested_at': datetime.utcnow().isoformat(),
            'status': 'pending_validation'
        }

        # ä½¿ç”¨Redis Streamsç¡®ä¿æ•°æ®æŒä¹…åŒ–
        await self.redis.xadd(
            self.ingestion_stream,
            ingestion_message,
            maxlen=1000000  # ä¿ç•™æœ€è¿‘100ä¸‡æ¡è®°å½•
        )

        return message_id

    async def add_to_batch(self, data: Dict):
        """æ·»åŠ æ•°æ®åˆ°æ‰¹é‡ç¼“å†²åŒº"""
        self.batch_buffer.append(data)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
        current_time = datetime.utcnow()
        time_diff = (current_time - self.last_flush_time).total_seconds()

        if (len(self.batch_buffer) >= self.batch_size or
            time_diff >= self.batch_timeout):
            await self.flush_batch()

    async def flush_batch(self):
        """æ‰¹é‡åˆ·æ–°æ•°æ®åˆ°MongoDB"""
        if not self.batch_buffer or not self.mongodb:
            return

        async with self.write_semaphore:
            try:
                # æŒ‰æ•°æ®ç±»å‹åˆ†ç»„
                grouped_data = {}
                for item in self.batch_buffer:
                    data_type = item.get('data_type', 'unknown')
                    if data_type not in grouped_data:
                        grouped_data[data_type] = []
                    grouped_data[data_type].append(item)

                # æ‰¹é‡å†™å…¥ä¸åŒé›†åˆ
                for data_type, items in grouped_data.items():
                    collection_name = f"weget_{data_type}s"
                    collection = self.mongodb[collection_name]

                    # ä½¿ç”¨writeConcern=majorityç¡®ä¿æ•°æ®å®‰å…¨
                    await collection.insert_many(
                        items,
                        ordered=False,  # å…è®¸éƒ¨åˆ†å¤±è´¥
                        write_concern={'w': 'majority', 'j': True}
                    )

                # æ›´æ–°ç»Ÿè®¡
                await self.redis.hincrby('pipeline_stats', 'batch_writes', 1)
                await self.redis.hincrby('pipeline_stats', 'documents_written', len(self.batch_buffer))

                print(f"Flushed batch of {len(self.batch_buffer)} documents")

            except Exception as e:
                print(f"Batch flush error: {e}")
                # å°†å¤±è´¥çš„æ•°æ®å‘é€åˆ°DLQ
                for item in self.batch_buffer:
                    await self.send_to_dlq(item, str(e))
            finally:
                self.batch_buffer.clear()
                self.last_flush_time = datetime.utcnow()

    async def send_to_dlq(self, data: Dict, error_msg: str):
        """å‘é€å¤±è´¥æ•°æ®åˆ°æ­»ä¿¡é˜Ÿåˆ—"""
        dlq_message = {
            'original_data': json.dumps(data),
            'error_message': error_msg,
            'failed_at': datetime.utcnow().isoformat(),
            'retry_count': data.get('retry_count', 0) + 1
        }

        await self.redis.xadd(self.dlq_stream, dlq_message)

    async def process_ingestion_queue(self, consumer_group: str, consumer_name: str):
        """å¤„ç†æ‘„å–é˜Ÿåˆ—ä¸­çš„æ•°æ®"""
        try:
            # åˆ›å»ºæ¶ˆè´¹è€…ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                await self.redis.xgroup_create(
                    self.ingestion_stream,
                    consumer_group,
                    id='0',
                    mkstream=True
                )
            except:
                pass  # ç»„å·²å­˜åœ¨

            while True:
                # è¯»å–æ¶ˆæ¯
                messages = await self.redis.xreadgroup(
                    consumer_group,
                    consumer_name,
                    {self.ingestion_stream: '>'},
                    count=10,
                    block=1000
                )

                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        await self._process_single_message(msg_id, fields, consumer_group)

        except Exception as e:
            logger.error(f"Ingestion processing error: {str(e)}")

    async def _process_single_message(self, msg_id: bytes, fields: Dict, consumer_group: str):
        """å¤„ç†å•æ¡æ¶ˆæ¯"""
        try:
            message_data = {k.decode(): v.decode() for k, v in fields.items()}

            data_type = message_data['data_type']
            raw_data = json.loads(message_data['raw_data'])
            source_info = json.loads(message_data['source_info'])

            # æ•°æ®éªŒè¯
            validated_data = await self._validate_data(data_type, raw_data)

            if validated_data:
                # éªŒè¯æˆåŠŸï¼Œå‘é€åˆ°å¤„ç†é˜Ÿåˆ—
                await self._enqueue_for_processing(
                    message_data['message_id'],
                    data_type,
                    validated_data,
                    source_info
                )

                # ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
                await self.redis.xack(self.ingestion_stream, consumer_group, msg_id)

            else:
                # éªŒè¯å¤±è´¥ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                await self._send_to_dlq(message_data, "Validation failed")
                await self.redis.xack(self.ingestion_stream, consumer_group, msg_id)

        except Exception as e:
            logger.error(f"Message processing error: {str(e)}")
            # ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œè®©å®ƒé‡æ–°è¢«å¤„ç†

    async def _validate_data(self, data_type: str, raw_data: Dict) -> Optional[Dict]:
        """éªŒè¯æ•°æ®"""
        try:
            if data_type == 'user':
                user_model = TwitterUserModel(**self._extract_user_data(raw_data))
                return user_model.dict()
            elif data_type == 'tweet':
                tweet_model = TwitterTweetModel(**self._extract_tweet_data(raw_data))
                return tweet_model.dict()
            elif data_type == 'reply':
                reply_model = TwitterReplyModel(**self._extract_reply_data(raw_data))
                return reply_model.dict()
            else:
                return None

        except Exception as e:
            logger.warning(f"Data validation failed for {data_type}: {str(e)}")
            return None

    async def _enqueue_for_processing(self, message_id: str, data_type: str, validated_data: Dict, source_info: Dict):
        """å°†éªŒè¯åçš„æ•°æ®åŠ å…¥å¤„ç†é˜Ÿåˆ—"""
        processing_message = {
            'original_message_id': message_id,
            'data_type': data_type,
            'validated_data': json.dumps(validated_data),
            'source_info': json.dumps(source_info),
            'validated_at': datetime.utcnow().isoformat(),
            'status': 'pending_storage'
        }

        await self.redis.xadd(self.processing_stream, processing_message)

    async def _send_to_dlq(self, original_message: Dict, error_reason: str):
        """å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—"""
        dlq_message = original_message.copy()
        dlq_message.update({
            'error_reason': error_reason,
            'failed_at': datetime.utcnow().isoformat(),
            'retry_count': 0
        })

        await self.redis.xadd(self.dlq_stream, dlq_message)
        logger.warning(f"Message sent to DLQ: {original_message.get('message_id', 'unknown')}")

# ä¿®æ”¹åçš„é‡‡é›†å™¨åŸºç±»
class DecoupledTwitterScraper(TwitterBaseScraper):
    """è§£è€¦çš„Twitteré‡‡é›†å™¨"""

    def __init__(self, cookie_manager, proxy_manager, data_pipeline):
        super().__init__(cookie_manager, proxy_manager)
        self.data_pipeline = data_pipeline

    async def save_scraped_data(self, data_type: str, raw_data: Dict, source_info: Dict) -> str:
        """ä¿å­˜é‡‡é›†çš„æ•°æ®åˆ°æ‘„å–ç®¡é“"""
        return await self.data_pipeline.enqueue_raw_data(data_type, raw_data, source_info)

# æ–°çš„Celeryä»»åŠ¡ï¼šæ•°æ®å¤„ç†å™¨
@app.task(bind=True, max_retries=3)
def data_processing_task(self, consumer_group: str = "processors", consumer_name: str = None):
    """æ•°æ®å¤„ç†ä»»åŠ¡"""
    if not consumer_name:
        consumer_name = f"processor_{self.request.id}"

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        redis_async = Redis.from_url(REDIS_URL)
        pipeline = DataIngestionPipeline(redis_async)

        # å¤„ç†æ‘„å–é˜Ÿåˆ—
        loop.run_until_complete(
            pipeline.process_ingestion_queue(consumer_group, consumer_name)
        )

    except Exception as e:
        logger.error(f"Data processing task failed: {str(e)}")
        raise

@app.task(bind=True, max_retries=3)
def data_storage_task(self, consumer_group: str = "storage", consumer_name: str = None):
    """æ•°æ®å­˜å‚¨ä»»åŠ¡"""
    if not consumer_name:
        consumer_name = f"storage_{self.request.id}"

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        redis_async = Redis.from_url(REDIS_URL)
        storage_processor = DataStorageProcessor(redis_async)

        # å¤„ç†å­˜å‚¨é˜Ÿåˆ—
        loop.run_until_complete(
            storage_processor.process_storage_queue(consumer_group, consumer_name)
        )

    except Exception as e:
        logger.error(f"Data storage task failed: {str(e)}")
        raise

class DataStorageProcessor:
    """æ•°æ®å­˜å‚¨å¤„ç†å™¨"""

    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.processing_stream = "weget:processing:stream"
        self.data_manager = EnhancedDataManager(mongo_client, neo4j_driver, redis_client)

    async def process_storage_queue(self, consumer_group: str, consumer_name: str):
        """å¤„ç†å­˜å‚¨é˜Ÿåˆ—"""
        try:
            # åˆ›å»ºæ¶ˆè´¹è€…ç»„
            try:
                await self.redis.xgroup_create(
                    self.processing_stream,
                    consumer_group,
                    id='0',
                    mkstream=True
                )
            except:
                pass

            while True:
                messages = await self.redis.xreadgroup(
                    consumer_group,
                    consumer_name,
                    {self.processing_stream: '>'},
                    count=5,
                    block=1000
                )

                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        await self._store_single_message(msg_id, fields, consumer_group)

        except Exception as e:
            logger.error(f"Storage processing error: {str(e)}")

    async def _store_single_message(self, msg_id: bytes, fields: Dict, consumer_group: str):
        """å­˜å‚¨å•æ¡æ¶ˆæ¯"""
        try:
            message_data = {k.decode(): v.decode() for k, v in fields.items()}

            data_type = message_data['data_type']
            validated_data = json.loads(message_data['validated_data'])

            # å­˜å‚¨åˆ°ç›¸åº”çš„æ•°æ®åº“
            if data_type == 'user':
                await self.data_manager.save_user_to_mongo(validated_data)
            elif data_type == 'tweet':
                await self.data_manager.save_tweet_to_mongo(validated_data)
            elif data_type == 'reply':
                await self.data_manager.save_reply_to_mongo(validated_data)

            # ç¡®è®¤æ¶ˆæ¯
            await self.redis.xack(self.processing_stream, consumer_group, msg_id)

        except Exception as e:
            logger.error(f"Storage error: {str(e)}")
            # ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œè®©å®ƒé‡æ–°å¤„ç†
```

#### æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†
```python
# core/dlq_manager.py
class DeadLetterQueueManager:
    """æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†å™¨"""

    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.dlq_stream = "weget:dlq:stream"

    async def get_dlq_stats(self) -> Dict:
        """è·å–æ­»ä¿¡é˜Ÿåˆ—ç»Ÿè®¡"""
        try:
            stream_info = await self.redis.xinfo_stream(self.dlq_stream)

            # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„ç»Ÿè®¡
            error_stats = {}

            # è¯»å–æœ€è¿‘çš„æ¶ˆæ¯è¿›è¡Œåˆ†æ
            messages = await self.redis.xrevrange(self.dlq_stream, count=1000)

            for msg_id, fields in messages:
                message_data = {k.decode(): v.decode() for k, v in fields.items()}
                error_reason = message_data.get('error_reason', 'unknown')

                if error_reason not in error_stats:
                    error_stats[error_reason] = 0
                error_stats[error_reason] += 1

            return {
                'total_messages': stream_info['length'],
                'error_breakdown': error_stats,
                'last_updated': datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"Failed to get DLQ stats: {str(e)}")
            return {}

    async def reprocess_dlq_messages(self, error_type: str = None, limit: int = 100) -> int:
        """é‡æ–°å¤„ç†æ­»ä¿¡é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯"""
        reprocessed = 0

        try:
            # è¯»å–DLQæ¶ˆæ¯
            messages = await self.redis.xrevrange(self.dlq_stream, count=limit)

            pipeline = DataIngestionPipeline(self.redis)

            for msg_id, fields in messages:
                message_data = {k.decode(): v.decode() for k, v in fields.items()}

                # å¦‚æœæŒ‡å®šäº†é”™è¯¯ç±»å‹ï¼Œåªå¤„ç†åŒ¹é…çš„æ¶ˆæ¯
                if error_type and message_data.get('error_reason') != error_type:
                    continue

                # é‡æ–°éªŒè¯æ•°æ®
                data_type = message_data['data_type']
                raw_data = json.loads(message_data['raw_data'])

                validated_data = await pipeline._validate_data(data_type, raw_data)

                if validated_data:
                    # éªŒè¯æˆåŠŸï¼Œé‡æ–°åŠ å…¥å¤„ç†é˜Ÿåˆ—
                    source_info = json.loads(message_data['source_info'])
                    await pipeline._enqueue_for_processing(
                        message_data['message_id'],
                        data_type,
                        validated_data,
                        source_info
                    )

                    # ä»DLQä¸­åˆ é™¤
                    await self.redis.xdel(self.dlq_stream, msg_id)
                    reprocessed += 1

        except Exception as e:
            logger.error(f"DLQ reprocessing error: {str(e)}")

        return reprocessed

    async def export_dlq_sample(self, count: int = 10) -> List[Dict]:
        """å¯¼å‡ºDLQæ ·æœ¬ç”¨äºåˆ†æ"""
        try:
            messages = await self.redis.xrevrange(self.dlq_stream, count=count)

            samples = []
            for msg_id, fields in messages:
                message_data = {k.decode(): v.decode() for k, v in fields.items()}

                # è§£æåŸå§‹æ•°æ®ç”¨äºåˆ†æ
                try:
                    raw_data = json.loads(message_data['raw_data'])
                    message_data['raw_data_preview'] = str(raw_data)[:500] + "..." if len(str(raw_data)) > 500 else str(raw_data)
                except:
                    message_data['raw_data_preview'] = "Failed to parse"

                samples.append(message_data)

            return samples

        except Exception as e:
            logger.error(f"Failed to export DLQ sample: {str(e)}")
            return []

# DLQç®¡ç†APIç«¯ç‚¹
@app.get("/admin/dlq/stats")
async def get_dlq_stats(current_user: str = Depends(verify_admin_token)):
    """è·å–æ­»ä¿¡é˜Ÿåˆ—ç»Ÿè®¡"""
    try:
        redis_async = Redis.from_url(REDIS_URL)
        dlq_manager = DeadLetterQueueManager(redis_async)

        stats = await dlq_manager.get_dlq_stats()
        return stats

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/dlq/reprocess")
async def reprocess_dlq(
    error_type: Optional[str] = None,
    limit: int = 100,
    current_user: str = Depends(verify_admin_token)
):
    """é‡æ–°å¤„ç†æ­»ä¿¡é˜Ÿåˆ—æ¶ˆæ¯"""
    try:
        redis_async = Redis.from_url(REDIS_URL)
        dlq_manager = DeadLetterQueueManager(redis_async)

        reprocessed_count = await dlq_manager.reprocess_dlq_messages(error_type, limit)

        return {
            "reprocessed_count": reprocessed_count,
            "error_type": error_type,
            "limit": limit
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

#### å¼‚æ­¥ä»£ç†å¥åº·æ£€æŸ¥ç³»ç»Ÿ
```python
# core/async_proxy_checker.py
import asyncio
import aiohttp
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import redis
import json

class AsyncProxyChecker:
    """å¼‚æ­¥ä»£ç†å¥åº·æ£€æŸ¥å™¨ - Sidecaræ¨¡å¼"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.check_interval = 300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        self.timeout = 10  # 10ç§’è¶…æ—¶
        self.test_urls = [
            'https://httpbin.org/ip',
            'https://x.com',
            'https://api.x.com/1.1/guest/activate.json'
        ]
        self.proxy_stats_key = "proxy_health_stats"
        self.proxy_cache_key = "proxy_cache"

    async def start_checker(self):
        """å¯åŠ¨ä»£ç†æ£€æŸ¥å™¨"""
        print("Starting async proxy checker...")

        while True:
            try:
                await self.check_all_proxies()
                await asyncio.sleep(self.check_interval)
            except Exception as e:
                print(f"Proxy checker error: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ

    async def check_all_proxies(self):
        """æ£€æŸ¥æ‰€æœ‰ä»£ç†"""
        # è·å–æ‰€æœ‰ä»£ç†åˆ—è¡¨
        proxy_keys = self.redis.keys("proxy:*")
        if not proxy_keys:
            return

        print(f"Checking {len(proxy_keys)} proxies...")

        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰ä»£ç†
        semaphore = asyncio.Semaphore(50)  # é™åˆ¶å¹¶å‘æ•°
        tasks = []

        for proxy_key in proxy_keys:
            proxy_data = self.redis.hgetall(proxy_key)
            if proxy_data:
                proxy_info = {
                    'id': proxy_key.decode().replace('proxy:', ''),
                    'host': proxy_data.get(b'host', b'').decode(),
                    'port': int(proxy_data.get(b'port', 0)),
                    'username': proxy_data.get(b'username', b'').decode(),
                    'password': proxy_data.get(b'password', b'').decode(),
                    'type': proxy_data.get(b'type', b'http').decode()
                }

                task = self.check_single_proxy(semaphore, proxy_info)
                tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰æ£€æŸ¥å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        healthy_count = 0
        total_count = len(results)

        for result in results:
            if isinstance(result, dict) and result.get('healthy'):
                healthy_count += 1

        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        await self.update_global_stats(healthy_count, total_count)

        print(f"Proxy check completed: {healthy_count}/{total_count} healthy")

    async def check_single_proxy(self, semaphore: asyncio.Semaphore, proxy_info: Dict) -> Dict:
        """æ£€æŸ¥å•ä¸ªä»£ç†"""
        async with semaphore:
            proxy_id = proxy_info['id']
            start_time = time.time()

            try:
                # æ„å»ºä»£ç†URL
                if proxy_info['username'] and proxy_info['password']:
                    proxy_url = f"{proxy_info['type']}://{proxy_info['username']}:{proxy_info['password']}@{proxy_info['host']}:{proxy_info['port']}"
                else:
                    proxy_url = f"{proxy_info['type']}://{proxy_info['host']}:{proxy_info['port']}"

                # æµ‹è¯•ä»£ç†è¿æ¥
                success_count = 0
                total_tests = len(self.test_urls)
                response_times = []

                async with aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                    connector=aiohttp.TCPConnector(ssl=False)
                ) as session:

                    for test_url in self.test_urls:
                        try:
                            test_start = time.time()
                            async with session.get(
                                test_url,
                                proxy=proxy_url,
                                timeout=aiohttp.ClientTimeout(total=self.timeout)
                            ) as response:
                                if response.status == 200:
                                    success_count += 1
                                    response_times.append(time.time() - test_start)
                        except:
                            pass  # å•ä¸ªURLå¤±è´¥ä¸å½±å“æ•´ä½“è¯„ä¼°

                # è®¡ç®—å¥åº·çŠ¶æ€
                success_rate = success_count / total_tests
                avg_response_time = sum(response_times) / len(response_times) if response_times else float('inf')

                is_healthy = success_rate >= 0.5 and avg_response_time < 30  # 50%æˆåŠŸç‡ä¸”å¹³å‡å“åº”æ—¶é—´<30ç§’

                # æ›´æ–°ä»£ç†çŠ¶æ€
                health_data = {
                    'proxy_id': proxy_id,
                    'healthy': is_healthy,
                    'success_rate': success_rate,
                    'avg_response_time': avg_response_time,
                    'last_check': datetime.utcnow().isoformat(),
                    'check_duration': time.time() - start_time
                }

                await self.update_proxy_health(proxy_id, health_data)

                return health_data

            except Exception as e:
                # ä»£ç†æ£€æŸ¥å¤±è´¥
                health_data = {
                    'proxy_id': proxy_id,
                    'healthy': False,
                    'error': str(e),
                    'last_check': datetime.utcnow().isoformat(),
                    'check_duration': time.time() - start_time
                }

                await self.update_proxy_health(proxy_id, health_data)
                return health_data

    async def update_proxy_health(self, proxy_id: str, health_data: Dict):
        """æ›´æ–°ä»£ç†å¥åº·çŠ¶æ€"""
        # æ›´æ–°ä»£ç†å¥åº·ç¼“å­˜
        cache_key = f"{self.proxy_cache_key}:{proxy_id}"

        # è®¾ç½®ç¼“å­˜ï¼ŒTTLä¸ºæ£€æŸ¥é—´éš”çš„2å€
        await self.redis.setex(
            cache_key,
            self.check_interval * 2,
            json.dumps(health_data)
        )

        # æ›´æ–°ä»£ç†çŠ¶æ€
        proxy_key = f"proxy:{proxy_id}"
        await self.redis.hset(proxy_key, 'healthy', str(health_data['healthy']))
        await self.redis.hset(proxy_key, 'last_check', health_data['last_check'])

        if 'avg_response_time' in health_data:
            await self.redis.hset(proxy_key, 'response_time', str(health_data['avg_response_time']))

    async def update_global_stats(self, healthy_count: int, total_count: int):
        """æ›´æ–°å…¨å±€ç»Ÿè®¡"""
        stats = {
            'total_proxies': total_count,
            'healthy_proxies': healthy_count,
            'unhealthy_proxies': total_count - healthy_count,
            'health_rate': healthy_count / total_count if total_count > 0 else 0,
            'last_check': datetime.utcnow().isoformat()
        }

        await self.redis.hmset(self.proxy_stats_key, stats)

    def get_cached_proxy_health(self, proxy_id: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„ä»£ç†å¥åº·çŠ¶æ€"""
        cache_key = f"{self.proxy_cache_key}:{proxy_id}"
        cached_data = self.redis.get(cache_key)

        if cached_data:
            return json.loads(cached_data.decode())
        return None

class OptimizedProxyManager:
    """ä¼˜åŒ–çš„ä»£ç†ç®¡ç†å™¨ - ä½¿ç”¨ç¼“å­˜çš„å¥åº·çŠ¶æ€"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.proxy_checker = AsyncProxyChecker(redis_client)

    async def get_healthy_proxy(self) -> Optional[Dict]:
        """è·å–å¥åº·çš„ä»£ç†ï¼ˆä»ç¼“å­˜è¯»å–ï¼‰"""
        # è·å–æ‰€æœ‰ä»£ç†
        proxy_keys = self.redis.keys("proxy:*")
        healthy_proxies = []

        for proxy_key in proxy_keys:
            proxy_id = proxy_key.decode().replace('proxy:', '')

            # ä»ç¼“å­˜è·å–å¥åº·çŠ¶æ€
            health_data = self.proxy_checker.get_cached_proxy_health(proxy_id)

            if health_data and health_data.get('healthy'):
                proxy_data = self.redis.hgetall(proxy_key)
                if proxy_data:
                    proxy_info = {
                        'id': proxy_id,
                        'host': proxy_data.get(b'host', b'').decode(),
                        'port': int(proxy_data.get(b'port', 0)),
                        'username': proxy_data.get(b'username', b'').decode(),
                        'password': proxy_data.get(b'password', b'').decode(),
                        'type': proxy_data.get(b'type', b'http').decode(),
                        'response_time': health_data.get('avg_response_time', float('inf'))
                    }
                    healthy_proxies.append(proxy_info)

        if not healthy_proxies:
            return None

        # æŒ‰å“åº”æ—¶é—´æ’åºï¼Œè¿”å›æœ€å¿«çš„
        healthy_proxies.sort(key=lambda x: x['response_time'])
        return healthy_proxies[0]

    async def get_proxy_stats(self) -> Dict:
        """è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.redis.hgetall(self.proxy_checker.proxy_stats_key)

        if stats:
            return {
                'total': int(stats.get(b'total_proxies', 0)),
                'healthy': int(stats.get(b'healthy_proxies', 0)),
                'unhealthy': int(stats.get(b'unhealthy_proxies', 0)),
                'health_rate': float(stats.get(b'health_rate', 0)),
                'last_check': stats.get(b'last_check', b'').decode()
            }

        return {
            'total': 0,
            'healthy': 0,
            'unhealthy': 0,
            'health_rate': 0,
            'last_check': ''
        }
```

### 15. è¿ç»´å“è¶Šæ€§å’Œå¯è§‚æµ‹æ€§

#### é›†ä¸­å¼æ—¥å¿—ç³»ç»Ÿ
```python
# core/logging_config.py
import logging
import json
from datetime import datetime
from typing import Dict, Any
import structlog
from pythonjsonlogger import jsonlogger

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, service_name: str, environment: str = "production"):
        self.service_name = service_name
        self.environment = environment
        self._setup_structured_logging()

    def _setup_structured_logging(self):
        """é…ç½®ç»“æ„åŒ–æ—¥å¿—"""
        # é…ç½®structlog
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # é…ç½®æ ‡å‡†åº“logging
        formatter = jsonlogger.JsonFormatter(
            '%(asctime)s %(name)s %(levelname)s %(message)s',
            datefmt='%Y-%m-%dT%H:%M:%S'
        )

        handler = logging.StreamHandler()
        handler.setFormatter(formatter)

        root_logger = logging.getLogger()
        root_logger.addHandler(handler)
        root_logger.setLevel(logging.INFO)

    def get_logger(self, module_name: str):
        """è·å–æ¨¡å—æ—¥å¿—è®°å½•å™¨"""
        logger = structlog.get_logger(module_name)
        return logger.bind(
            service=self.service_name,
            environment=self.environment,
            module=module_name
        )

# æ—¥å¿—ä¸­é—´ä»¶
class LoggingMiddleware:
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""

    def __init__(self, app, logger):
        self.app = app
        self.logger = logger

    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            start_time = datetime.utcnow()

            # è®°å½•è¯·æ±‚å¼€å§‹
            self.logger.info(
                "Request started",
                method=scope["method"],
                path=scope["path"],
                query_string=scope["query_string"].decode(),
                client_ip=scope.get("client", ["unknown", None])[0]
            )

            async def send_wrapper(message):
                if message["type"] == "http.response.start":
                    # è®°å½•å“åº”
                    duration = (datetime.utcnow() - start_time).total_seconds()
                    self.logger.info(
                        "Request completed",
                        status_code=message["status"],
                        duration_seconds=duration,
                        method=scope["method"],
                        path=scope["path"]
                    )
                await send(message)

            await self.app(scope, receive, send_wrapper)
        else:
            await self.app(scope, receive, send)

# Celeryä»»åŠ¡æ—¥å¿—è£…é¥°å™¨
def log_task_execution(func):
    """Celeryä»»åŠ¡æ‰§è¡Œæ—¥å¿—è£…é¥°å™¨"""
    def wrapper(self, *args, **kwargs):
        logger = structlog.get_logger(func.__name__)
        task_logger = logger.bind(
            task_id=self.request.id,
            task_name=func.__name__,
            args=str(args)[:200],  # é™åˆ¶å‚æ•°é•¿åº¦
            kwargs=str(kwargs)[:200]
        )

        task_logger.info("Task started")
        start_time = datetime.utcnow()

        try:
            result = func(self, *args, **kwargs)

            duration = (datetime.utcnow() - start_time).total_seconds()
            task_logger.info(
                "Task completed successfully",
                duration_seconds=duration,
                result_type=type(result).__name__
            )

            return result

        except Exception as e:
            duration = (datetime.utcnow() - start_time).total_seconds()
            task_logger.error(
                "Task failed",
                duration_seconds=duration,
                error_type=type(e).__name__,
                error_message=str(e),
                retry_count=self.request.retries
            )
            raise

    return wrapper

# åº”ç”¨æ—¥å¿—é…ç½®
def setup_application_logging():
    """è®¾ç½®åº”ç”¨ç¨‹åºæ—¥å¿—"""
    structured_logger = StructuredLogger("weget", os.getenv("ENVIRONMENT", "production"))

    # ä¸ºä¸åŒæ¨¡å—è®¾ç½®æ—¥å¿—è®°å½•å™¨
    loggers = {
        'scraper': structured_logger.get_logger('scraper'),
        'data_pipeline': structured_logger.get_logger('data_pipeline'),
        'cookie_manager': structured_logger.get_logger('cookie_manager'),
        'proxy_manager': structured_logger.get_logger('proxy_manager'),
        'api': structured_logger.get_logger('api'),
        'monitor': structured_logger.get_logger('monitor')
    }

    return loggers

# ä¿®æ”¹ç°æœ‰çš„é‡‡é›†å™¨ä»¥ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
class LoggingEnhancedScraper(DecoupledTwitterScraper):
    """å¢å¼ºæ—¥å¿—çš„é‡‡é›†å™¨"""

    def __init__(self, cookie_manager, proxy_manager, data_pipeline):
        super().__init__(cookie_manager, proxy_manager, data_pipeline)
        self.logger = structlog.get_logger('scraper')

    async def make_request(self, endpoint: str, params: Dict, account_id: str = None) -> Dict:
        """å¢å¼ºæ—¥å¿—çš„è¯·æ±‚æ–¹æ³•"""
        request_id = str(uuid.uuid4())

        self.logger.info(
            "API request started",
            request_id=request_id,
            endpoint=endpoint,
            account_id=account_id,
            params_keys=list(params.keys())
        )

        start_time = datetime.utcnow()

        try:
            result = await super().make_request(endpoint, params, account_id)

            duration = (datetime.utcnow() - start_time).total_seconds()
            self.logger.info(
                "API request successful",
                request_id=request_id,
                duration_seconds=duration,
                response_size=len(str(result))
            )

            return result

        except Exception as e:
            duration = (datetime.utcnow() - start_time).total_seconds()
            self.logger.error(
                "API request failed",
                request_id=request_id,
                duration_seconds=duration,
                error_type=type(e).__name__,
                error_message=str(e),
                account_id=account_id
            )
            raise
```

#### ELK Stack é…ç½®è¯´æ˜

**é‡è¦**: ELK Stack é…ç½®å·²è¿ç§»åˆ° Helm Chart ç®¡ç†ï¼Œé€šè¿‡ `logging.enabled=true` å¯ç”¨ã€‚

**æ—¥å¿—èšåˆéƒ¨ç½²**:
```bash
# å¯ç”¨ ELK Stack
helm upgrade --install weget ./weget-chart \
  --set logging.enabled=true \
  --set logging.elasticsearch.replicas=3 \
  --set logging.logstash.replicas=2 \
  --set logging.kibana.enabled=true

# è®¿é—® Kibana ä»ªè¡¨æ¿
kubectl port-forward svc/weget-kibana 5601:5601
```

**æ—¥å¿—é…ç½®ç‰¹æ€§**:
- **è‡ªåŠ¨æ—¥å¿—æ”¶é›†**: Filebeat è‡ªåŠ¨æ”¶é›†å®¹å™¨æ—¥å¿—
- **ç»“æ„åŒ–è§£æ**: Logstash è§£æ JSON æ ¼å¼æ—¥å¿—
- **ç´¢å¼•ç®¡ç†**: æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ›å»ºç´¢å¼•ï¼Œæ”¯æŒç”Ÿå‘½å‘¨æœŸç®¡ç†
- **é”™è¯¯èšåˆ**: é”™è¯¯æ—¥å¿—å•ç‹¬ç´¢å¼•ï¼Œä¾¿äºå‘Šè­¦
- **æ€§èƒ½ä¼˜åŒ–**: åˆ†ç‰‡å’Œå‰¯æœ¬é…ç½®ä¼˜åŒ–

```yaml
# config/filebeat/filebeat.yml
filebeat.inputs:
- type: container
  paths:
    - '/var/lib/docker/containers/*/*.log'
  processors:
    - add_docker_metadata:
        host: "unix:///var/run/docker.sock"
    - decode_json_fields:
        fields: ["message"]
        target: ""
        overwrite_keys: true

output.elasticsearch:
  hosts: ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
  index: "weget-logs-%{+yyyy.MM.dd}"

setup.template.name: "weget-logs"
setup.template.pattern: "weget-logs-*"
setup.template.settings:
  index.number_of_shards: 1
  index.number_of_replicas: 0

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

```yaml
# config/logstash/pipeline/weget.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [container][name] =~ /weget/ {
    # è§£æJSONæ—¥å¿—
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
    }

    # æ·»åŠ æœåŠ¡æ ‡è¯†
    mutate {
      add_field => { "service_type" => "weget" }
    }

    # è§£ææ—¶é—´æˆ³
    if [timestamp] {
      date {
        match => [ "timestamp", "ISO8601" ]
      }
    }

    # æå–é”™è¯¯ä¿¡æ¯
    if [level] == "ERROR" or [levelname] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }

    # æå–ä»»åŠ¡ä¿¡æ¯
    if [task_id] {
      mutate {
        add_field => { "log_type" => "task" }
      }
    }

    # æå–APIè¯·æ±‚ä¿¡æ¯
    if [request_id] {
      mutate {
        add_field => { "log_type" => "api_request" }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "weget-logs-%{+YYYY.MM.dd}"
  }

  # é”™è¯¯æ—¥å¿—é¢å¤–è¾“å‡ºåˆ°ä¸“é—¨çš„ç´¢å¼•
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "weget-errors-%{+YYYY.MM.dd}"
    }
  }
}
```

### 16. KubernetesåŠ¨æ€æ‰©ç¼©å®¹

#### Helm Charté…ç½®
```yaml
# helm/weget/Chart.yaml
apiVersion: v2
name: weget
description: WeGet X(Twitter) Data Collection System
type: application
version: 1.0.0
appVersion: "1.0.0"
keywords:
  - twitter
  - data-collection
  - scraping
home: https://github.com/your-org/weget
sources:
  - https://github.com/your-org/weget
maintainers:
  - name: WeGet Team
    email: team@weget.com

dependencies:
  - name: redis
    version: 17.x.x
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
  - name: mongodb
    version: 13.x.x
    repository: https://charts.bitnami.com/bitnami
    condition: mongodb.enabled
  - name: neo4j
    version: 4.x.x
    repository: https://helm.neo4j.com/neo4j
    condition: neo4j.enabled
```

```yaml
# helm/weget/values.yaml
# Global configuration
global:
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: ""

# Application configuration
app:
  name: weget
  version: "1.0.0"

image:
  registry: docker.io
  repository: weget/weget
  tag: "1.0.0"
  pullPolicy: IfNotPresent

# Deployment configuration
deployment:
  replicaCount: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

# Service configuration
service:
  type: ClusterIP
  port: 8000
  targetPort: 8000

# Ingress configuration
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: weget.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: weget-tls
      hosts:
        - weget.example.com

# Resources
resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

# Autoscaling
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Node selection
nodeSelector: {}
tolerations: []
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - weget
        topologyKey: kubernetes.io/hostname

# Security context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# Pod security context
podSecurityContext:
  seccompProfile:
    type: RuntimeDefault

# Environment variables
env:
  - name: REDIS_URL
    value: "redis://weget-redis:6379"
  - name: MONGODB_URL
    valueFrom:
      secretKeyRef:
        name: weget-secrets
        key: mongodb-url
  - name: NEO4J_URL
    valueFrom:
      secretKeyRef:
        name: weget-secrets
        key: neo4j-url

# ConfigMap
configMap:
  enabled: true
  data:
    app.yaml: |
      logging:
        level: INFO
        format: json
      scraping:
        max_concurrent_tasks: 100
        request_timeout: 30
        retry_attempts: 3

# Secrets (managed by external-secrets or sealed-secrets)
secrets:
  enabled: true
  data: {}  # Populated by external systems

# Persistent volumes
persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 10Gi

# Redis configuration
redis:
  enabled: true
  auth:
    enabled: true
    password: "redis-password"
  master:
    persistence:
      enabled: true
      size: 8Gi

# MongoDB configuration
mongodb:
  enabled: true
  auth:
    enabled: true
    rootPassword: "mongodb-password"
  persistence:
    enabled: true
    size: 20Gi

# Neo4j configuration
neo4j:
  enabled: true
  neo4j:
    password: "neo4j-password"
  volumes:
    data:
      mode: "defaultStorageClass"
      defaultStorageClass:
        requests:
          storage: 10Gi

# Monitoring
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    path: /metrics
  prometheusRule:
    enabled: true

# KEDA scaling
keda:
  enabled: true
  scaledObjects:
    - name: weget-redis-scaler
      scaleTargetRef:
        name: weget
      minReplicaCount: 3
      maxReplicaCount: 20
      triggers:
        - type: redis
          metadata:
            address: weget-redis:6379
            listName: weget:tasks
            listLength: "10"
```

```yaml
# helm/weget/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "weget.fullname" . }}
  labels:
    {{- include "weget.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.deployment.replicaCount }}
  {{- end }}
  strategy:
    {{- toYaml .Values.deployment.strategy | nindent 4 }}
  selector:
    matchLabels:
      {{- include "weget.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
      labels:
        {{- include "weget.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.global.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "weget.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.targetPort }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            {{- toYaml .Values.env | nindent 12 }}
          envFrom:
            - configMapRef:
                name: {{ include "weget.fullname" . }}-config
            - secretRef:
                name: {{ include "weget.fullname" . }}-secrets
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
            - name: data-volume
              mountPath: /app/data
      volumes:
        - name: config-volume
          configMap:
            name: {{ include "weget.fullname" . }}-config
        - name: data-volume
          {{- if .Values.persistence.enabled }}
          persistentVolumeClaim:
            claimName: {{ include "weget.fullname" . }}-data
          {{- else }}
          emptyDir: {}
          {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

#### KEDAé…ç½®
```yaml
# k8s/keda-scaler.yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: weget-search-worker-scaler
  namespace: weget
spec:
  scaleTargetRef:
    name: weget-search-worker
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
  - type: redis
    metadata:
      address: redis:6379
      listName: search
      listLength: '10'
      enableTLS: 'false'

---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: weget-profile-worker-scaler
  namespace: weget
spec:
  scaleTargetRef:
    name: weget-profile-worker
  minReplicaCount: 1
  maxReplicaCount: 15
  triggers:
  - type: redis
    metadata:
      address: redis:6379
      listName: profile
      listLength: '5'
      enableTLS: 'false'

---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: weget-processing-worker-scaler
  namespace: weget
spec:
  scaleTargetRef:
    name: weget-processing-worker
  minReplicaCount: 3
  maxReplicaCount: 30
  triggers:
  - type: redis-streams
    metadata:
      address: redis:6379
      stream: weget:ingestion:stream
      consumerGroup: processors
      pendingEntriesCount: '20'
      enableTLS: 'false'
```

#### Kuberneteséƒ¨ç½²é…ç½®
```yaml
# k8s/weget-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: weget-search-worker
  namespace: weget
spec:
  replicas: 2
  selector:
    matchLabels:
      app: weget-search-worker
  template:
    metadata:
      labels:
        app: weget-search-worker
    spec:
      containers:
      - name: worker
        image: weget:latest
        command: ["celery", "-A", "core.tasks", "worker", "--loglevel=info", "--queues=search", "--concurrency=4"]
        env:
        - name: REDIS_HOST
          value: "redis"
        - name: MONGODB_URI
          valueFrom:
            secretKeyRef:
              name: weget-secrets
              key: mongodb-uri
        - name: VAULT_TOKEN
          valueFrom:
            secretKeyRef:
              name: weget-secrets
              key: vault-token
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - celery
            - -A
            - core.tasks
            - inspect
            - ping
          initialDelaySeconds: 30
          periodSeconds: 60
        readinessProbe:
          exec:
            command:
            - celery
            - -A
            - core.tasks
            - inspect
            - active
          initialDelaySeconds: 10
          periodSeconds: 30

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: weget-processing-worker
  namespace: weget
spec:
  replicas: 3
  selector:
    matchLabels:
      app: weget-processing-worker
  template:
    metadata:
      labels:
        app: weget-processing-worker
    spec:
      containers:
      - name: processor
        image: weget:latest
        command: ["python", "-c", "from core.tasks import data_processing_task; data_processing_task.delay()"]
        env:
        - name: REDIS_HOST
          value: "redis"
        - name: MONGODB_URI
          valueFrom:
            secretKeyRef:
              name: weget-secrets
              key: mongodb-uri
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

#### CI/CDè´¨é‡é—¸é—¨é…ç½®
```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  quality-gate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # å®Œæ•´å†å²ç”¨äºSonarQube

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff mypy pytest pytest-cov pytest-asyncio
        pip install -r requirements.txt

    - name: Code formatting check (Ruff)
      run: |
        ruff check . --output-format=github
        ruff format --check .

    - name: Type checking (MyPy)
      run: |
        mypy --config-file pyproject.toml src/

    - name: Security scan (Bandit)
      run: |
        pip install bandit[toml]
        bandit -r src/ -f json -o bandit-report.json

    - name: Dependency vulnerability scan
      run: |
        pip install safety
        safety check --json --output safety-report.json

    - name: Secret scanning (TruffleHog)
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD

    - name: Run tests with coverage
      run: |
        pytest --cov=src --cov-report=xml --cov-report=html --cov-fail-under=90
      env:
        PYTHONPATH: src

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

    - name: SonarQube Scan
      uses: sonarqube-quality-gate-action@master
      env:
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

  build-and-push:
    needs: quality-gate
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write  # for cosign

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Install Cosign
      uses: sigstore/cosign-installer@v3

    - name: Sign container image
      run: |
        cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}

    - name: Generate SBOM
      uses: anchore/sbom-action@v0
      with:
        image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
        format: spdx-json
        output-file: sbom.spdx.json

    - name: Attest SBOM
      run: |
        cosign attest --yes --predicate sbom.spdx.json --type spdxjson ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_STAGING }}

    - name: Deploy to staging
      run: |
        helm upgrade --install weget-staging ./helm/weget \
          --namespace weget-staging \
          --create-namespace \
          --values ./helm/weget/values-staging.yaml \
          --set image.tag=${{ github.sha }}

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_PROD }}

    - name: Deploy to production
      run: |
        helm upgrade --install weget ./helm/weget \
          --namespace weget-prod \
          --create-namespace \
          --values ./helm/weget/values-prod.yaml \
          --set image.tag=${{ github.sha }}
```

```toml
# pyproject.toml - ä»£ç è´¨é‡é…ç½®
[tool.ruff]
target-version = "py311"
line-length = 120
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
    "S",  # bandit
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"tests/*" = ["S101"]  # allow assert in tests

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
markers = [
    "slow: marks tests as slow",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]
```

```yaml
# .github/workflows/security.yml
name: Security Scan

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œ
  workflow_dispatch:

jobs:
  container-scan:
    runs-on: ubuntu-latest
    steps:
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'ghcr.io/${{ github.repository }}:latest'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  dependency-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Run Snyk to check for vulnerabilities
      uses: snyk/actions/python@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
      with:
        args: --severity-threshold=high
```

### 17. ä»£ç é‡æ„å’Œæœ€ä½³å®è·µ

#### é‡æ„çš„æ•°æ®æå–å™¨
```python
# modules/data_extractors.py
from typing import Dict, Optional, List
from abc import ABC, abstractmethod

class BaseDataExtractor(ABC):
    """æ•°æ®æå–å™¨åŸºç±»"""

    @abstractmethod
    def extract(self, raw_response: Dict) -> Dict:
        """ä»åŸå§‹å“åº”ä¸­æå–æ•°æ®"""
        pass

    @abstractmethod
    def get_model_class(self):
        """è·å–å¯¹åº”çš„Pydanticæ¨¡å‹ç±»"""
        pass

class TwitterUserExtractor(BaseDataExtractor):
    """Twitterç”¨æˆ·æ•°æ®æå–å™¨"""

    def extract(self, raw_response: Dict) -> Dict:
        """æå–ç”¨æˆ·æ•°æ®"""
        try:
            # å¯¼èˆªåˆ°ç”¨æˆ·æ•°æ®
            user_results = raw_response.get('data', {}).get('user', {}).get('result', {})
            if not user_results:
                # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                user_results = self._find_user_data_alternative_paths(raw_response)

            if not user_results:
                raise ValueError("No user data found in response")

            legacy = user_results.get('legacy', {})

            # æå–åŸºç¡€å­—æ®µ
            extracted_data = {
                'user_id': user_results.get('rest_id') or legacy.get('id_str'),
                'username': legacy.get('screen_name'),
                'display_name': legacy.get('name'),
                'followers_count': legacy.get('followers_count', 0),
                'following_count': legacy.get('friends_count', 0),
                'tweet_count': legacy.get('statuses_count', 0),
                'verified': legacy.get('verified', False),
                'created_at': legacy.get('created_at'),
                'description': legacy.get('description'),
                'location': legacy.get('location'),
                'profile_image_url': legacy.get('profile_image_url_https')
            }

            # æå–æ‰©å±•ä¿¡æ¯
            if 'professional' in user_results:
                extracted_data['is_business_account'] = True
                extracted_data['business_category'] = user_results['professional'].get('category', [])
            else:
                extracted_data['is_business_account'] = False
                extracted_data['business_category'] = []

            # æå–éªŒè¯ä¿¡æ¯
            if 'affiliates_highlighted_label' in user_results:
                extracted_data['verification_type'] = 'blue'
            elif legacy.get('verified'):
                extracted_data['verification_type'] = 'legacy'
            else:
                extracted_data['verification_type'] = 'none'

            return extracted_data

        except Exception as e:
            raise ValueError(f"Failed to extract user data: {str(e)}")

    def _find_user_data_alternative_paths(self, raw_response: Dict) -> Optional[Dict]:
        """åœ¨å“åº”ä¸­æŸ¥æ‰¾ç”¨æˆ·æ•°æ®çš„æ›¿ä»£è·¯å¾„"""
        # å°è¯•ä¸åŒçš„å¯èƒ½è·¯å¾„
        possible_paths = [
            ['data', 'user_by_screen_name', 'result'],
            ['data', 'user_result', 'result'],
            ['includes', 'users', 0],
        ]

        for path in possible_paths:
            current = raw_response
            try:
                for key in path:
                    if isinstance(key, int):
                        current = current[key]
                    else:
                        current = current.get(key, {})
                if current and isinstance(current, dict):
                    return current
            except (KeyError, IndexError, TypeError):
                continue

        return None

    def get_model_class(self):
        return TwitterUserModel

class TwitterTweetExtractor(BaseDataExtractor):
    """Twitteræ¨æ–‡æ•°æ®æå–å™¨"""

    def extract(self, raw_response: Dict) -> Dict:
        """æå–æ¨æ–‡æ•°æ®"""
        try:
            # æŸ¥æ‰¾æ¨æ–‡æ•°æ®
            tweet_result = self._find_tweet_data(raw_response)
            if not tweet_result:
                raise ValueError("No tweet data found in response")

            legacy = tweet_result.get('legacy', {})
            core = tweet_result.get('core', {})

            # æå–åŸºç¡€å­—æ®µ
            extracted_data = {
                'tweet_id': tweet_result.get('rest_id') or legacy.get('id_str'),
                'user_id': self._extract_user_id(tweet_result),
                'content': legacy.get('full_text', ''),
                'created_at': legacy.get('created_at'),
                'retweet_count': legacy.get('retweet_count', 0),
                'favorite_count': legacy.get('favorite_count', 0),
                'reply_count': legacy.get('reply_count', 0),
                'quote_count': legacy.get('quote_count', 0),
                'view_count': tweet_result.get('views', {}).get('count', 0),
                'is_retweet': legacy.get('retweeted', False),
                'is_quote': 'quoted_status_id_str' in legacy,
                'lang': legacy.get('lang'),
                'source': legacy.get('source', '')
            }

            # æå–åª’ä½“ä¿¡æ¯
            extracted_data['media'] = self._extract_media_data(legacy)

            # æå–å®ä½“ä¿¡æ¯
            entities = legacy.get('entities', {})
            extracted_data['hashtags'] = [tag['text'] for tag in entities.get('hashtags', [])]
            extracted_data['urls'] = [url['expanded_url'] for url in entities.get('urls', []) if url.get('expanded_url')]
            extracted_data['user_mentions'] = [mention['screen_name'] for mention in entities.get('user_mentions', [])]

            # æå–åœ°ç†ä½ç½®ä¿¡æ¯
            if 'geo' in legacy and legacy['geo']:
                extracted_data['geo_coordinates'] = legacy['geo'].get('coordinates', [])
            else:
                extracted_data['geo_coordinates'] = []

            # æå–æ•æ„Ÿå†…å®¹æ ‡è®°
            extracted_data['possibly_sensitive'] = legacy.get('possibly_sensitive', False)

            return extracted_data

        except Exception as e:
            raise ValueError(f"Failed to extract tweet data: {str(e)}")

    def _find_tweet_data(self, raw_response: Dict) -> Optional[Dict]:
        """æŸ¥æ‰¾æ¨æ–‡æ•°æ®"""
        # å°è¯•ä¸åŒçš„è·¯å¾„
        possible_paths = [
            ['data', 'tweetResult', 'result'],
            ['data', 'tweet_result', 'result'],
            ['includes', 'tweets', 0],
        ]

        # ä¹Ÿå¯èƒ½åœ¨timeline entriesä¸­
        if 'data' in raw_response:
            timeline_data = raw_response['data']
            for key in timeline_data:
                if 'timeline' in key.lower():
                    timeline = timeline_data[key]
                    if isinstance(timeline, dict) and 'timeline' in timeline:
                        instructions = timeline['timeline'].get('instructions', [])
                        for instruction in instructions:
                            if instruction.get('type') == 'TimelineAddEntries':
                                entries = instruction.get('entries', [])
                                for entry in entries:
                                    if entry.get('entryId', '').startswith('tweet-'):
                                        content = entry.get('content', {})
                                        item_content = content.get('itemContent', {})
                                        tweet_results = item_content.get('tweet_results', {})
                                        result = tweet_results.get('result', {})
                                        if result and result.get('__typename') == 'Tweet':
                                            return result

        # å°è¯•æ ‡å‡†è·¯å¾„
        for path in possible_paths:
            current = raw_response
            try:
                for key in path:
                    if isinstance(key, int):
                        current = current[key]
                    else:
                        current = current.get(key, {})
                if current and isinstance(current, dict):
                    return current
            except (KeyError, IndexError, TypeError):
                continue

        return None

    def _extract_user_id(self, tweet_result: Dict) -> str:
        """æå–ç”¨æˆ·ID"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        user_id_paths = [
            ['core', 'user_results', 'result', 'rest_id'],
            ['core', 'user_results', 'result', 'legacy', 'id_str'],
            ['legacy', 'user_id_str']
        ]

        for path in user_id_paths:
            current = tweet_result
            try:
                for key in path:
                    current = current.get(key, {})
                if current and isinstance(current, str):
                    return current
            except (AttributeError, TypeError):
                continue

        return ""

    def _extract_media_data(self, legacy: Dict) -> List[Dict]:
        """æå–åª’ä½“æ•°æ®"""
        media_list = []
        entities = legacy.get('entities', {})
        extended_entities = legacy.get('extended_entities', {})

        # ä¼˜å…ˆä½¿ç”¨extended_entities
        media_source = extended_entities.get('media', entities.get('media', []))

        for media in media_source:
            media_info = {
                'type': media.get('type', 'photo'),
                'url': media.get('media_url_https', ''),
                'video_info': None
            }

            # å¤„ç†è§†é¢‘ä¿¡æ¯
            if media.get('type') in ['video', 'animated_gif']:
                video_info = media.get('video_info', {})
                if video_info:
                    media_info['video_info'] = {
                        'duration_millis': video_info.get('duration_millis', 0),
                        'aspect_ratio': video_info.get('aspect_ratio', []),
                        'variants': video_info.get('variants', [])
                    }

            media_list.append(media_info)

        return media_list

    def get_model_class(self):
        return TwitterTweetModel

class TwitterReplyExtractor(BaseDataExtractor):
    """Twitterå›å¤æ•°æ®æå–å™¨"""

    def extract(self, raw_response: Dict) -> Dict:
        """æå–å›å¤æ•°æ®"""
        # å›å¤çš„æå–é€»è¾‘ä¸æ¨æ–‡ç±»ä¼¼ï¼Œä½†éœ€è¦é¢å¤–çš„çˆ¶æ¨æ–‡ä¿¡æ¯
        tweet_extractor = TwitterTweetExtractor()
        base_data = tweet_extractor.extract(raw_response)

        # è½¬æ¢ä¸ºå›å¤æ ¼å¼
        reply_data = {
            'reply_id': base_data['tweet_id'],
            'parent_tweet_id': self._extract_parent_tweet_id(raw_response),
            'user_id': base_data['user_id'],
            'content': base_data['content'],
            'created_at': base_data['created_at'],
            'retweet_count': base_data['retweet_count'],
            'favorite_count': base_data['favorite_count'],
            'reply_count': base_data['reply_count'],
            'is_reply_to_reply': self._is_reply_to_reply(raw_response)
        }

        return reply_data

    def _extract_parent_tweet_id(self, raw_response: Dict) -> str:
        """æå–çˆ¶æ¨æ–‡ID"""
        tweet_result = TwitterTweetExtractor()._find_tweet_data(raw_response)
        if tweet_result:
            legacy = tweet_result.get('legacy', {})
            return legacy.get('in_reply_to_status_id_str', '')
        return ""

    def _is_reply_to_reply(self, raw_response: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¯¹å›å¤çš„å›å¤"""
        # è¿™éœ€è¦é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥åˆ¤æ–­
        # ç®€åŒ–å®ç°ï¼Œå¯ä»¥é€šè¿‡æ£€æŸ¥å›å¤é“¾æ¥ç¡®å®š
        return False

    def get_model_class(self):
        return TwitterReplyModel

# æ•°æ®æå–å·¥å‚
class DataExtractorFactory:
    """æ•°æ®æå–å™¨å·¥å‚"""

    _extractors = {
        'user': TwitterUserExtractor,
        'tweet': TwitterTweetExtractor,
        'reply': TwitterReplyExtractor
    }

    @classmethod
    def get_extractor(cls, data_type: str) -> BaseDataExtractor:
        """è·å–æ•°æ®æå–å™¨"""
        if data_type not in cls._extractors:
            raise ValueError(f"Unknown data type: {data_type}")

        return cls._extractors[data_type]()

    @classmethod
    def register_extractor(cls, data_type: str, extractor_class):
        """æ³¨å†Œæ–°çš„æ•°æ®æå–å™¨"""
        cls._extractors[data_type] = extractor_class

# é‡æ„åçš„é‡‡é›†å™¨
class RefactoredTwitterScraper(LoggingEnhancedScraper):
    """é‡æ„åçš„Twitteré‡‡é›†å™¨"""

    def __init__(self, cookie_manager, proxy_manager, data_pipeline):
        super().__init__(cookie_manager, proxy_manager, data_pipeline)
        self.extractor_factory = DataExtractorFactory()

    async def extract_and_save_data(self, data_type: str, raw_response: Dict, source_info: Dict) -> bool:
        """æå–å¹¶ä¿å­˜æ•°æ®"""
        try:
            # è·å–å¯¹åº”çš„æå–å™¨
            extractor = self.extractor_factory.get_extractor(data_type)

            # æå–æ•°æ®
            extracted_data = extractor.extract(raw_response)

            # éªŒè¯æ•°æ®
            model_class = extractor.get_model_class()
            validated_model = model_class(**extracted_data)

            # ä¿å­˜åˆ°æ•°æ®ç®¡é“
            message_id = await self.data_pipeline.enqueue_raw_data(
                data_type,
                validated_model.dict(),
                source_info
            )

            self.logger.info(
                "Data extracted and queued",
                data_type=data_type,
                message_id=message_id,
                extracted_fields=list(extracted_data.keys())
            )

            return True

        except Exception as e:
            self.logger.error(
                "Data extraction failed",
                data_type=data_type,
                error_type=type(e).__name__,
                error_message=str(e)
            )

            # å‘é€åŸå§‹æ•°æ®åˆ°æ­»ä¿¡é˜Ÿåˆ—
            await self.data_pipeline._send_to_dlq(
                {
                    'data_type': data_type,
                    'raw_data': json.dumps(raw_response),
                    'source_info': json.dumps(source_info)
                },
                f"Extraction failed: {str(e)}"
            )

            return False
```

#### å¤–ç½®Prometheusè§„åˆ™é…ç½®
```yaml
# config/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: weget-alerts
  namespace: weget-prod
  labels:
    app: weget
    prometheus: kube-prometheus
spec:
  groups:
  - name: weget.rules
    interval: 30s
    rules:
    # è´¦å·å¥åº·è§„åˆ™
    - alert: WeGetAccountBanRateHigh
      expr: (weget_accounts_banned / weget_accounts_total) > 0.1
      for: 5m
      labels:
        severity: warning
        service: weget
        component: account-manager
      annotations:
        summary: "WeGetè´¦å·å°ç¦ç‡è¿‡é«˜"
        description: "è´¦å·å°ç¦ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œè¶…è¿‡10%é˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/account-ban-rate"

    - alert: WeGetAccountBanRateCritical
      expr: (weget_accounts_banned / weget_accounts_total) > 0.2
      for: 2m
      labels:
        severity: critical
        service: weget
        component: account-manager
      annotations:
        summary: "WeGetè´¦å·å°ç¦ç‡ä¸¥é‡è¿‡é«˜"
        description: "è´¦å·å°ç¦ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œè¶…è¿‡20%ä¸´ç•Œé˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/account-ban-rate"

    # ä»£ç†å¥åº·è§„åˆ™
    - alert: WeGetProxyFailureRateHigh
      expr: (weget_proxies_failed / weget_proxies_total) > 0.3
      for: 5m
      labels:
        severity: warning
        service: weget
        component: proxy-manager
      annotations:
        summary: "WeGetä»£ç†å¤±è´¥ç‡è¿‡é«˜"
        description: "ä»£ç†å¤±è´¥ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œè¶…è¿‡30%é˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/proxy-failure-rate"

    # ä»»åŠ¡æ‰§è¡Œè§„åˆ™
    - alert: WeGetTaskFailureRateHigh
      expr: rate(weget_tasks_failed_total[5m]) / rate(weget_tasks_completed_total[5m]) > 0.05
      for: 3m
      labels:
        severity: critical
        service: weget
        component: task-scheduler
      annotations:
        summary: "WeGetä»»åŠ¡å¤±è´¥ç‡è¿‡é«˜"
        description: "ä»»åŠ¡å¤±è´¥ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œè¶…è¿‡5%é˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/task-failure-rate"

    - alert: WeGetTaskQueueBacklog
      expr: weget_tasks_pending > 1000
      for: 10m
      labels:
        severity: warning
        service: weget
        component: task-scheduler
      annotations:
        summary: "WeGetä»»åŠ¡é˜Ÿåˆ—ç§¯å‹"
        description: "å¾…å¤„ç†ä»»åŠ¡æ•°é‡ä¸º {{ $value }}ï¼Œè¶…è¿‡1000ä¸ª"
        runbook_url: "https://docs.weget.com/runbooks/task-queue-backlog"

    # ç³»ç»Ÿèµ„æºè§„åˆ™
    - alert: WeGetHighCPUUsage
      expr: weget_cpu_usage_percent > 80
      for: 5m
      labels:
        severity: warning
        service: weget
        component: system
      annotations:
        summary: "WeGet CPUä½¿ç”¨ç‡è¿‡é«˜"
        description: "CPUä½¿ç”¨ç‡ä¸º {{ $value }}%ï¼Œè¶…è¿‡80%é˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/high-cpu-usage"

    - alert: WeGetHighMemoryUsage
      expr: (weget_memory_usage_bytes / (1024*1024*1024)) > 8
      for: 5m
      labels:
        severity: warning
        service: weget
        component: system
      annotations:
        summary: "WeGetå†…å­˜ä½¿ç”¨è¿‡é«˜"
        description: "å†…å­˜ä½¿ç”¨é‡ä¸º {{ $value | humanize }}GBï¼Œè¶…è¿‡8GBé˜ˆå€¼"
        runbook_url: "https://docs.weget.com/runbooks/high-memory-usage"

    # æ•°æ®é‡‡é›†è§„åˆ™
    - alert: WeGetDataCollectionRateDropped
      expr: rate(weget_tweets_collected_total[5m]) < 100
      for: 10m
      labels:
        severity: warning
        service: weget
        component: data-collector
      annotations:
        summary: "WeGetæ•°æ®é‡‡é›†é€Ÿç‡ä¸‹é™"
        description: "æ•°æ®é‡‡é›†é€Ÿç‡ä¸º {{ $value }}/åˆ†é’Ÿï¼Œä½äºé¢„æœŸ"
        runbook_url: "https://docs.weget.com/runbooks/data-collection-rate"

    - alert: WeGetDataValidationErrors
      expr: rate(weget_data_validation_errors_total[5m]) > 10
      for: 5m
      labels:
        severity: warning
        service: weget
        component: data-validator
      annotations:
        summary: "WeGetæ•°æ®éªŒè¯é”™è¯¯å¢åŠ "
        description: "æ•°æ®éªŒè¯é”™è¯¯ç‡ä¸º {{ $value }}/åˆ†é’Ÿ"
        runbook_url: "https://docs.weget.com/runbooks/data-validation-errors"

    # æµè§ˆå™¨å®ä¾‹è§„åˆ™
    - alert: WeGetBrowserInstancesHigh
      expr: weget_browser_instances > 100
      for: 5m
      labels:
        severity: warning
        service: weget
        component: browser-pool
      annotations:
        summary: "WeGetæµè§ˆå™¨å®ä¾‹æ•°è¿‡å¤š"
        description: "æµè§ˆå™¨å®ä¾‹æ•°ä¸º {{ $value }}ï¼Œå¯èƒ½å­˜åœ¨èµ„æºæ³„æ¼"
        runbook_url: "https://docs.weget.com/runbooks/browser-instances-high"

    # æœåŠ¡å¯ç”¨æ€§è§„åˆ™
    - alert: WeGetServiceDown
      expr: up{job="weget"} == 0
      for: 1m
      labels:
        severity: critical
        service: weget
        component: service
      annotations:
        summary: "WeGetæœåŠ¡ä¸å¯ç”¨"
        description: "WeGetæœåŠ¡å®ä¾‹ {{ $labels.instance }} å·²ä¸‹çº¿"
        runbook_url: "https://docs.weget.com/runbooks/service-down"
```

```yaml
# config/alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@weget.com'
  smtp_auth_username: 'alerts@weget.com'
  smtp_auth_password_file: '/run/secrets/smtp_password'

route:
  group_by: ['alertname', 'service', 'component']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
    group_wait: 5s
    repeat_interval: 30m
  - match:
      severity: warning
    receiver: 'warning-alerts'
    repeat_interval: 2h

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://weget-api:8000/webhooks/alerts'
    send_resolved: true

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@weget.com'
    subject: '[CRITICAL] WeGet Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Runbook: {{ .Annotations.runbook_url }}
      {{ end }}
  webhook_configs:
  - url: 'http://weget-api:8000/webhooks/alerts'
    send_resolved: true
  - url: 'https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN'
    send_resolved: true

- name: 'warning-alerts'
  email_configs:
  - to: 'team@weget.com'
    subject: '[WARNING] WeGet Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Runbook: {{ .Annotations.runbook_url }}
      {{ end }}

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'service', 'component']
```

#### OpenTelemetryå¯è§‚æµ‹æ€§é›†æˆ
```python
# core/observability.py
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.pymongo import PymongoInstrumentor
from opentelemetry.instrumentation.celery import CeleryInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.resources import Resource
import logging

class ObservabilitySetup:
    """OpenTelemetryå¯è§‚æµ‹æ€§é…ç½®"""

    def __init__(self, service_name: str = "weget", service_version: str = "1.0.0"):
        self.service_name = service_name
        self.service_version = service_version
        self.resource = Resource.create({
            "service.name": service_name,
            "service.version": service_version,
            "service.namespace": "weget",
        })

    def setup_tracing(self, jaeger_endpoint: str = "http://localhost:14268/api/traces"):
        """è®¾ç½®åˆ†å¸ƒå¼è¿½è¸ª"""
        # é…ç½®Tracer Provider
        trace.set_tracer_provider(TracerProvider(resource=self.resource))

        # é…ç½®Jaegerå¯¼å‡ºå™¨
        jaeger_exporter = JaegerExporter(
            endpoint=jaeger_endpoint,
        )

        # æ·»åŠ Spanå¤„ç†å™¨
        span_processor = BatchSpanProcessor(jaeger_exporter)
        trace.get_tracer_provider().add_span_processor(span_processor)

        # è‡ªåŠ¨ä»ªè¡¨åŒ–
        FastAPIInstrumentor.instrument()
        RedisInstrumentor.instrument()
        PymongoInstrumentor.instrument()
        CeleryInstrumentor.instrument()

        print(f"Tracing initialized for {self.service_name}")

    def setup_metrics(self, prometheus_port: int = 8000):
        """è®¾ç½®æŒ‡æ ‡æ”¶é›†"""
        # é…ç½®PrometheusæŒ‡æ ‡è¯»å–å™¨
        prometheus_reader = PrometheusMetricReader(port=prometheus_port)

        # é…ç½®Meter Provider
        metrics.set_meter_provider(MeterProvider(
            resource=self.resource,
            metric_readers=[prometheus_reader]
        ))

        print(f"Metrics initialized on port {prometheus_port}")

    def get_tracer(self, name: str = None):
        """è·å–Tracerå®ä¾‹"""
        return trace.get_tracer(name or self.service_name)

    def get_meter(self, name: str = None):
        """è·å–Meterå®ä¾‹"""
        return metrics.get_meter(name or self.service_name)

# è£…é¥°å™¨ç”¨äºè‡ªåŠ¨è¿½è¸ª
def trace_function(operation_name: str = None):
    """å‡½æ•°è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(operation_name or func.__name__) as span:
                try:
                    # æ·»åŠ å‡½æ•°å‚æ•°ä½œä¸ºå±æ€§
                    span.set_attribute("function.name", func.__name__)
                    span.set_attribute("function.module", func.__module__)

                    result = func(*args, **kwargs)
                    span.set_attribute("function.result.type", type(result).__name__)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                    raise
        return wrapper
    return decorator

# å¼‚æ­¥ç‰ˆæœ¬
def trace_async_function(operation_name: str = None):
    """å¼‚æ­¥å‡½æ•°è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(operation_name or func.__name__) as span:
                try:
                    span.set_attribute("function.name", func.__name__)
                    span.set_attribute("function.module", func.__module__)
                    span.set_attribute("function.async", True)

                    result = await func(*args, **kwargs)
                    span.set_attribute("function.result.type", type(result).__name__)
                    return result
                except Exception as e:
                    span.record_exception(e)
                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                    raise
        return wrapper
    return decorator

# é›†æˆåˆ°é‡‡é›†å™¨
class TracedTwitterScraper:
    """å¸¦è¿½è¸ªçš„Twitteré‡‡é›†å™¨"""

    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
        self.meter = metrics.get_meter(__name__)

        # åˆ›å»ºæŒ‡æ ‡
        self.scrape_counter = self.meter.create_counter(
            "scraper_requests_total",
            description="Total number of scrape requests"
        )
        self.scrape_duration = self.meter.create_histogram(
            "scraper_duration_seconds",
            description="Duration of scrape requests"
        )
        self.scrape_errors = self.meter.create_counter(
            "scraper_errors_total",
            description="Total number of scrape errors"
        )

    @trace_async_function("scrape_user_profile")
    async def scrape_user_profile(self, username: str) -> Dict:
        """é‡‡é›†ç”¨æˆ·èµ„æ–™"""
        with self.tracer.start_as_current_span("scrape_user_profile") as span:
            span.set_attribute("user.username", username)

            start_time = time.time()
            try:
                # æ¨¡æ‹Ÿé‡‡é›†é€»è¾‘
                result = await self._perform_scraping(username)

                # è®°å½•æˆåŠŸæŒ‡æ ‡
                self.scrape_counter.add(1, {"type": "user_profile", "status": "success"})
                span.set_attribute("scrape.result.tweets_count", len(result.get("tweets", [])))

                return result

            except Exception as e:
                # è®°å½•é”™è¯¯æŒ‡æ ‡
                self.scrape_errors.add(1, {"type": "user_profile", "error": type(e).__name__})
                span.record_exception(e)
                raise
            finally:
                # è®°å½•æŒç»­æ—¶é—´
                duration = time.time() - start_time
                self.scrape_duration.record(duration, {"type": "user_profile"})

    async def _perform_scraping(self, username: str) -> Dict:
        """æ‰§è¡Œå®é™…çš„é‡‡é›†æ“ä½œ"""
        # è¿™é‡Œæ˜¯å®é™…çš„é‡‡é›†é€»è¾‘
        return {"username": username, "tweets": []}

# æ—¥å¿—é›†æˆ
class TracingLogHandler(logging.Handler):
    """å°†æ—¥å¿—é›†æˆåˆ°è¿½è¸ªä¸­"""

    def emit(self, record):
        current_span = trace.get_current_span()
        if current_span.is_recording():
            current_span.add_event(
                name="log",
                attributes={
                    "log.severity": record.levelname,
                    "log.message": record.getMessage(),
                    "log.logger": record.name,
                }
            )

# å®Œæ•´çš„é“¾è·¯è¿½è¸ªå®ç°
class DistributedTracing:
    """åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªç®¡ç†å™¨"""

    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
        self.meter = metrics.get_meter(__name__)

        # åˆ›å»ºè‡ªå®šä¹‰æŒ‡æ ‡
        self.operation_counter = self.meter.create_counter(
            "weget_operations_total",
            description="Total number of operations"
        )
        self.operation_duration = self.meter.create_histogram(
            "weget_operation_duration_seconds",
            description="Operation duration in seconds"
        )

    def trace_scraping_pipeline(self, operation_name: str):
        """è¿½è¸ªé‡‡é›†ç®¡é“è£…é¥°å™¨"""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                with self.tracer.start_as_current_span(operation_name) as span:
                    # è®¾ç½®åŸºç¡€å±æ€§
                    span.set_attribute("operation.name", operation_name)
                    span.set_attribute("operation.type", "scraping")
                    span.set_attribute("service.name", "weget")

                    start_time = time.time()

                    try:
                        # æ·»åŠ è¾“å…¥å‚æ•°
                        if args:
                            span.set_attribute("operation.args_count", len(args))
                        if kwargs:
                            for key, value in kwargs.items():
                                if isinstance(value, (str, int, float, bool)):
                                    span.set_attribute(f"operation.param.{key}", value)

                        # æ‰§è¡Œæ“ä½œ
                        result = await func(*args, **kwargs)

                        # è®°å½•æˆåŠŸ
                        span.set_status(trace.Status(trace.StatusCode.OK))
                        span.set_attribute("operation.status", "success")

                        if isinstance(result, dict):
                            span.set_attribute("operation.result.type", "dict")
                            span.set_attribute("operation.result.keys_count", len(result.keys()))
                        elif isinstance(result, list):
                            span.set_attribute("operation.result.type", "list")
                            span.set_attribute("operation.result.items_count", len(result))

                        # è®°å½•æŒ‡æ ‡
                        self.operation_counter.add(1, {
                            "operation": operation_name,
                            "status": "success"
                        })

                        return result

                    except Exception as e:
                        # è®°å½•é”™è¯¯
                        span.record_exception(e)
                        span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                        span.set_attribute("operation.status", "error")
                        span.set_attribute("operation.error.type", type(e).__name__)

                        # è®°å½•é”™è¯¯æŒ‡æ ‡
                        self.operation_counter.add(1, {
                            "operation": operation_name,
                            "status": "error",
                            "error_type": type(e).__name__
                        })

                        raise

                    finally:
                        # è®°å½•æŒç»­æ—¶é—´
                        duration = time.time() - start_time
                        span.set_attribute("operation.duration_seconds", duration)

                        self.operation_duration.record(duration, {
                            "operation": operation_name
                        })

            return wrapper
        return decorator

# å…¨å±€è¿½è¸ªå®ä¾‹
distributed_tracing = DistributedTracing()
```

**å¯è§‚æµ‹æ€§é…ç½®è¯´æ˜**:

**é‡è¦**: å¯è§‚æµ‹æ€§ç»„ä»¶å·²è¿ç§»åˆ° Helm Chart ç»Ÿä¸€ç®¡ç†ï¼Œé€šè¿‡ `observability.enabled=true` å¯ç”¨ã€‚

```bash
# å¯ç”¨å®Œæ•´å¯è§‚æµ‹æ€§æ ˆ
helm upgrade --install weget ./weget-chart \
  --set observability.enabled=true \
  --set observability.jaeger.enabled=true \
  --set observability.tempo.enabled=true \
  --set observability.grafana.enabled=true \
  --set observability.prometheus.enabled=true

# è®¿é—®ç›‘æ§ç•Œé¢
kubectl port-forward svc/weget-grafana 3000:3000    # Grafana
kubectl port-forward svc/weget-jaeger 16686:16686   # Jaeger UI
kubectl port-forward svc/weget-prometheus 9090:9090 # Prometheus
```

**å¯è§‚æµ‹æ€§ç‰¹æ€§**:
- **åˆ†å¸ƒå¼è¿½è¸ª**: Jaeger + Tempo å…¨é“¾è·¯è¿½è¸ª
- **æŒ‡æ ‡ç›‘æ§**: Prometheus + Grafana å®æ—¶ç›‘æ§
- **è‡ªåŠ¨å‘ç°**: Kubernetes æœåŠ¡è‡ªåŠ¨å‘ç°
- **å‘Šè­¦é›†æˆ**: PrometheusRule CRD è‡ªåŠ¨å‘Šè­¦

```yaml
# config/tempo.yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

ingester:
  trace_idle_period: 10s
  max_block_bytes: 1_000_000
  max_block_duration: 5m

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/traces
    wal:
      path: /tmp/tempo/wal
    pool:
      max_workers: 100
      queue_depth: 10000
```



### 18. æ™ºèƒ½åŒ–è¿ç»´ä¸é¢„æµ‹æ€§ç»´æŠ¤

#### é¢„æµ‹æ€§è´¦å·å¥åº·è¯„åˆ†ç³»ç»Ÿ
```python
# core/predictive_health.py
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import joblib
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional

class PredictiveAccountHealthScorer:
    """é¢„æµ‹æ€§è´¦å·å¥åº·è¯„åˆ†ç³»ç»Ÿ"""

    def __init__(self, redis_client, model_path: str = "models/account_health_model.pkl"):
        self.redis = redis_client
        self.model_path = model_path
        self.model = None
        self.feature_columns = [
            'account_age_days', 'total_requests', 'success_rate', 'failure_rate',
            'consecutive_failures', 'requests_per_hour', 'proxy_switches_per_day',
            'endpoint_diversity_score', 'read_write_ratio', 'proxy_reputation_score',
            'time_since_last_success', 'request_pattern_stability', 'peak_hour_activity'
        ]

    async def extract_account_features(self, account_id: str, lookback_hours: int = 24) -> Dict:
        """æå–è´¦å·ç‰¹å¾ç”¨äºé¢„æµ‹"""
        try:
            # è·å–è´¦å·ç”Ÿå‘½å‘¨æœŸæ•°æ®
            lifecycle_key = f"account:lifecycle:{account_id}"
            lifecycle_data = self.redis.get(lifecycle_key)

            if not lifecycle_data:
                return {}

            account_info = json.loads(lifecycle_data)

            # è®¡ç®—è´¦å·å¹´é¾„
            created_at = datetime.fromisoformat(account_info.get('created_at', ''))
            account_age_days = (datetime.utcnow() - created_at).days

            # è·å–æœ€è¿‘24å°æ—¶çš„è¯·æ±‚ç»Ÿè®¡
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=lookback_hours)

            request_stats = await self._get_request_statistics(account_id, start_time, end_time)
            proxy_stats = await self._get_proxy_statistics(account_id, start_time, end_time)

            features = {
                'account_age_days': account_age_days,
                'total_requests': account_info.get('total_requests', 0),
                'success_rate': account_info.get('successful_requests', 0) / max(account_info.get('total_requests', 1), 1),
                'failure_rate': account_info.get('failed_requests', 0) / max(account_info.get('total_requests', 1), 1),
                'consecutive_failures': request_stats.get('consecutive_failures', 0),
                'requests_per_hour': request_stats.get('requests_per_hour', 0),
                'proxy_switches_per_day': proxy_stats.get('switches_per_day', 0),
                'endpoint_diversity_score': request_stats.get('endpoint_diversity', 0),
                'read_write_ratio': request_stats.get('read_write_ratio', 1.0),
                'proxy_reputation_score': proxy_stats.get('reputation_score', 0.5),
                'time_since_last_success': request_stats.get('time_since_last_success', 0),
                'request_pattern_stability': request_stats.get('pattern_stability', 0.5),
                'peak_hour_activity': request_stats.get('peak_hour_ratio', 0.5)
            }

            return features

        except Exception as e:
            logger.error(f"Failed to extract features for account {account_id}: {str(e)}")
            return {}

    async def _get_request_statistics(self, account_id: str, start_time: datetime, end_time: datetime) -> Dict:
        """è·å–è¯·æ±‚ç»Ÿè®¡æ•°æ®"""
        # ä»Redisæ—¶åºæ•°æ®æˆ–æ—¥å¿—ä¸­è·å–è¯·æ±‚ç»Ÿè®¡
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»è¯¦ç»†çš„è¯·æ±‚æ—¥å¿—ä¸­è®¡ç®—

        stats = {
            'consecutive_failures': 0,
            'requests_per_hour': 0,
            'endpoint_diversity': 0.5,
            'read_write_ratio': 1.0,
            'time_since_last_success': 0,
            'pattern_stability': 0.5,
            'peak_hour_ratio': 0.5
        }

        # å®é™…å®ç°åº”è¯¥åˆ†æè¯·æ±‚æ—¥å¿—
        return stats

    async def _get_proxy_statistics(self, account_id: str, start_time: datetime, end_time: datetime) -> Dict:
        """è·å–ä»£ç†ä½¿ç”¨ç»Ÿè®¡"""
        stats = {
            'switches_per_day': 0,
            'reputation_score': 0.5
        }

        # å®é™…å®ç°åº”è¯¥åˆ†æä»£ç†ä½¿ç”¨è®°å½•
        return stats

    async def predict_account_health(self, account_id: str, prediction_hours: int = 6) -> Dict:
        """é¢„æµ‹è´¦å·åœ¨æœªæ¥Nå°æ—¶å†…çš„å¥åº·çŠ¶å†µ"""
        try:
            if not self.model:
                await self.load_model()

            features = await self.extract_account_features(account_id)

            if not features:
                return {'risk_score': 0.5, 'confidence': 0.0, 'recommendation': 'insufficient_data'}

            # å‡†å¤‡ç‰¹å¾å‘é‡
            feature_vector = np.array([[features.get(col, 0) for col in self.feature_columns]])

            # é¢„æµ‹é£é™©æ¦‚ç‡
            risk_probability = self.model.predict_proba(feature_vector)[0][1]  # è·å–æ­£ç±»æ¦‚ç‡
            confidence = max(self.model.predict_proba(feature_vector)[0]) - 0.5  # ç½®ä¿¡åº¦

            # ç”Ÿæˆå»ºè®®
            recommendation = self._generate_recommendation(risk_probability, features)

            prediction_result = {
                'account_id': account_id,
                'risk_score': float(risk_probability),
                'confidence': float(confidence),
                'recommendation': recommendation,
                'prediction_horizon_hours': prediction_hours,
                'predicted_at': datetime.utcnow().isoformat(),
                'features_used': features
            }

            # ä¿å­˜é¢„æµ‹ç»“æœ
            prediction_key = f"account:prediction:{account_id}"
            self.redis.setex(prediction_key, 3600 * prediction_hours, json.dumps(prediction_result))

            return prediction_result

        except Exception as e:
            logger.error(f"Failed to predict health for account {account_id}: {str(e)}")
            return {'risk_score': 0.5, 'confidence': 0.0, 'recommendation': 'prediction_failed'}

    def _generate_recommendation(self, risk_score: float, features: Dict) -> str:
        """åŸºäºé£é™©åˆ†æ•°å’Œç‰¹å¾ç”Ÿæˆå»ºè®®"""
        if risk_score > 0.8:
            return 'immediate_cooldown'
        elif risk_score > 0.6:
            return 'reduce_frequency'
        elif risk_score > 0.4:
            return 'monitor_closely'
        elif risk_score > 0.2:
            return 'normal_operation'
        else:
            return 'optimal_health'

    async def train_model(self, training_data_days: int = 30) -> Dict:
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        try:
            # æ”¶é›†è®­ç»ƒæ•°æ®
            training_data = await self._collect_training_data(training_data_days)

            if len(training_data) < 100:  # æœ€å°‘éœ€è¦100ä¸ªæ ·æœ¬
                return {'status': 'insufficient_data', 'samples': len(training_data)}

            df = pd.DataFrame(training_data)

            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            X = df[self.feature_columns].fillna(0)
            y = df['is_banned']  # 1è¡¨ç¤ºè¢«å°ç¦ï¼Œ0è¡¨ç¤ºå¥åº·

            # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # è®­ç»ƒæ¨¡å‹
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )

            self.model.fit(X_train, y_train)

            # è¯„ä¼°æ¨¡å‹
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)[:, 1]

            auc_score = roc_auc_score(y_test, y_pred_proba)
            classification_rep = classification_report(y_test, y_pred, output_dict=True)

            # ä¿å­˜æ¨¡å‹
            joblib.dump(self.model, self.model_path)

            # ä¿å­˜è®­ç»ƒç»“æœ
            training_result = {
                'training_date': datetime.utcnow().isoformat(),
                'samples_count': len(training_data),
                'auc_score': float(auc_score),
                'precision': float(classification_rep['1']['precision']),
                'recall': float(classification_rep['1']['recall']),
                'f1_score': float(classification_rep['1']['f1-score']),
                'feature_importance': dict(zip(self.feature_columns, self.model.feature_importances_))
            }

            self.redis.setex('model:training:result', 86400 * 7, json.dumps(training_result))

            logger.info(f"Model trained successfully. AUC: {auc_score:.3f}")
            return training_result

        except Exception as e:
            logger.error(f"Failed to train model: {str(e)}")
            return {'status': 'training_failed', 'error': str(e)}

    async def _collect_training_data(self, days: int) -> List[Dict]:
        """æ”¶é›†è®­ç»ƒæ•°æ®"""
        training_data = []

        # æ‰«ææ‰€æœ‰è´¦å·çš„å†å²æ•°æ®
        for key in self.redis.scan_iter(match="account:lifecycle:*"):
            account_id = key.decode().replace("account:lifecycle:", "")

            # è·å–è´¦å·å†å²çŠ¶æ€å˜åŒ–
            lifecycle_data = self.redis.get(key)
            if lifecycle_data:
                account_info = json.loads(lifecycle_data)

                # å¦‚æœè´¦å·æ›¾ç»è¢«å°ç¦ï¼Œä½œä¸ºæ­£æ ·æœ¬
                if account_info.get('status') == 'banned':
                    features = await self.extract_account_features(account_id)
                    if features:
                        features['is_banned'] = 1
                        training_data.append(features)

                # å¥åº·è´¦å·ä½œä¸ºè´Ÿæ ·æœ¬
                elif account_info.get('status') == 'active':
                    features = await self.extract_account_features(account_id)
                    if features:
                        features['is_banned'] = 0
                        training_data.append(features)

        return training_data

    async def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            self.model = joblib.load(self.model_path)
            logger.info("Predictive model loaded successfully")
        except Exception as e:
            logger.warning(f"Failed to load model: {str(e)}, using default model")
            # ä½¿ç”¨é»˜è®¤çš„ç®€å•æ¨¡å‹
            self.model = LogisticRegression()

    async def batch_predict_all_accounts(self) -> Dict:
        """æ‰¹é‡é¢„æµ‹æ‰€æœ‰æ´»è·ƒè´¦å·çš„å¥åº·çŠ¶å†µ"""
        predictions = {}

        for key in self.redis.scan_iter(match="account:lifecycle:*"):
            account_id = key.decode().replace("account:lifecycle:", "")

            lifecycle_data = self.redis.get(key)
            if lifecycle_data:
                account_info = json.loads(lifecycle_data)

                # åªé¢„æµ‹æ´»è·ƒè´¦å·
                if account_info.get('status') == 'active':
                    prediction = await self.predict_account_health(account_id)
                    predictions[account_id] = prediction

        return predictions

# å®šæ—¶ä»»åŠ¡ï¼šé¢„æµ‹æ€§ç»´æŠ¤
@app.task
def predictive_maintenance_task():
    """é¢„æµ‹æ€§ç»´æŠ¤å®šæ—¶ä»»åŠ¡"""
    try:
        predictor = PredictiveAccountHealthScorer(redis_client)

        # æ‰¹é‡é¢„æµ‹æ‰€æœ‰è´¦å·
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        predictions = loop.run_until_complete(predictor.batch_predict_all_accounts())

        # æ ¹æ®é¢„æµ‹ç»“æœé‡‡å–è¡ŒåŠ¨
        lifecycle_manager = AccountLifecycleManager(redis_client, secrets_manager)

        for account_id, prediction in predictions.items():
            risk_score = prediction.get('risk_score', 0.5)
            recommendation = prediction.get('recommendation', 'normal_operation')

            if recommendation == 'immediate_cooldown':
                loop.run_until_complete(
                    lifecycle_manager.transition_account_status(
                        account_id,
                        AccountStatus.COOLING_DOWN,
                        f"Predictive model: high risk ({risk_score:.3f})"
                    )
                )
            elif recommendation == 'reduce_frequency':
                # é™ä½è¯¥è´¦å·çš„ä½¿ç”¨é¢‘ç‡
                await _reduce_account_frequency(account_id, risk_score)

        loop.close()

        logger.info(f"Predictive maintenance completed for {len(predictions)} accounts")

    except Exception as e:
        logger.error(f"Predictive maintenance failed: {str(e)}")

async def _reduce_account_frequency(account_id: str, risk_score: float):
    """é™ä½è´¦å·ä½¿ç”¨é¢‘ç‡"""
    # åœ¨Cookieç®¡ç†å™¨ä¸­è®¾ç½®è¯¥è´¦å·çš„ä½¿ç”¨æƒé‡
    frequency_key = f"account:frequency:{account_id}"

    # æ ¹æ®é£é™©åˆ†æ•°è°ƒæ•´é¢‘ç‡æƒé‡
    frequency_weight = max(0.1, 1.0 - risk_score)

    redis_client.setex(frequency_key, 3600, str(frequency_weight))

    logger.info(f"Reduced frequency for account {account_id} to {frequency_weight:.2f}")
```

#### æ™ºèƒ½æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿ
```python
# core/intelligent_dlq.py
import re
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum

class ErrorCategory(Enum):
    KNOWN_FIXABLE = "known_fixable"
    KNOWN_UNFIXABLE = "known_unfixable"
    UNKNOWN = "unknown"
    PLATFORM_CHANGE = "platform_change"

class IntelligentDLQManager(DeadLetterQueueManager):
    """æ™ºèƒ½æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†å™¨"""

    def __init__(self, redis_client):
        super().__init__(redis_client)
        self.error_patterns = self._load_error_patterns()
        self.auto_fix_handlers = self._register_auto_fix_handlers()

    def _load_error_patterns(self) -> Dict:
        """åŠ è½½é”™è¯¯æ¨¡å¼é…ç½®"""
        return {
            # å·²çŸ¥å¯ä¿®å¤é”™è¯¯
            'missing_optional_field': {
                'pattern': r'field required.*(\w+)',
                'category': ErrorCategory.KNOWN_FIXABLE,
                'auto_fix': True,
                'handler': 'fix_missing_field'
            },
            'invalid_date_format': {
                'pattern': r'invalid datetime format.*(\d{4}-\d{2}-\d{2})',
                'category': ErrorCategory.KNOWN_FIXABLE,
                'auto_fix': True,
                'handler': 'fix_date_format'
            },
            'unexpected_field_type': {
                'pattern': r'value is not a valid (\w+)',
                'category': ErrorCategory.KNOWN_FIXABLE,
                'auto_fix': True,
                'handler': 'fix_field_type'
            },

            # å·²çŸ¥ä¸å¯ä¿®å¤é”™è¯¯
            'deleted_content': {
                'pattern': r'(deleted|removed|not found)',
                'category': ErrorCategory.KNOWN_UNFIXABLE,
                'auto_fix': False,
                'handler': None
            },
            'private_account': {
                'pattern': r'(private|protected|suspended)',
                'category': ErrorCategory.KNOWN_UNFIXABLE,
                'auto_fix': False,
                'handler': None
            },

            # å¹³å°å˜åŒ–æŒ‡ç¤ºå™¨
            'new_field_structure': {
                'pattern': r'KeyError.*(\w+).*not found',
                'category': ErrorCategory.PLATFORM_CHANGE,
                'auto_fix': False,
                'handler': 'analyze_platform_change'
            }
        }

    def _register_auto_fix_handlers(self) -> Dict:
        """æ³¨å†Œè‡ªåŠ¨ä¿®å¤å¤„ç†å™¨"""
        return {
            'fix_missing_field': self._fix_missing_field,
            'fix_date_format': self._fix_date_format,
            'fix_field_type': self._fix_field_type,
            'analyze_platform_change': self._analyze_platform_change
        }

    async def categorize_error(self, error_message: str) -> Tuple[ErrorCategory, Optional[str]]:
        """åˆ†ç±»é”™è¯¯ä¿¡æ¯"""
        for pattern_name, pattern_config in self.error_patterns.items():
            if re.search(pattern_config['pattern'], error_message, re.IGNORECASE):
                return pattern_config['category'], pattern_name

        return ErrorCategory.UNKNOWN, None

    async def intelligent_dlq_processing(self) -> Dict:
        """æ™ºèƒ½DLQå¤„ç†"""
        try:
            processing_stats = {
                'total_processed': 0,
                'auto_fixed': 0,
                'categorized': 0,
                'alerts_triggered': 0,
                'categories': {category.value: 0 for category in ErrorCategory}
            }

            # è·å–DLQä¸­çš„æ‰€æœ‰æ¶ˆæ¯
            messages = await self.redis.xrevrange(self.dlq_stream, count=1000)

            for msg_id, fields in messages:
                message_data = {k.decode(): v.decode() for k, v in fields.items()}
                error_reason = message_data.get('error_reason', '')

                # åˆ†ç±»é”™è¯¯
                category, pattern_name = await self.categorize_error(error_reason)
                processing_stats['categories'][category.value] += 1
                processing_stats['categorized'] += 1

                # å°è¯•è‡ªåŠ¨ä¿®å¤
                if category == ErrorCategory.KNOWN_FIXABLE and pattern_name:
                    pattern_config = self.error_patterns[pattern_name]
                    if pattern_config['auto_fix']:
                        success = await self._attempt_auto_fix(message_data, pattern_config)
                        if success:
                            processing_stats['auto_fixed'] += 1
                            # ä»DLQä¸­åˆ é™¤å·²ä¿®å¤çš„æ¶ˆæ¯
                            await self.redis.xdel(self.dlq_stream, msg_id)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å‘Šè­¦
                elif category == ErrorCategory.PLATFORM_CHANGE:
                    await self._trigger_platform_change_alert(message_data, error_reason)
                    processing_stats['alerts_triggered'] += 1

                processing_stats['total_processed'] += 1

            # æ£€æŸ¥æœªçŸ¥é”™è¯¯çš„æ¿€å¢
            await self._check_unknown_error_surge(processing_stats)

            # ä¿å­˜å¤„ç†ç»Ÿè®¡
            stats_key = f"dlq:processing:stats:{datetime.utcnow().strftime('%Y%m%d_%H')}"
            self.redis.setex(stats_key, 86400, json.dumps(processing_stats))

            return processing_stats

        except Exception as e:
            logger.error(f"Intelligent DLQ processing failed: {str(e)}")
            return {'status': 'failed', 'error': str(e)}

    async def _attempt_auto_fix(self, message_data: Dict, pattern_config: Dict) -> bool:
        """å°è¯•è‡ªåŠ¨ä¿®å¤é”™è¯¯"""
        try:
            handler_name = pattern_config['handler']
            if handler_name in self.auto_fix_handlers:
                handler = self.auto_fix_handlers[handler_name]

                # è§£æåŸå§‹æ•°æ®
                raw_data = json.loads(message_data['raw_data'])

                # å°è¯•ä¿®å¤
                fixed_data = await handler(raw_data, message_data['error_reason'])

                if fixed_data:
                    # é‡æ–°éªŒè¯ä¿®å¤åçš„æ•°æ®
                    data_type = message_data['data_type']
                    pipeline = DataIngestionPipeline(self.redis)

                    validated_data = await pipeline._validate_data(data_type, fixed_data)

                    if validated_data:
                        # é‡æ–°åŠ å…¥å¤„ç†é˜Ÿåˆ—
                        source_info = json.loads(message_data['source_info'])
                        await pipeline._enqueue_for_processing(
                            message_data['message_id'],
                            data_type,
                            validated_data,
                            source_info
                        )

                        logger.info(f"Auto-fixed DLQ message: {message_data['message_id']}")
                        return True

        except Exception as e:
            logger.warning(f"Auto-fix failed for message {message_data.get('message_id', 'unknown')}: {str(e)}")

        return False

    async def _fix_missing_field(self, raw_data: Dict, error_message: str) -> Optional[Dict]:
        """ä¿®å¤ç¼ºå¤±å­—æ®µé”™è¯¯"""
        try:
            # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–ç¼ºå¤±çš„å­—æ®µå
            match = re.search(r'field required.*(\w+)', error_message)
            if match:
                missing_field = match.group(1)

                # æ ¹æ®å­—æ®µç±»å‹è®¾ç½®é»˜è®¤å€¼
                default_values = {
                    'view_count': 0,
                    'quote_count': 0,
                    'bookmark_count': 0,
                    'community_notes': [],
                    'edit_history': [],
                    'conversation_id': '',
                    'source_label': '',
                    'possibly_sensitive': False
                }

                if missing_field in default_values:
                    raw_data[missing_field] = default_values[missing_field]
                    logger.info(f"Added missing field '{missing_field}' with default value")
                    return raw_data

        except Exception as e:
            logger.warning(f"Failed to fix missing field: {str(e)}")

        return None

    async def _fix_date_format(self, raw_data: Dict, error_message: str) -> Optional[Dict]:
        """ä¿®å¤æ—¥æœŸæ ¼å¼é”™è¯¯"""
        try:
            # æŸ¥æ‰¾å¹¶ä¿®å¤æ—¥æœŸå­—æ®µ
            date_fields = ['created_at', 'updated_at', 'last_seen']

            for field in date_fields:
                if field in raw_data:
                    date_value = raw_data[field]

                    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼è½¬æ¢
                    fixed_date = self._normalize_date_format(date_value)
                    if fixed_date:
                        raw_data[field] = fixed_date
                        logger.info(f"Fixed date format for field '{field}'")
                        return raw_data

        except Exception as e:
            logger.warning(f"Failed to fix date format: {str(e)}")

        return None

    def _normalize_date_format(self, date_value: str) -> Optional[str]:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        import dateutil.parser

        try:
            # ä½¿ç”¨dateutilè§£æå„ç§æ—¥æœŸæ ¼å¼
            parsed_date = dateutil.parser.parse(date_value)
            return parsed_date.isoformat()
        except:
            return None

    async def _fix_field_type(self, raw_data: Dict, error_message: str) -> Optional[Dict]:
        """ä¿®å¤å­—æ®µç±»å‹é”™è¯¯"""
        try:
            # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–å­—æ®µä¿¡æ¯
            match = re.search(r'value is not a valid (\w+)', error_message)
            if match:
                expected_type = match.group(1)

                # å°è¯•ç±»å‹è½¬æ¢
                for field, value in raw_data.items():
                    if isinstance(value, str) and expected_type == 'integer':
                        try:
                            raw_data[field] = int(value)
                            return raw_data
                        except ValueError:
                            continue
                    elif isinstance(value, (int, float)) and expected_type == 'string':
                        raw_data[field] = str(value)
                        return raw_data

        except Exception as e:
            logger.warning(f"Failed to fix field type: {str(e)}")

        return None

    async def _analyze_platform_change(self, raw_data: Dict, error_message: str) -> Optional[Dict]:
        """åˆ†æå¹³å°å˜åŒ–"""
        # è¿™ä¸ªå¤„ç†å™¨ä¸»è¦ç”¨äºè®°å½•å’Œåˆ†æï¼Œä¸è¿›è¡Œä¿®å¤
        platform_change_info = {
            'detected_at': datetime.utcnow().isoformat(),
            'error_message': error_message,
            'raw_data_sample': str(raw_data)[:500],
            'data_type': 'platform_change_detection'
        }

        # ä¿å­˜å¹³å°å˜åŒ–æ£€æµ‹ç»“æœ
        change_key = f"platform:change:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        self.redis.setex(change_key, 86400 * 7, json.dumps(platform_change_info))

        logger.warning(f"Platform change detected: {error_message}")
        return None

    async def _trigger_platform_change_alert(self, message_data: Dict, error_reason: str):
        """è§¦å‘å¹³å°å˜åŒ–å‘Šè­¦"""
        alert_data = {
            'alert_type': 'platform_change',
            'severity': 'high',
            'message': f"Potential platform change detected: {error_reason}",
            'sample_data': message_data,
            'triggered_at': datetime.utcnow().isoformat(),
            'requires_human_intervention': True
        }

        # å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
        alert_key = f"alert:platform_change:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        self.redis.setex(alert_key, 86400, json.dumps(alert_data))

        # å¦‚æœé…ç½®äº†é‚®ä»¶å‘Šè­¦ï¼Œå‘é€é‚®ä»¶
        if hasattr(self, 'email_alerter'):
            await self.email_alerter.send_critical_alert(alert_data)

    async def _check_unknown_error_surge(self, processing_stats: Dict):
        """æ£€æŸ¥æœªçŸ¥é”™è¯¯æ¿€å¢"""
        unknown_count = processing_stats['categories'][ErrorCategory.UNKNOWN.value]
        total_count = processing_stats['total_processed']

        if total_count > 0:
            unknown_ratio = unknown_count / total_count

            # å¦‚æœæœªçŸ¥é”™è¯¯æ¯”ä¾‹è¶…è¿‡30%ï¼Œè§¦å‘å‘Šè­¦
            if unknown_ratio > 0.3 and unknown_count > 10:
                surge_alert = {
                    'alert_type': 'unknown_error_surge',
                    'severity': 'critical',
                    'message': f"Unknown error surge detected: {unknown_count}/{total_count} ({unknown_ratio:.1%})",
                    'stats': processing_stats,
                    'triggered_at': datetime.utcnow().isoformat(),
                    'requires_immediate_attention': True
                }

                alert_key = f"alert:error_surge:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
                self.redis.setex(alert_key, 86400, json.dumps(surge_alert))

                logger.critical(f"Unknown error surge detected: {unknown_ratio:.1%}")

# å®šæ—¶ä»»åŠ¡ï¼šæ™ºèƒ½DLQå¤„ç†
@app.task
def intelligent_dlq_processing_task():
    """æ™ºèƒ½DLQå¤„ç†å®šæ—¶ä»»åŠ¡"""
    try:
        dlq_manager = IntelligentDLQManager(redis_client)

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        stats = loop.run_until_complete(dlq_manager.intelligent_dlq_processing())

        loop.close()

        logger.info(f"Intelligent DLQ processing completed: {stats}")

        return stats

    except Exception as e:
        logger.error(f"Intelligent DLQ processing task failed: {str(e)}")
        return {'status': 'failed', 'error': str(e)}
```

### 19. æ•°æ®æº¯æºä¸Schemaç‰ˆæœ¬ç®¡ç†

#### å¢å¼ºçš„æ•°æ®æ¨¡å‹ï¼ˆå¸¦æº¯æºä¿¡æ¯ï¼‰
```python
# models/enhanced_models.py
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

class DataProvenance(BaseModel):
    """æ•°æ®æº¯æºä¿¡æ¯"""
    scraped_by_version: str = Field(..., description="é‡‡é›†å™¨ä»£ç ç‰ˆæœ¬")
    account_id: str = Field(..., description="ä½¿ç”¨çš„é‡‡é›†è´¦å·ID")
    proxy_id: Optional[str] = Field(None, description="ä½¿ç”¨çš„ä»£ç†ID")
    api_config_version: str = Field(..., description="GraphQLè‡ªé€‚åº”é…ç½®ç‰ˆæœ¬")
    ingestion_timestamp: str = Field(..., description="è¿›å…¥æ•°æ®ç®¡é“çš„æ—¶é—´")
    task_id: str = Field(..., description="å¯¹åº”çš„Celeryä»»åŠ¡ID")
    extraction_method: str = Field(..., description="æ•°æ®æå–æ–¹æ³•: dom_parsing, api_intercept, hybrid")
    page_url: Optional[str] = Field(None, description="é‡‡é›†çš„é¡µé¢URL")
    browser_session_id: Optional[str] = Field(None, description="æµè§ˆå™¨ä¼šè¯ID")
    data_quality_score: float = Field(default=1.0, description="æ•°æ®è´¨é‡è¯„åˆ† 0-1")

class SchemaMetadata(BaseModel):
    """Schemaå…ƒæ•°æ®"""
    schema_version: str = Field(..., description="æ•°æ®æ¨¡å¼ç‰ˆæœ¬")
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    created_at: str = Field(..., description="è®°å½•åˆ›å»ºæ—¶é—´")
    updated_at: Optional[str] = Field(None, description="è®°å½•æ›´æ–°æ—¶é—´")
    migration_history: List[str] = Field(default_factory=list, description="è¿ç§»å†å²")

class EnhancedTwitterUserModel(BaseModel):
    """å¢å¼ºçš„Twitterç”¨æˆ·æ¨¡å‹"""
    # åŸæœ‰å­—æ®µ
    user_id: str = Field(..., regex=r'^\d+$')
    username: str = Field(..., min_length=1, max_length=15)
    display_name: str = Field(..., max_length=50)
    followers_count: int = Field(..., ge=0)
    following_count: int = Field(..., ge=0)
    tweet_count: int = Field(..., ge=0)
    verified: bool = False
    created_at: str
    description: Optional[str] = Field(None, max_length=160)
    location: Optional[str] = Field(None, max_length=30)
    profile_image_url: Optional[str] = None

    # æ–°å¢å­—æ®µ
    verification_type: str = Field(default="none", description="éªŒè¯ç±»å‹: none, legacy, blue, business")
    is_business_account: bool = Field(default=False)
    business_category: List[str] = Field(default_factory=list)
    subscription_tier: Optional[str] = Field(None, description="è®¢é˜…ç­‰çº§")

    # å…ƒæ•°æ®å­—æ®µ
    _provenance: DataProvenance = Field(..., description="æ•°æ®æº¯æºä¿¡æ¯")
    _schema: SchemaMetadata = Field(..., description="Schemaå…ƒæ•°æ®")

    class Config:
        schema_extra = {
            "example": {
                "user_id": "123456789",
                "username": "example_user",
                "display_name": "Example User",
                "_provenance": {
                    "scraped_by_version": "v2.1.0",
                    "account_id": "scraper_001",
                    "api_config_version": "20250628_093000"
                },
                "_schema": {
                    "schema_version": "2.1",
                    "model_name": "EnhancedTwitterUserModel"
                }
            }
        }

class EnhancedTwitterTweetModel(BaseModel):
    """å¢å¼ºçš„Twitteræ¨æ–‡æ¨¡å‹"""
    # åŸæœ‰å­—æ®µ
    tweet_id: str = Field(..., regex=r'^\d+$')
    user_id: str = Field(..., regex=r'^\d+$')
    content: str = Field(..., max_length=280)
    created_at: str
    retweet_count: int = Field(..., ge=0)
    favorite_count: int = Field(..., ge=0)
    reply_count: int = Field(..., ge=0)
    quote_count: int = Field(..., ge=0)
    view_count: Optional[int] = Field(None, ge=0)
    media: List[Dict] = []
    hashtags: List[str] = []
    urls: List[str] = []
    user_mentions: List[str] = []
    is_retweet: bool = False
    is_quote: bool = False
    lang: Optional[str] = None
    source: Optional[str] = None

    # æ–°å¢å­—æ®µ
    community_notes: List[Dict] = Field(default_factory=list, description="ç¤¾åŒºç¬”è®°")
    edit_history: List[Dict] = Field(default_factory=list, description="ç¼–è¾‘å†å²")
    conversation_id: Optional[str] = Field(None, description="å¯¹è¯ID")
    bookmark_count: int = Field(default=0, ge=0, description="æ”¶è—æ•°")
    geo_coordinates: List[float] = Field(default_factory=list, description="åœ°ç†åæ ‡")
    possibly_sensitive: bool = Field(default=False, description="å¯èƒ½æ•æ„Ÿå†…å®¹")

    # å…ƒæ•°æ®å­—æ®µ
    _provenance: DataProvenance = Field(..., description="æ•°æ®æº¯æºä¿¡æ¯")
    _schema: SchemaMetadata = Field(..., description="Schemaå…ƒæ•°æ®")

class DataVersionManager:
    """æ•°æ®ç‰ˆæœ¬ç®¡ç†å™¨"""

    CURRENT_SCHEMA_VERSIONS = {
        'user': '2.1',
        'tweet': '2.1',
        'reply': '1.2'
    }

    def __init__(self, redis_client):
        self.redis = redis_client

    def create_provenance(self,
                         account_id: str,
                         task_id: str,
                         extraction_method: str,
                         proxy_id: Optional[str] = None,
                         page_url: Optional[str] = None,
                         browser_session_id: Optional[str] = None) -> DataProvenance:
        """åˆ›å»ºæ•°æ®æº¯æºä¿¡æ¯"""

        # è·å–å½“å‰ä»£ç ç‰ˆæœ¬
        code_version = self._get_current_code_version()

        # è·å–APIé…ç½®ç‰ˆæœ¬
        api_config_version = self._get_api_config_version()

        return DataProvenance(
            scraped_by_version=code_version,
            account_id=account_id,
            proxy_id=proxy_id,
            api_config_version=api_config_version,
            ingestion_timestamp=datetime.utcnow().isoformat(),
            task_id=task_id,
            extraction_method=extraction_method,
            page_url=page_url,
            browser_session_id=browser_session_id,
            data_quality_score=1.0  # é»˜è®¤æ»¡åˆ†ï¼Œåç»­å¯ä»¥æ ¹æ®éªŒè¯ç»“æœè°ƒæ•´
        )

    def create_schema_metadata(self, model_name: str, data_type: str) -> SchemaMetadata:
        """åˆ›å»ºSchemaå…ƒæ•°æ®"""
        return SchemaMetadata(
            schema_version=self.CURRENT_SCHEMA_VERSIONS.get(data_type, '1.0'),
            model_name=model_name,
            created_at=datetime.utcnow().isoformat(),
            migration_history=[]
        )

    def _get_current_code_version(self) -> str:
        """è·å–å½“å‰ä»£ç ç‰ˆæœ¬"""
        # ä»ç¯å¢ƒå˜é‡æˆ–ç‰ˆæœ¬æ–‡ä»¶ä¸­è·å–
        import os
        return os.getenv('WEGET_VERSION', 'v2.1.0')

    def _get_api_config_version(self) -> str:
        """è·å–APIé…ç½®ç‰ˆæœ¬"""
        api_config = self.redis.get('twitter:api:config')
        if api_config:
            config_data = json.loads(api_config)
            return config_data.get('version', 'unknown')
        return 'unknown'

    async def migrate_data_schema(self, from_version: str, to_version: str, data_type: str) -> Dict:
        """æ•°æ®Schemaè¿ç§»"""
        migration_stats = {
            'total_records': 0,
            'migrated_records': 0,
            'failed_records': 0,
            'migration_start': datetime.utcnow().isoformat()
        }

        try:
            # è·å–éœ€è¦è¿ç§»çš„æ•°æ®
            collection_name = f"{data_type}s"  # users, tweets, replies

            # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥è¿æ¥MongoDB
            # æŸ¥æ‰¾æŒ‡å®šç‰ˆæœ¬çš„æ•°æ®
            query = {"_schema.schema_version": from_version}

            # æ¨¡æ‹Ÿè¿ç§»è¿‡ç¨‹
            migration_rules = self._get_migration_rules(from_version, to_version, data_type)

            # æ‰¹é‡å¤„ç†æ•°æ®
            batch_size = 1000
            processed = 0

            # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨MongoDBçš„èšåˆç®¡é“è¿›è¡Œæ‰¹é‡æ›´æ–°
            logger.info(f"Starting migration from {from_version} to {to_version} for {data_type}")

            migration_stats['migration_end'] = datetime.utcnow().isoformat()
            migration_stats['migration_rules'] = migration_rules

            return migration_stats

        except Exception as e:
            logger.error(f"Schema migration failed: {str(e)}")
            migration_stats['error'] = str(e)
            return migration_stats

    def _get_migration_rules(self, from_version: str, to_version: str, data_type: str) -> List[Dict]:
        """è·å–è¿ç§»è§„åˆ™"""
        migration_rules = {
            ('2.0', '2.1', 'user'): [
                {
                    'action': 'add_field',
                    'field': 'subscription_tier',
                    'default_value': None
                },
                {
                    'action': 'add_field',
                    'field': 'verification_type',
                    'default_value': 'none'
                }
            ],
            ('2.0', '2.1', 'tweet'): [
                {
                    'action': 'add_field',
                    'field': 'community_notes',
                    'default_value': []
                },
                {
                    'action': 'add_field',
                    'field': 'bookmark_count',
                    'default_value': 0
                }
            ]
        }

        return migration_rules.get((from_version, to_version, data_type), [])

# é›†æˆåˆ°æ•°æ®ç®¡ç†å™¨
class ProvenanceAwareDataManager(EnhancedDataManager):
    """å¸¦æº¯æºä¿¡æ¯çš„æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, mongodb_client, neo4j_driver, redis_client):
        super().__init__(mongodb_client, neo4j_driver, redis_client)
        self.version_manager = DataVersionManager(redis_client)

    async def save_user_with_provenance(self,
                                      raw_user_data: Dict,
                                      account_id: str,
                                      task_id: str,
                                      extraction_method: str,
                                      **provenance_kwargs) -> bool:
        """ä¿å­˜å¸¦æº¯æºä¿¡æ¯çš„ç”¨æˆ·æ•°æ®"""
        try:
            # åˆ›å»ºæº¯æºä¿¡æ¯
            provenance = self.version_manager.create_provenance(
                account_id=account_id,
                task_id=task_id,
                extraction_method=extraction_method,
                **provenance_kwargs
            )

            # åˆ›å»ºSchemaå…ƒæ•°æ®
            schema_metadata = self.version_manager.create_schema_metadata(
                model_name="EnhancedTwitterUserModel",
                data_type="user"
            )

            # æ·»åŠ å…ƒæ•°æ®åˆ°åŸå§‹æ•°æ®
            enhanced_data = raw_user_data.copy()
            enhanced_data['_provenance'] = provenance.dict()
            enhanced_data['_schema'] = schema_metadata.dict()

            # éªŒè¯å¢å¼ºåçš„æ•°æ®
            user_model = EnhancedTwitterUserModel(**enhanced_data)

            # ä¿å­˜åˆ°MongoDB
            await self.save_user_to_mongo(user_model.dict())

            # è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡
            await self._record_data_quality_with_provenance('user', True, provenance)

            return True

        except Exception as e:
            logger.error(f"Failed to save user with provenance: {str(e)}")
            await self._record_data_quality_with_provenance('user', False, None)
            return False

    async def save_tweet_with_provenance(self,
                                       raw_tweet_data: Dict,
                                       account_id: str,
                                       task_id: str,
                                       extraction_method: str,
                                       **provenance_kwargs) -> bool:
        """ä¿å­˜å¸¦æº¯æºä¿¡æ¯çš„æ¨æ–‡æ•°æ®"""
        try:
            provenance = self.version_manager.create_provenance(
                account_id=account_id,
                task_id=task_id,
                extraction_method=extraction_method,
                **provenance_kwargs
            )

            schema_metadata = self.version_manager.create_schema_metadata(
                model_name="EnhancedTwitterTweetModel",
                data_type="tweet"
            )

            enhanced_data = raw_tweet_data.copy()
            enhanced_data['_provenance'] = provenance.dict()
            enhanced_data['_schema'] = schema_metadata.dict()

            tweet_model = EnhancedTwitterTweetModel(**enhanced_data)

            await self.save_tweet_to_mongo(tweet_model.dict())
            await self._record_data_quality_with_provenance('tweet', True, provenance)

            return True

        except Exception as e:
            logger.error(f"Failed to save tweet with provenance: {str(e)}")
            await self._record_data_quality_with_provenance('tweet', False, None)
            return False

    async def _record_data_quality_with_provenance(self,
                                                 data_type: str,
                                                 is_valid: bool,
                                                 provenance: Optional[DataProvenance]):
        """è®°å½•å¸¦æº¯æºä¿¡æ¯çš„æ•°æ®è´¨é‡æŒ‡æ ‡"""
        date_key = datetime.utcnow().strftime('%Y-%m-%d')
        quality_key = f"data_quality:{data_type}:{date_key}"

        # åŸºç¡€è´¨é‡è®¡æ•°
        field = 'valid_count' if is_valid else 'invalid_count'
        self.redis.hincrby(quality_key, field, 1)

        # å¦‚æœæœ‰æº¯æºä¿¡æ¯ï¼Œè®°å½•æ›´è¯¦ç»†çš„ç»Ÿè®¡
        if provenance:
            # æŒ‰è´¦å·ç»Ÿè®¡
            account_key = f"data_quality:by_account:{provenance.account_id}:{date_key}"
            self.redis.hincrby(account_key, field, 1)

            # æŒ‰ä»£ç†ç»Ÿè®¡
            if provenance.proxy_id:
                proxy_key = f"data_quality:by_proxy:{provenance.proxy_id}:{date_key}"
                self.redis.hincrby(proxy_key, field, 1)

            # æŒ‰æå–æ–¹æ³•ç»Ÿè®¡
            method_key = f"data_quality:by_method:{provenance.extraction_method}:{date_key}"
            self.redis.hincrby(method_key, field, 1)

        self.redis.expire(quality_key, 86400 * 30)  # ä¿ç•™30å¤©

    async def query_data_by_provenance(self,
                                     account_id: Optional[str] = None,
                                     proxy_id: Optional[str] = None,
                                     task_id: Optional[str] = None,
                                     date_range: Optional[Tuple[str, str]] = None) -> List[Dict]:
        """æ ¹æ®æº¯æºä¿¡æ¯æŸ¥è¯¢æ•°æ®"""
        query = {}

        if account_id:
            query['_provenance.account_id'] = account_id
        if proxy_id:
            query['_provenance.proxy_id'] = proxy_id
        if task_id:
            query['_provenance.task_id'] = task_id
        if date_range:
            query['_provenance.ingestion_timestamp'] = {
                '$gte': date_range[0],
                '$lte': date_range[1]
            }

        # è¿™é‡Œåº”è¯¥å®é™…æŸ¥è¯¢MongoDB
        # è¿”å›åŒ¹é…çš„æ–‡æ¡£
        return []
```

### 20. å®¡è®¡æ—¥å¿—ä¸æƒé™æ§åˆ¶ç³»ç»Ÿ

#### å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
```python
# core/audit_system.py
from enum import Enum
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
import hashlib

class AuditAction(Enum):
    """å®¡è®¡åŠ¨ä½œç±»å‹"""
    JOB_SUBMIT = "job_submit"
    JOB_CANCEL = "job_cancel"
    DATA_QUERY = "data_query"
    SYSTEM_CONFIG = "system_config"
    USER_LOGIN = "user_login"
    USER_LOGOUT = "user_logout"
    ADMIN_ACTION = "admin_action"
    DATA_EXPORT = "data_export"
    ACCOUNT_MANAGE = "account_manage"
    PROXY_MANAGE = "proxy_manage"

class AuditSeverity(Enum):
    """å®¡è®¡ä¸¥é‡çº§åˆ«"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AuditLogger:
    """å®¡è®¡æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, mongodb_client, redis_client):
        self.mongo = mongodb_client
        self.redis = redis_client
        self.audit_collection = self.mongo.weget.audit_logs

        # ç¡®ä¿å®¡è®¡æ—¥å¿—é›†åˆçš„ç´¢å¼•
        self._ensure_audit_indexes()

    def _ensure_audit_indexes(self):
        """ç¡®ä¿å®¡è®¡æ—¥å¿—çš„ç´¢å¼•"""
        try:
            # åˆ›å»ºå¤åˆç´¢å¼•ä»¥ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
            self.audit_collection.create_index([
                ("timestamp", -1),
                ("user_id", 1),
                ("action", 1)
            ])

            self.audit_collection.create_index([
                ("ip_address", 1),
                ("timestamp", -1)
            ])

            self.audit_collection.create_index([
                ("session_id", 1)
            ])

        except Exception as e:
            logger.warning(f"Failed to create audit indexes: {str(e)}")

    async def log_action(self,
                        user_id: str,
                        action: AuditAction,
                        resource: str,
                        details: Dict[str, Any],
                        ip_address: str,
                        user_agent: str,
                        session_id: Optional[str] = None,
                        severity: AuditSeverity = AuditSeverity.MEDIUM) -> str:
        """è®°å½•å®¡è®¡æ—¥å¿—"""

        audit_id = self._generate_audit_id()
        timestamp = datetime.utcnow()

        audit_record = {
            'audit_id': audit_id,
            'timestamp': timestamp,
            'user_id': user_id,
            'action': action.value,
            'resource': resource,
            'details': details,
            'ip_address': ip_address,
            'user_agent': user_agent,
            'session_id': session_id,
            'severity': severity.value,
            'checksum': self._calculate_checksum({
                'audit_id': audit_id,
                'timestamp': timestamp.isoformat(),
                'user_id': user_id,
                'action': action.value,
                'resource': resource
            })
        }

        try:
            # ä¿å­˜åˆ°MongoDBï¼ˆé˜²ç¯¡æ”¹å­˜å‚¨ï¼‰
            await self.audit_collection.insert_one(audit_record)

            # åŒæ—¶ä¿å­˜åˆ°Redisç”¨äºå®æ—¶ç›‘æ§
            redis_key = f"audit:recent:{timestamp.strftime('%Y%m%d_%H')}"
            self.redis.lpush(redis_key, json.dumps(audit_record, default=str))
            self.redis.expire(redis_key, 86400)  # 24å°æ—¶è¿‡æœŸ

            # é«˜ä¸¥é‡çº§åˆ«çš„å®¡è®¡äº‹ä»¶ç«‹å³è§¦å‘å‘Šè­¦
            if severity in [AuditSeverity.HIGH, AuditSeverity.CRITICAL]:
                await self._trigger_audit_alert(audit_record)

            logger.info(f"Audit logged: {action.value} by {user_id}")
            return audit_id

        except Exception as e:
            logger.error(f"Failed to log audit action: {str(e)}")
            # å®¡è®¡æ—¥å¿—å¤±è´¥æ˜¯ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å‘Šè­¦
            await self._handle_audit_failure(audit_record, str(e))
            raise

    def _generate_audit_id(self) -> str:
        """ç”Ÿæˆå®¡è®¡ID"""
        import uuid
        return str(uuid.uuid4())

    def _calculate_checksum(self, data: Dict) -> str:
        """è®¡ç®—æ•°æ®æ ¡éªŒå’Œä»¥é˜²ç¯¡æ”¹"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()

    async def _trigger_audit_alert(self, audit_record: Dict):
        """è§¦å‘å®¡è®¡å‘Šè­¦"""
        alert_data = {
            'alert_type': 'high_severity_audit',
            'audit_record': audit_record,
            'triggered_at': datetime.utcnow().isoformat()
        }

        alert_key = f"alert:audit:{audit_record['audit_id']}"
        self.redis.setex(alert_key, 86400, json.dumps(alert_data, default=str))

    async def _handle_audit_failure(self, audit_record: Dict, error: str):
        """å¤„ç†å®¡è®¡æ—¥å¿—å¤±è´¥"""
        failure_record = {
            'failed_audit': audit_record,
            'error': error,
            'failure_time': datetime.utcnow().isoformat()
        }

        # å°è¯•ä¿å­˜åˆ°å¤‡ç”¨å­˜å‚¨
        failure_key = f"audit:failure:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        self.redis.setex(failure_key, 86400 * 7, json.dumps(failure_record, default=str))

    async def query_audit_logs(self,
                              user_id: Optional[str] = None,
                              action: Optional[AuditAction] = None,
                              start_time: Optional[datetime] = None,
                              end_time: Optional[datetime] = None,
                              ip_address: Optional[str] = None,
                              limit: int = 100) -> List[Dict]:
        """æŸ¥è¯¢å®¡è®¡æ—¥å¿—"""

        query = {}

        if user_id:
            query['user_id'] = user_id
        if action:
            query['action'] = action.value
        if ip_address:
            query['ip_address'] = ip_address
        if start_time or end_time:
            time_query = {}
            if start_time:
                time_query['$gte'] = start_time
            if end_time:
                time_query['$lte'] = end_time
            query['timestamp'] = time_query

        try:
            cursor = self.audit_collection.find(query).sort('timestamp', -1).limit(limit)
            results = await cursor.to_list(length=limit)

            # éªŒè¯è®°å½•å®Œæ•´æ€§
            verified_results = []
            for record in results:
                if self._verify_record_integrity(record):
                    verified_results.append(record)
                else:
                    logger.warning(f"Audit record integrity check failed: {record.get('audit_id', 'unknown')}")

            return verified_results

        except Exception as e:
            logger.error(f"Failed to query audit logs: {str(e)}")
            return []

    def _verify_record_integrity(self, record: Dict) -> bool:
        """éªŒè¯è®°å½•å®Œæ•´æ€§"""
        try:
            stored_checksum = record.pop('checksum', '')
            calculated_checksum = self._calculate_checksum({
                'audit_id': record['audit_id'],
                'timestamp': record['timestamp'].isoformat() if isinstance(record['timestamp'], datetime) else record['timestamp'],
                'user_id': record['user_id'],
                'action': record['action'],
                'resource': record['resource']
            })

            # æ¢å¤checksumå­—æ®µ
            record['checksum'] = stored_checksum

            return stored_checksum == calculated_checksum

        except Exception as e:
            logger.warning(f"Failed to verify record integrity: {str(e)}")
            return False

class UserRole(Enum):
    """ç”¨æˆ·è§’è‰²"""
    ADMIN = "admin"
    OPERATOR = "operator"
    ANALYST = "analyst"
    VIEWER = "viewer"

class Permission(Enum):
    """æƒé™ç±»å‹"""
    # ä»»åŠ¡ç®¡ç†æƒé™
    SUBMIT_JOBS = "submit_jobs"
    CANCEL_JOBS = "cancel_jobs"
    VIEW_JOBS = "view_jobs"

    # ç³»ç»Ÿç®¡ç†æƒé™
    MANAGE_ACCOUNTS = "manage_accounts"
    MANAGE_PROXIES = "manage_proxies"
    VIEW_SYSTEM_HEALTH = "view_system_health"
    MODIFY_SYSTEM_CONFIG = "modify_system_config"

    # æ•°æ®è®¿é—®æƒé™
    QUERY_DATA = "query_data"
    EXPORT_DATA = "export_data"
    DELETE_DATA = "delete_data"

    # ç”¨æˆ·ç®¡ç†æƒé™
    MANAGE_USERS = "manage_users"
    VIEW_AUDIT_LOGS = "view_audit_logs"

class RBACManager:
    """åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ç®¡ç†å™¨"""

    # è§’è‰²æƒé™æ˜ å°„
    ROLE_PERMISSIONS = {
        UserRole.ADMIN: [
            Permission.SUBMIT_JOBS, Permission.CANCEL_JOBS, Permission.VIEW_JOBS,
            Permission.MANAGE_ACCOUNTS, Permission.MANAGE_PROXIES,
            Permission.VIEW_SYSTEM_HEALTH, Permission.MODIFY_SYSTEM_CONFIG,
            Permission.QUERY_DATA, Permission.EXPORT_DATA, Permission.DELETE_DATA,
            Permission.MANAGE_USERS, Permission.VIEW_AUDIT_LOGS
        ],
        UserRole.OPERATOR: [
            Permission.SUBMIT_JOBS, Permission.CANCEL_JOBS, Permission.VIEW_JOBS,
            Permission.VIEW_SYSTEM_HEALTH,
            Permission.QUERY_DATA, Permission.EXPORT_DATA
        ],
        UserRole.ANALYST: [
            Permission.VIEW_JOBS,
            Permission.QUERY_DATA, Permission.EXPORT_DATA
        ],
        UserRole.VIEWER: [
            Permission.VIEW_JOBS,
            Permission.QUERY_DATA
        ]
    }

    def __init__(self, redis_client):
        self.redis = redis_client

    def has_permission(self, user_role: UserRole, permission: Permission) -> bool:
        """æ£€æŸ¥ç”¨æˆ·è§’è‰²æ˜¯å¦å…·æœ‰æŒ‡å®šæƒé™"""
        return permission in self.ROLE_PERMISSIONS.get(user_role, [])

    def get_user_permissions(self, user_role: UserRole) -> List[Permission]:
        """è·å–ç”¨æˆ·è§’è‰²çš„æ‰€æœ‰æƒé™"""
        return self.ROLE_PERMISSIONS.get(user_role, [])

    async def create_user_session(self, user_id: str, user_role: UserRole, ip_address: str) -> str:
        """åˆ›å»ºç”¨æˆ·ä¼šè¯"""
        import uuid
        session_id = str(uuid.uuid4())

        session_data = {
            'user_id': user_id,
            'role': user_role.value,
            'permissions': [p.value for p in self.get_user_permissions(user_role)],
            'ip_address': ip_address,
            'created_at': datetime.utcnow().isoformat(),
            'last_activity': datetime.utcnow().isoformat()
        }

        session_key = f"session:{session_id}"
        self.redis.setex(session_key, 86400, json.dumps(session_data))  # 24å°æ—¶è¿‡æœŸ

        return session_id

    async def validate_session(self, session_id: str) -> Optional[Dict]:
        """éªŒè¯ä¼šè¯"""
        session_key = f"session:{session_id}"
        session_data = self.redis.get(session_key)

        if session_data:
            data = json.loads(session_data)

            # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
            data['last_activity'] = datetime.utcnow().isoformat()
            self.redis.setex(session_key, 86400, json.dumps(data))

            return data

        return None

    async def revoke_session(self, session_id: str):
        """æ’¤é”€ä¼šè¯"""
        session_key = f"session:{session_id}"
        self.redis.delete(session_key)

# æƒé™è£…é¥°å™¨
def require_permission(permission: Permission):
    """æƒé™æ£€æŸ¥è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # ä»è¯·æ±‚ä¸­è·å–ä¼šè¯ä¿¡æ¯
            session_data = kwargs.get('current_session')
            if not session_data:
                raise HTTPException(status_code=401, detail="Authentication required")

            user_permissions = session_data.get('permissions', [])
            if permission.value not in user_permissions:
                raise HTTPException(status_code=403, detail="Insufficient permissions")

            return await func(*args, **kwargs)
        return wrapper
    return decorator

def require_role(required_role: UserRole):
    """è§’è‰²æ£€æŸ¥è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            session_data = kwargs.get('current_session')
            if not session_data:
                raise HTTPException(status_code=401, detail="Authentication required")

            user_role = session_data.get('role')
            if user_role != required_role.value:
                raise HTTPException(status_code=403, detail="Insufficient role")

            return await func(*args, **kwargs)
        return wrapper
    return decorator
```

#### å¢å¼ºçš„APIæ¥å£ï¼ˆé›†æˆå®¡è®¡å’Œæƒé™ï¼‰
```python
# api/enhanced_main.py
from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import jwt
from datetime import datetime, timedelta

app = FastAPI(title="WeGet X Data Collection API", version="2.1.0")
security = HTTPBearer()

# å…¨å±€ä¾èµ–
audit_logger = AuditLogger(mongo_client, redis_client)
rbac_manager = RBACManager(redis_client)

# ä¼šè¯ä¾èµ–
async def get_current_session(request: Request, credentials: HTTPAuthorizationCredentials = Depends(security)):
    """è·å–å½“å‰ä¼šè¯ä¿¡æ¯"""
    try:
        # è§£æJWTè·å–ä¼šè¯ID
        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=["HS256"])
        session_id = payload.get("session_id")

        if not session_id:
            raise HTTPException(status_code=401, detail="Invalid token")

        # éªŒè¯ä¼šè¯
        session_data = await rbac_manager.validate_session(session_id)
        if not session_data:
            raise HTTPException(status_code=401, detail="Session expired")

        # æ·»åŠ è¯·æ±‚ä¿¡æ¯
        session_data['request_ip'] = request.client.host
        session_data['user_agent'] = request.headers.get('user-agent', '')

        return session_data

    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

# æ•°æ®æ ‡æ³¨API
class DataTagRequest(BaseModel):
    tags: List[str]
    metadata: Optional[Dict[str, Any]] = None

class UserMetadataRequest(BaseModel):
    metadata: Dict[str, Any]

@app.post("/data/tweets/{tweet_id}/tags")
@require_permission(Permission.MODIFY_DATA)
async def tag_tweet(
    tweet_id: str,
    request_data: DataTagRequest,
    current_session: Dict = Depends(get_current_session)
):
    """ä¸ºæ¨æ–‡æ·»åŠ æ ‡ç­¾"""
    try:
        # è®°å½•å®¡è®¡æ—¥å¿—
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.DATA_MODIFY,
            resource=f"tweet:{tweet_id}",
            details={
                'action': 'add_tags',
                'tags': request_data.tags,
                'metadata': request_data.metadata
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id')
        )

        # æ›´æ–°æ¨æ–‡æ ‡ç­¾
        collection = mongo_client.weget.tweets
        update_result = await collection.update_one(
            {'tweet_id': tweet_id},
            {
                '$addToSet': {'tags': {'$each': request_data.tags}},
                '$set': {
                    'metadata': request_data.metadata,
                    'last_modified': datetime.utcnow(),
                    'modified_by': current_session['user_id']
                }
            }
        )

        if update_result.matched_count == 0:
            raise HTTPException(status_code=404, detail="Tweet not found")

        return {
            "status": "success",
            "tweet_id": tweet_id,
            "tags_added": request_data.tags,
            "modified_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to tag tweet {tweet_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/data/users/{user_id}/metadata")
@require_permission(Permission.MODIFY_DATA)
async def update_user_metadata(
    user_id: str,
    request_data: UserMetadataRequest,
    current_session: Dict = Depends(get_current_session)
):
    """æ›´æ–°ç”¨æˆ·å…ƒæ•°æ®"""
    try:
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.DATA_MODIFY,
            resource=f"user:{user_id}",
            details={
                'action': 'update_metadata',
                'metadata': request_data.metadata
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id')
        )

        collection = mongo_client.weget.users
        update_result = await collection.update_one(
            {'user_id': user_id},
            {
                '$set': {
                    'business_metadata': request_data.metadata,
                    'last_modified': datetime.utcnow(),
                    'modified_by': current_session['user_id']
                }
            }
        )

        if update_result.matched_count == 0:
            raise HTTPException(status_code=404, detail="User not found")

        return {
            "status": "success",
            "user_id": user_id,
            "metadata_updated": True,
            "modified_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to update user metadata {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# å¢å¼ºçš„ä»»åŠ¡ç®¡ç†API
@app.post("/jobs/search")
@require_permission(Permission.SUBMIT_JOBS)
async def submit_search_jobs_enhanced(
    request: SearchJobRequest,
    current_session: Dict = Depends(get_current_session)
):
    """æäº¤æœç´¢é‡‡é›†ä»»åŠ¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        # è®°å½•å®¡è®¡æ—¥å¿—
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.JOB_SUBMIT,
            resource="search_jobs",
            details={
                'keywords': request.keywords,
                'count': request.count,
                'priority': request.priority
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id')
        )

        scheduler = TaskScheduler(redis_client)
        task_ids = scheduler.submit_search_jobs(
            keywords=request.keywords,
            count=request.count,
            priority=request.priority,
            submitted_by=current_session['user_id']  # æ·»åŠ æäº¤è€…ä¿¡æ¯
        )

        estimated_completion = datetime.utcnow() + timedelta(minutes=len(task_ids) * 2)

        return JobResponse(
            task_ids=task_ids,
            submitted_at=datetime.utcnow().isoformat(),
            estimated_completion=estimated_completion.isoformat(),
            submitted_by=current_session['user_id']
        )

    except Exception as e:
        logger.error(f"Failed to submit search jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# å®¡è®¡æ—¥å¿—æŸ¥è¯¢API
@app.get("/admin/audit/logs")
@require_permission(Permission.VIEW_AUDIT_LOGS)
async def get_audit_logs(
    user_id: Optional[str] = None,
    action: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 100,
    current_session: Dict = Depends(get_current_session)
):
    """æŸ¥è¯¢å®¡è®¡æ—¥å¿—"""
    try:
        # è®°å½•æŸ¥è¯¢å®¡è®¡æ—¥å¿—çš„æ“ä½œ
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.VIEW_AUDIT_LOGS,
            resource="audit_logs",
            details={
                'query_user_id': user_id,
                'query_action': action,
                'query_date_range': [start_date, end_date],
                'limit': limit
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id'),
            severity=AuditSeverity.HIGH  # æŸ¥çœ‹å®¡è®¡æ—¥å¿—æ˜¯é«˜æ•æ„Ÿæ“ä½œ
        )

        # è§£ææ—¥æœŸå‚æ•°
        start_time = datetime.fromisoformat(start_date) if start_date else None
        end_time = datetime.fromisoformat(end_date) if end_date else None
        action_enum = AuditAction(action) if action else None

        logs = await audit_logger.query_audit_logs(
            user_id=user_id,
            action=action_enum,
            start_time=start_time,
            end_time=end_time,
            limit=limit
        )

        return {
            "logs": logs,
            "total": len(logs),
            "query_params": {
                "user_id": user_id,
                "action": action,
                "start_date": start_date,
                "end_date": end_date,
                "limit": limit
            }
        }

    except Exception as e:
        logger.error(f"Failed to query audit logs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# æ•°æ®æº¯æºæŸ¥è¯¢API
@app.get("/admin/data/provenance")
@require_permission(Permission.QUERY_DATA)
async def query_data_provenance(
    account_id: Optional[str] = None,
    proxy_id: Optional[str] = None,
    task_id: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_session: Dict = Depends(get_current_session)
):
    """æŸ¥è¯¢æ•°æ®æº¯æºä¿¡æ¯"""
    try:
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.DATA_QUERY,
            resource="data_provenance",
            details={
                'account_id': account_id,
                'proxy_id': proxy_id,
                'task_id': task_id,
                'date_range': [start_date, end_date]
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id')
        )

        data_manager = ProvenanceAwareDataManager(mongo_client, neo4j_driver, redis_client)

        date_range = None
        if start_date and end_date:
            date_range = (start_date, end_date)

        results = await data_manager.query_data_by_provenance(
            account_id=account_id,
            proxy_id=proxy_id,
            task_id=task_id,
            date_range=date_range
        )

        return {
            "results": results,
            "total": len(results),
            "query_params": {
                "account_id": account_id,
                "proxy_id": proxy_id,
                "task_id": task_id,
                "date_range": date_range
            }
        }

    except Exception as e:
        logger.error(f"Failed to query data provenance: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# é¢„æµ‹æ€§å¥åº·è¯„åˆ†API
@app.get("/admin/accounts/{account_id}/health-prediction")
@require_permission(Permission.VIEW_SYSTEM_HEALTH)
async def get_account_health_prediction(
    account_id: str,
    prediction_hours: int = 6,
    current_session: Dict = Depends(get_current_session)
):
    """è·å–è´¦å·å¥åº·é¢„æµ‹"""
    try:
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.SYSTEM_MONITOR,
            resource=f"account_health:{account_id}",
            details={
                'prediction_hours': prediction_hours
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id')
        )

        predictor = PredictiveAccountHealthScorer(redis_client)
        prediction = await predictor.predict_account_health(account_id, prediction_hours)

        return prediction

    except Exception as e:
        logger.error(f"Failed to get health prediction for account {account_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# æ™ºèƒ½DLQç®¡ç†API
@app.post("/admin/dlq/intelligent-process")
@require_permission(Permission.MODIFY_SYSTEM_CONFIG)
async def trigger_intelligent_dlq_processing(
    current_session: Dict = Depends(get_current_session)
):
    """è§¦å‘æ™ºèƒ½DLQå¤„ç†"""
    try:
        await audit_logger.log_action(
            user_id=current_session['user_id'],
            action=AuditAction.ADMIN_ACTION,
            resource="dlq_processing",
            details={
                'action': 'trigger_intelligent_processing'
            },
            ip_address=current_session['request_ip'],
            user_agent=current_session['user_agent'],
            session_id=current_session.get('session_id'),
            severity=AuditSeverity.HIGH
        )

        dlq_manager = IntelligentDLQManager(redis_client)
        stats = await dlq_manager.intelligent_dlq_processing()

        return {
            "status": "completed",
            "processing_stats": stats,
            "triggered_by": current_session['user_id'],
            "triggered_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to trigger intelligent DLQ processing: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ç”¨æˆ·ç™»å½•API
@app.post("/auth/login")
async def login(request: Request, username: str, password: str):
    """ç”¨æˆ·ç™»å½•"""
    try:
        # éªŒè¯ç”¨æˆ·å‡­æ®ï¼ˆè¿™é‡Œç®€åŒ–å®ç°ï¼‰
        user_info = await authenticate_user(username, password)
        if not user_info:
            raise HTTPException(status_code=401, detail="Invalid credentials")

        # åˆ›å»ºä¼šè¯
        user_role = UserRole(user_info['role'])
        session_id = await rbac_manager.create_user_session(
            user_id=user_info['user_id'],
            user_role=user_role,
            ip_address=request.client.host
        )

        # ç”ŸæˆJWT
        token_payload = {
            "session_id": session_id,
            "user_id": user_info['user_id'],
            "role": user_role.value,
            "exp": datetime.utcnow() + timedelta(hours=24)
        }

        token = jwt.encode(token_payload, SECRET_KEY, algorithm="HS256")

        # è®°å½•ç™»å½•å®¡è®¡
        await audit_logger.log_action(
            user_id=user_info['user_id'],
            action=AuditAction.USER_LOGIN,
            resource="authentication",
            details={
                'username': username,
                'role': user_role.value
            },
            ip_address=request.client.host,
            user_agent=request.headers.get('user-agent', ''),
            session_id=session_id
        )

        return {
            "access_token": token,
            "token_type": "bearer",
            "user_id": user_info['user_id'],
            "role": user_role.value,
            "permissions": [p.value for p in rbac_manager.get_user_permissions(user_role)]
        }

    except Exception as e:
        logger.error(f"Login failed for user {username}: {str(e)}")
        raise HTTPException(status_code=500, detail="Login failed")

async def authenticate_user(username: str, password: str) -> Optional[Dict]:
    """ç”¨æˆ·è®¤è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # å®é™…å®ç°åº”è¯¥æŸ¥è¯¢ç”¨æˆ·æ•°æ®åº“å¹¶éªŒè¯å¯†ç å“ˆå¸Œ
    users = {
        "admin": {"user_id": "admin_001", "role": "admin", "password_hash": "admin_hash"},
        "operator": {"user_id": "op_001", "role": "operator", "password_hash": "op_hash"},
        "analyst": {"user_id": "analyst_001", "role": "analyst", "password_hash": "analyst_hash"}
    }

    user = users.get(username)
    if user and verify_password(password, user['password_hash']):
        return user
    return None

def verify_password(password: str, password_hash: str) -> bool:
    """éªŒè¯å¯†ç ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # å®é™…å®ç°åº”è¯¥ä½¿ç”¨bcryptç­‰å®‰å…¨çš„å¯†ç éªŒè¯
    return f"{password}_hash" == password_hash
```

### 21. åˆè§„ä¸æˆæœ¬ä¼˜åŒ–

#### GDPRåˆè§„å®ç°
```python
# core/gdpr_compliance.py
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from enum import Enum
import asyncio
import json
from pydantic import BaseModel

class DataProcessingPurpose(str, Enum):
    RESEARCH = "research"
    ANALYTICS = "analytics"
    MONITORING = "monitoring"
    COMPLIANCE = "compliance"

class DataRetentionPolicy(BaseModel):
    purpose: DataProcessingPurpose
    retention_days: int
    auto_delete: bool = True
    legal_basis: str

class GDPRDataManager:
    """GDPRåˆè§„æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, mongodb_client, redis_client):
        self.mongodb = mongodb_client
        self.redis = redis_client
        self.retention_policies = {
            DataProcessingPurpose.RESEARCH: DataRetentionPolicy(
                purpose=DataProcessingPurpose.RESEARCH,
                retention_days=365,
                legal_basis="legitimate_interest"
            ),
            DataProcessingPurpose.ANALYTICS: DataRetentionPolicy(
                purpose=DataProcessingPurpose.ANALYTICS,
                retention_days=90,
                legal_basis="legitimate_interest"
            ),
            DataProcessingPurpose.MONITORING: DataRetentionPolicy(
                purpose=DataProcessingPurpose.MONITORING,
                retention_days=30,
                legal_basis="legitimate_interest"
            )
        }

    async def process_deletion_request(self, user_identifier: str, request_type: str = "user_id") -> Dict:
        """å¤„ç†ç”¨æˆ·åˆ é™¤è¯·æ±‚"""
        deletion_id = f"del_{int(datetime.utcnow().timestamp())}"

        try:
            # è®°å½•åˆ é™¤è¯·æ±‚
            deletion_request = {
                "deletion_id": deletion_id,
                "user_identifier": user_identifier,
                "request_type": request_type,
                "requested_at": datetime.utcnow().isoformat(),
                "status": "processing",
                "collections_affected": []
            }

            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ•°æ®
            affected_collections = await self._find_user_data(user_identifier, request_type)
            deletion_request["collections_affected"] = affected_collections

            # æ‰§è¡Œåˆ é™¤
            deletion_results = {}
            for collection_name in affected_collections:
                result = await self._delete_from_collection(collection_name, user_identifier, request_type)
                deletion_results[collection_name] = result

            # æ·»åŠ takedownæ ‡è®°
            await self._add_takedown_marker(user_identifier, request_type)

            # æ›´æ–°åˆ é™¤è¯·æ±‚çŠ¶æ€
            deletion_request.update({
                "status": "completed",
                "completed_at": datetime.utcnow().isoformat(),
                "deletion_results": deletion_results
            })

            # ä¿å­˜åˆ é™¤è®°å½•ï¼ˆç”¨äºå®¡è®¡ï¼‰
            await self.mongodb.gdpr_deletions.insert_one(deletion_request)

            return {
                "deletion_id": deletion_id,
                "status": "success",
                "message": f"Data for {user_identifier} has been deleted",
                "affected_records": sum(r.get("deleted_count", 0) for r in deletion_results.values())
            }

        except Exception as e:
            # è®°å½•å¤±è´¥
            await self.mongodb.gdpr_deletions.insert_one({
                "deletion_id": deletion_id,
                "user_identifier": user_identifier,
                "status": "failed",
                "error": str(e),
                "requested_at": datetime.utcnow().isoformat()
            })

            return {
                "deletion_id": deletion_id,
                "status": "error",
                "message": f"Failed to delete data: {str(e)}"
            }

    async def _find_user_data(self, user_identifier: str, request_type: str) -> List[str]:
        """æŸ¥æ‰¾ç”¨æˆ·ç›¸å…³çš„æ‰€æœ‰æ•°æ®é›†åˆ"""
        collections = []

        # æ ¹æ®è¯·æ±‚ç±»å‹ç¡®å®šæŸ¥è¯¢å­—æ®µ
        query_field = {
            "user_id": "user_id",
            "username": "username",
            "email": "email",
            "twitter_id": "twitter_id"
        }.get(request_type, "user_id")

        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½åŒ…å«ç”¨æˆ·æ•°æ®çš„é›†åˆ
        collection_names = [
            "weget_users", "weget_tweets", "weget_replies",
            "weget_followers", "weget_following", "weget_media",
            "weget_search_results", "weget_user_profiles"
        ]

        for collection_name in collection_names:
            collection = self.mongodb[collection_name]
            count = await collection.count_documents({query_field: user_identifier})
            if count > 0:
                collections.append(collection_name)

        return collections

    async def _delete_from_collection(self, collection_name: str, user_identifier: str, request_type: str) -> Dict:
        """ä»æŒ‡å®šé›†åˆåˆ é™¤ç”¨æˆ·æ•°æ®"""
        query_field = {
            "user_id": "user_id",
            "username": "username",
            "email": "email",
            "twitter_id": "twitter_id"
        }.get(request_type, "user_id")

        collection = self.mongodb[collection_name]

        # è½¯åˆ é™¤ï¼šæ·»åŠ åˆ é™¤æ ‡è®°è€Œä¸æ˜¯ç‰©ç†åˆ é™¤
        result = await collection.update_many(
            {query_field: user_identifier},
            {
                "$set": {
                    "takedown_at": datetime.utcnow().isoformat(),
                    "takedown_reason": "user_request",
                    "gdpr_deleted": True
                }
            }
        )

        return {
            "collection": collection_name,
            "matched_count": result.matched_count,
            "modified_count": result.modified_count
        }

    async def _add_takedown_marker(self, user_identifier: str, request_type: str):
        """æ·»åŠ takedownæ ‡è®°åˆ°Redisï¼Œé˜²æ­¢äºŒæ¬¡åˆ†å‘"""
        takedown_key = f"takedown:{request_type}:{user_identifier}"
        await self.redis.setex(
            takedown_key,
            86400 * 365,  # ä¿ç•™1å¹´
            json.dumps({
                "takedown_at": datetime.utcnow().isoformat(),
                "reason": "gdpr_deletion"
            })
        )

    async def check_takedown_status(self, user_identifier: str, request_type: str = "user_id") -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²è¢«takedown"""
        takedown_key = f"takedown:{request_type}:{user_identifier}"
        return await self.redis.exists(takedown_key)
```

#### å¸¦å®½ä¼˜åŒ– - åª’ä½“å»¶è¿ŸæŠ“å–
```python
# core/media_optimizer.py
import asyncio
import aiohttp
from typing import Dict, List, Optional
from datetime import datetime
import hashlib
from PIL import Image
import io

class MediaOptimizer:
    """åª’ä½“æ–‡ä»¶ä¼˜åŒ–å™¨ - èŠ‚çœ60-70%å¸¦å®½"""

    def __init__(self, redis_client, storage_client):
        self.redis = redis_client
        self.storage = storage_client
        self.download_queue = "media:download:queue"
        self.processed_queue = "media:processed:queue"

    async def queue_media_for_download(self, media_urls: List[str], priority: str = "normal") -> str:
        """å°†åª’ä½“URLåŠ å…¥ä¸‹è½½é˜Ÿåˆ—"""
        task_id = f"media_{int(datetime.utcnow().timestamp())}"

        media_task = {
            "task_id": task_id,
            "urls": media_urls,
            "priority": priority,
            "queued_at": datetime.utcnow().isoformat(),
            "status": "queued"
        }

        # æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©é˜Ÿåˆ—
        queue_name = f"{self.download_queue}:{priority}"
        await self.redis.lpush(queue_name, json.dumps(media_task))

        return task_id

    async def process_media_download_queue(self, worker_id: str):
        """å¤„ç†åª’ä½“ä¸‹è½½é˜Ÿåˆ—"""
        while True:
            try:
                # æŒ‰ä¼˜å…ˆçº§å¤„ç†ï¼šhigh -> normal -> low
                for priority in ["high", "normal", "low"]:
                    queue_name = f"{self.download_queue}:{priority}"

                    # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                    task_data = await self.redis.brpop(queue_name, timeout=5)
                    if task_data:
                        _, task_json = task_data
                        task = json.loads(task_json)

                        await self._process_media_task(task, worker_id)
                        break
                else:
                    # æ²¡æœ‰ä»»åŠ¡æ—¶çŸ­æš‚ä¼‘æ¯
                    await asyncio.sleep(1)

            except Exception as e:
                print(f"Media download worker {worker_id} error: {e}")
                await asyncio.sleep(5)

    async def _process_media_task(self, task: Dict, worker_id: str):
        """å¤„ç†å•ä¸ªåª’ä½“ä»»åŠ¡"""
        task_id = task["task_id"]

        try:
            downloaded_media = []

            async with aiohttp.ClientSession() as session:
                for url in task["urls"]:
                    try:
                        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                        url_hash = hashlib.md5(url.encode()).hexdigest()
                        cached_path = await self.redis.get(f"media:cache:{url_hash}")

                        if cached_path:
                            downloaded_media.append({
                                "original_url": url,
                                "cached_path": cached_path.decode(),
                                "from_cache": True
                            })
                            continue

                        # ä¸‹è½½åª’ä½“æ–‡ä»¶
                        media_info = await self._download_and_optimize_media(session, url, url_hash)
                        if media_info:
                            downloaded_media.append(media_info)

                            # ç¼“å­˜è·¯å¾„
                            await self.redis.setex(
                                f"media:cache:{url_hash}",
                                86400 * 30,  # 30å¤©ç¼“å­˜
                                media_info["optimized_path"]
                            )

                    except Exception as e:
                        print(f"Failed to download {url}: {e}")
                        continue

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result = {
                "task_id": task_id,
                "status": "completed",
                "downloaded_count": len(downloaded_media),
                "total_count": len(task["urls"]),
                "completed_at": datetime.utcnow().isoformat(),
                "downloaded_media": downloaded_media,
                "worker_id": worker_id
            }

            await self.redis.lpush(self.processed_queue, json.dumps(result))

        except Exception as e:
            # ä»»åŠ¡å¤±è´¥
            error_result = {
                "task_id": task_id,
                "status": "failed",
                "error": str(e),
                "failed_at": datetime.utcnow().isoformat(),
                "worker_id": worker_id
            }

            await self.redis.lpush(self.processed_queue, json.dumps(error_result))

    async def _download_and_optimize_media(self, session: aiohttp.ClientSession, url: str, url_hash: str) -> Optional[Dict]:
        """ä¸‹è½½å¹¶ä¼˜åŒ–åª’ä½“æ–‡ä»¶"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status != 200:
                    return None

                content = await response.read()
                content_type = response.headers.get('content-type', '')

                # ç¡®å®šæ–‡ä»¶ç±»å‹å’Œæ‰©å±•å
                if 'image' in content_type:
                    optimized_content, file_ext = await self._optimize_image(content, content_type)
                elif 'video' in content_type:
                    # è§†é¢‘æ–‡ä»¶æš‚æ—¶ä¸ä¼˜åŒ–ï¼Œç›´æ¥å­˜å‚¨
                    optimized_content = content
                    file_ext = '.mp4'
                else:
                    optimized_content = content
                    file_ext = '.bin'

                # å­˜å‚¨åˆ°å¯¹è±¡å­˜å‚¨
                file_path = f"media/{url_hash}{file_ext}"
                storage_url = await self.storage.upload(file_path, optimized_content)

                return {
                    "original_url": url,
                    "optimized_path": storage_url,
                    "original_size": len(content),
                    "optimized_size": len(optimized_content),
                    "compression_ratio": len(optimized_content) / len(content),
                    "content_type": content_type,
                    "from_cache": False
                }

        except Exception as e:
            print(f"Error downloading/optimizing {url}: {e}")
            return None

    async def _optimize_image(self, content: bytes, content_type: str) -> tuple:
        """ä¼˜åŒ–å›¾ç‰‡æ–‡ä»¶"""
        try:
            # ä½¿ç”¨PILä¼˜åŒ–å›¾ç‰‡
            image = Image.open(io.BytesIO(content))

            # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœéœ€è¦ï¼‰
            if image.mode in ('RGBA', 'LA', 'P'):
                image = image.convert('RGB')

            # è°ƒæ•´å¤§å°ï¼ˆæœ€å¤§1920x1080ï¼‰
            max_size = (1920, 1080)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)

            # å‹ç¼©ä¿å­˜
            output = io.BytesIO()
            if 'jpeg' in content_type or 'jpg' in content_type:
                image.save(output, format='JPEG', quality=85, optimize=True)
                file_ext = '.jpg'
            elif 'png' in content_type:
                image.save(output, format='PNG', optimize=True)
                file_ext = '.png'
            else:
                image.save(output, format='JPEG', quality=85, optimize=True)
                file_ext = '.jpg'

            return output.getvalue(), file_ext

        except Exception as e:
            print(f"Image optimization failed: {e}")
            return content, '.jpg'
```

### 22. æ•°æ®ç´¢å¼•ä¼˜åŒ–ä¸å†·çƒ­åˆ†å±‚

#### MongoDBç´¢å¼•ä¼˜åŒ–ç­–ç•¥
```python
# core/database_optimizer.py
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import asyncio
import logging

logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""

    def __init__(self, mongodb_client):
        self.mongodb = mongodb_client
        self.db = mongodb_client.weget

    async def create_optimized_indexes(self):
        """åˆ›å»ºä¼˜åŒ–çš„ç´¢å¼•"""
        try:
            # ç”¨æˆ·é›†åˆç´¢å¼•
            await self.db.twitter_users.create_index([
                ("user_id", 1)
            ], unique=True, background=True)

            await self.db.twitter_users.create_index([
                ("username", 1)
            ], background=True)

            await self.db.twitter_users.create_index([
                ("created_at", -1),
                ("followers_count", -1)
            ], background=True)

            # æ¨æ–‡é›†åˆç´¢å¼• - ä¿®å¤ï¼šä½¿ç”¨DateTimeField
            await self.db.twitter_tweets.create_index([
                ("tweet_id", 1)
            ], unique=True, background=True)

            await self.db.twitter_tweets.create_index([
                ("user_id", 1),
                ("created_at", -1)  # ç°åœ¨æ˜¯DateTimeFieldï¼Œæ”¯æŒæ—¶é—´èŒƒå›´æŸ¥è¯¢
            ], background=True)

            await self.db.twitter_tweets.create_index([
                ("created_at", -1)  # æŒ‰æ—¶é—´æ’åºçš„å…¨å±€ç´¢å¼•
            ], background=True)

            await self.db.twitter_tweets.create_index([
                ("hashtags", 1)
            ], background=True)

            # TTLç´¢å¼• - è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®
            await self.db.twitter_tweets.create_index([
                ("takedown_at", 1)
            ], expireAfterSeconds=0, background=True)

            await self.db.twitter_users.create_index([
                ("takedown_at", 1)
            ], expireAfterSeconds=0, background=True)

            # å¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
            await self.db.twitter_tweets.create_index([
                ("lang", 1),
                ("created_at", -1),
                ("like_count", -1)
            ], background=True)

            logger.info("Database indexes created successfully")

        except Exception as e:
            logger.error(f"Failed to create indexes: {str(e)}")
            raise

    async def implement_data_tiering(self):
        """å®ç°æ•°æ®åˆ†å±‚ç­–ç•¥"""
        try:
            # çƒ­æ•°æ®ï¼šæœ€è¿‘30å¤©
            hot_cutoff = datetime.utcnow() - timedelta(days=30)

            # æ¸©æ•°æ®ï¼š30-90å¤©
            warm_cutoff = datetime.utcnow() - timedelta(days=90)

            # å†·æ•°æ®ï¼š>90å¤©ï¼Œç§»åŠ¨åˆ°å½’æ¡£å­˜å‚¨
            cold_cutoff = datetime.utcnow() - timedelta(days=90)

            # ç»Ÿè®¡å„å±‚æ•°æ®é‡
            hot_tweets = await self.db.twitter_tweets.count_documents({
                "created_at": {"$gte": hot_cutoff}
            })

            warm_tweets = await self.db.twitter_tweets.count_documents({
                "created_at": {"$gte": warm_cutoff, "$lt": hot_cutoff}
            })

            cold_tweets = await self.db.twitter_tweets.count_documents({
                "created_at": {"$lt": cold_cutoff}
            })

            logger.info(f"Data tiering stats - Hot: {hot_tweets}, Warm: {warm_tweets}, Cold: {cold_tweets}")

            # ä¸ºå†·æ•°æ®åˆ›å»ºå½’æ¡£ä»»åŠ¡
            if cold_tweets > 0:
                await self._schedule_cold_data_archival(cold_cutoff)

        except Exception as e:
            logger.error(f"Data tiering implementation failed: {str(e)}")
            raise

    async def _schedule_cold_data_archival(self, cutoff_date: datetime):
        """è°ƒåº¦å†·æ•°æ®å½’æ¡£"""
        archival_task = {
            "task_type": "cold_data_archival",
            "cutoff_date": cutoff_date.isoformat(),
            "collections": ["twitter_tweets", "twitter_users"],
            "scheduled_at": datetime.utcnow().isoformat(),
            "status": "pending"
        }

        # æ·»åŠ åˆ°å½’æ¡£é˜Ÿåˆ—
        await self.db.archival_tasks.insert_one(archival_task)
        logger.info(f"Scheduled cold data archival for data before {cutoff_date}")

class ArchivalScheduler:
    """å½’æ¡£è°ƒåº¦å™¨"""

    def __init__(self, mongodb_client, s3_client):
        self.mongodb = mongodb_client
        self.s3 = s3_client
        self.db = mongodb_client.weget
        self.is_running = False

    async def start_scheduler(self):
        """å¯åŠ¨å½’æ¡£è°ƒåº¦å™¨"""
        self.is_running = True

        while self.is_running:
            try:
                # æ£€æŸ¥å¾…å¤„ç†çš„å½’æ¡£ä»»åŠ¡
                pending_tasks = await self.db.archival_tasks.find({
                    "status": "pending"
                }).to_list(length=10)

                for task in pending_tasks:
                    await self._process_archival_task(task)

                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                await asyncio.sleep(3600)

            except Exception as e:
                print(f"Archival scheduler error: {e}")
                await asyncio.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿ

    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.is_running = False
```

## æœ€ç»ˆé¡¹ç›®æ€»ç»“ä¸å®æ–½å»ºè®®

### ğŸš€ **å®Œæ•´æ¶æ„ç‰¹æ€§ï¼ˆæœ€ç»ˆç‰ˆï¼‰**

#### **1. æ™ºèƒ½åŒ–è¿ç»´**
- **é¢„æµ‹æ€§ç»´æŠ¤**: æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹è´¦å·å¥åº·çŠ¶å†µ
- **æ™ºèƒ½DLQç®¡ç†**: è‡ªåŠ¨é”™è¯¯åˆ†ç±»å’Œä¿®å¤æœºåˆ¶
- **è‡ªé€‚åº”API**: è‡ªåŠ¨æ£€æµ‹å’Œé€‚é…å¹³å°å˜åŒ–

#### **2. ä¼ä¸šçº§æ•°æ®ç®¡ç†**
- **æ•°æ®æº¯æº**: å®Œæ•´çš„æ•°æ®æ¥æºè¿½è¸ª
- **Schemaç‰ˆæœ¬ç®¡ç†**: æ”¯æŒæ•°æ®æ¨¡å‹æ¼”è¿›
- **æ•°æ®æ ‡æ³¨**: åŠ¨æ€ä¸šåŠ¡æ ‡ç­¾å’Œå…ƒæ•°æ®ç®¡ç†

#### **3. å®‰å…¨ä¸åˆè§„**
- **å®¡è®¡æ—¥å¿—**: é˜²ç¯¡æ”¹çš„å®Œæ•´æ“ä½œè®°å½•
- **RBACæƒé™æ§åˆ¶**: ç»†ç²’åº¦çš„è§’è‰²æƒé™ç®¡ç†
- **ä¼šè¯ç®¡ç†**: å®‰å…¨çš„ç”¨æˆ·ä¼šè¯æ§åˆ¶

#### **4. æç«¯å¯æ‰©å±•æ€§**
- **è§£è€¦æ•°æ®ç®¡é“**: é‡‡é›†ã€éªŒè¯ã€å­˜å‚¨å®Œå…¨åˆ†ç¦»
- **åŠ¨æ€æ‰©ç¼©å®¹**: KEDAè‡ªåŠ¨èµ„æºè°ƒåº¦
- **æ··åˆå­˜å‚¨**: å†…å®¹å’Œå…³ç³»æ•°æ®ä¼˜åŒ–å­˜å‚¨

### ğŸ“Š **æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡**

| æŒ‡æ ‡ç±»å‹ | ç›®æ ‡å€¼ | å®ç°æ–¹å¼ |
|---------|--------|----------|
| é‡‡é›†é€Ÿåº¦ | 200ä¸‡+æ¡/å°æ—¶ | è§£è€¦ç®¡é“ + é¢„æµ‹æ€§ç»´æŠ¤ |
| æ•°æ®è´¨é‡ | 99.95% | PydanticéªŒè¯ + æ™ºèƒ½DLQ |
| ç³»ç»Ÿå¯ç”¨æ€§ | 99.99% | å¤šå±‚å®¹é”™ + é¢„æµ‹æ€§ç»´æŠ¤ |
| è´¦å·å­˜æ´»ç‡ | 95%+ | æœºå™¨å­¦ä¹ å¥åº·é¢„æµ‹ |
| æ•…éšœæ¢å¤æ—¶é—´ | <5åˆ†é’Ÿ | è‡ªåŠ¨åŒ–æ•…éšœå¤„ç† |
| æ•°æ®æº¯æºè¦†ç›– | 100% | å®Œæ•´çš„æº¯æºè®°å½• |

### ğŸ›¡ï¸ **å®‰å…¨åˆè§„ç‰¹æ€§**

1. **æ•°æ®å®Œæ•´æ€§**: æ ¡éªŒå’Œé˜²ç¯¡æ”¹æœºåˆ¶
2. **è®¿é—®æ§åˆ¶**: åŸºäºè§’è‰²çš„ç»†ç²’åº¦æƒé™
3. **å®¡è®¡è¿½è¸ª**: æ‰€æœ‰æ“ä½œçš„å®Œæ•´è®°å½•
4. **å¯†é’¥ç®¡ç†**: HashiCorp Vaulté›†æˆ
5. **ä¼šè¯å®‰å…¨**: JWT + Redisä¼šè¯ç®¡ç†

### ğŸ¯ **å®æ–½è·¯çº¿å›¾ï¼ˆæœ€ç»ˆç‰ˆï¼‰**

#### **é˜¶æ®µ1: æ ¸å¿ƒåŸºç¡€ (3å‘¨)**
- åŸºç¡€é‡‡é›†æ¡†æ¶ + æµè§ˆå™¨è‡ªåŠ¨åŒ–
- è§£è€¦æ•°æ®ç®¡é“ + æ­»ä¿¡é˜Ÿåˆ—
- åŸºç¡€ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

#### **é˜¶æ®µ2: æ™ºèƒ½åŒ–åŠŸèƒ½ (2å‘¨)**
- é¢„æµ‹æ€§è´¦å·å¥åº·è¯„åˆ†
- æ™ºèƒ½DLQç®¡ç†ç³»ç»Ÿ
- APIè‡ªé€‚åº”æœºåˆ¶

#### **é˜¶æ®µ3: æ•°æ®ç®¡ç† (2å‘¨)**
- æ•°æ®æº¯æºç³»ç»Ÿ
- Schemaç‰ˆæœ¬ç®¡ç†
- æ•°æ®æ ‡æ³¨API

#### **é˜¶æ®µ4: å®‰å…¨åˆè§„ (1å‘¨)**
- å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
- RBACæƒé™æ§åˆ¶
- å®‰å…¨APIæ¥å£

#### **é˜¶æ®µ5: ç”Ÿäº§éƒ¨ç½² (1å‘¨)**
- Kuberneteséƒ¨ç½²
- æ€§èƒ½è°ƒä¼˜
- è¿ç»´åŸ¹è®­

### ğŸ’¡ **æŠ€æœ¯åˆ›æ–°äº®ç‚¹**

1. **ä¸šç•Œé¦–åˆ›**: é¢„æµ‹æ€§è´¦å·å¥åº·è¯„åˆ†ç³»ç»Ÿ
2. **æ™ºèƒ½è¿ç»´**: è‡ªåŠ¨åŒ–DLQå¤„ç†å’Œé”™è¯¯ä¿®å¤
3. **å®Œæ•´æº¯æº**: ç«¯åˆ°ç«¯çš„æ•°æ®æ¥æºè¿½è¸ª
4. **è‡ªé€‚åº”æ¶æ„**: è‡ªåŠ¨åº”å¯¹å¹³å°å˜åŒ–
5. **ä¼ä¸šçº§å®‰å…¨**: å®Œæ•´çš„å®¡è®¡å’Œæƒé™ä½“ç³»

---

### 23. CI/CD å®‰å…¨æ‰«æä¸è´¨é‡é—¨ç¦

#### 23.1 GitHub Actions CI é…ç½®
```yaml
# .github/workflows/ci.yml
name: WeGet CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Run TruffleHog OSS
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --debug --only-verified

    - name: Run Semgrep
      uses: returntocorp/semgrep-action@v1
      with:
        config: >-
          p/security-audit
          p/secrets
          p/python
        generateSarif: "1"

    - name: Upload Semgrep results to GitHub
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: semgrep.sarif
      if: always()

    - name: Run Bandit Security Linter
      run: |
        pip install bandit[toml]
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . -f txt

    - name: Upload Bandit results
      uses: actions/upload-artifact@v3
      with:
        name: bandit-report
        path: bandit-report.json

  code-quality:
    name: Code Quality Check
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff black isort mypy pytest pytest-cov
        pip install -r requirements.txt

    - name: Run Ruff (ä¸¥æ ¼æ¨¡å¼ - é›¶å®¹é”™)
      run: |
        # ä¸¥æ ¼æ¨¡å¼ï¼šä»»ä½•è¿è§„éƒ½ä¼šå¯¼è‡´å¤±è´¥ï¼Œå®Œå…¨ç§»é™¤ --exit-zero
        ruff check . \
          --select I,E,F,W,N,UP,B,A,C4,T20,S,BLE,FBT,ARG,PTH,PD,PL,TRY,NPY,PERF,FURB,LOG,RUF \
          --exit-non-zero-on-fix \
          --output-format=github \
          --show-fixes

        # è‡ªå®šä¹‰è§„åˆ™æ£€æŸ¥
        echo "ğŸ” Running custom WeGet-specific checks..."

        # æ£€æŸ¥æ˜¯å¦æœ‰åŒæ­¥Rediså¯¼å…¥
        if grep -r "^import redis$\|^from redis import" --include="*.py" . | grep -v "sync_wrapper"; then
          echo "âŒ Found synchronous Redis imports outside of SyncRedisWrapper"
          echo "Use 'import redis.asyncio as redis' instead"
          exit 1
        fi

        # æ£€æŸ¥AsyncRedisClientä½¿ç”¨ï¼ˆåº”è¯¥è¢«å¼ƒç”¨ï¼‰
        if grep -r "class AsyncRedisClient" --include="*.py" .; then
          echo "âŒ Found deprecated AsyncRedisClient class"
          echo "Use unified AsyncRedisManager from core.redis_manager instead"
          exit 1
        fi

        # æ£€æŸ¥é‡å¤çš„Docker Composeæ–‡ä»¶
        compose_count=$(find . -name "docker-compose*.yml" -not -path "./generated/*" | wc -l)
        if [ "$compose_count" -gt 1 ]; then
          echo "âŒ Found multiple Docker Compose files: $compose_count"
          echo "Only auto-generated docker-compose.dev.yml should exist"
          exit 1
        fi

        # æ£€æŸ¥PlaywrightåŒæ­¥è°ƒç”¨
        if grep -r "playwright\.sync_api\|sync_playwright" --include="*.py" .; then
          echo "âŒ Found synchronous Playwright usage"
          echo "Use async Playwright API instead"
          exit 1
        fi

        # æ£€æŸ¥é˜»å¡çš„Redisæ“ä½œ
        if grep -r "\.get(\|\.set(\|\.hget(\|\.hset(" --include="*.py" . | grep -v "await\|async def\|# sync allowed"; then
          echo "âŒ Found potentially blocking Redis operations"
          echo "Ensure all Redis operations are awaited in async functions"
          exit 1
        fi

    - name: Run Black (ä¸¥æ ¼æ ¼å¼æ£€æŸ¥)
      run: |
        black --check --diff .
        if [ $? -ne 0 ]; then
          echo "âŒ Code formatting issues found. Run 'black .' to fix."
          exit 1
        fi

    - name: Run isort (å¯¼å…¥æ’åºæ£€æŸ¥)
      run: |
        isort --check-only --diff .
        if [ $? -ne 0 ]; then
          echo "âŒ Import sorting issues found. Run 'isort .' to fix."
          exit 1
        fi

    - name: Run MyPy (ç±»å‹æ£€æŸ¥)
      run: |
        mypy . --ignore-missing-imports --strict-optional --warn-redundant-casts --warn-unused-ignores
        if [ $? -ne 0 ]; then
          echo "âŒ Type checking failed"
          exit 1
        fi

    - name: Check for placeholder code
      run: |
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å ä½ç¬¦ä»£ç 
        if grep -r "pass  # TODO\|FIXME\|NotImplementedError\|raise NotImplementedError" --include="*.py" .; then
          echo "âŒ Found placeholder code that must be implemented before merge"
          exit 1
        fi
        echo "âœ… No placeholder code found"

    - name: Check for hardcoded secrets
      run: |
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¡¬ç¼–ç çš„å¯†ç æˆ–å¯†é’¥
        if grep -r -i "password.*=.*['\"].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" .; then
          echo "âŒ Found potential hardcoded passwords"
          exit 1
        fi
        if grep -r -i "api_key.*=.*['\"].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" .; then
          echo "âŒ Found potential hardcoded API keys"
          exit 1
        fi
        echo "âœ… No hardcoded secrets found"

    - name: Run tests with strict coverage
      run: |
        # ä¸¥æ ¼çš„æµ‹è¯•è¦†ç›–ç‡è¦æ±‚
        pytest --cov=. --cov-report=xml --cov-report=term --cov-fail-under=80 --cov-branch
        # æ£€æŸ¥æ˜¯å¦æœ‰è·³è¿‡çš„æµ‹è¯•
        if pytest --collect-only -q | grep -i "skip\|xfail"; then
          echo "âš ï¸  Found skipped or expected-to-fail tests"
          pytest --collect-only -q | grep -i "skip\|xfail"
        fi

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

  docker-security:
    name: Docker Security Scan
    runs-on: ubuntu-latest
    needs: [security-scan, code-quality]
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Build Docker image
      run: |
        docker build -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} .

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run Hadolint
      uses: hadolint/hadolint-action@v3.1.0
      with:
        dockerfile: Dockerfile
        failure-threshold: error

  helm-security:
    name: Helm Security Check
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'

    - name: Lint Helm charts
      run: |
        helm lint weget-chart/

    - name: Run Checkov on Helm templates
      uses: bridgecrewio/checkov-action@master
      with:
        directory: weget-chart/
        framework: helm
        output_format: sarif
        output_file_path: checkov-helm.sarif

    - name: Upload Checkov results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: checkov-helm.sarif

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [security-scan, code-quality]
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      mongodb:
        image: mongo:6
        env:
          MONGO_INITDB_ROOT_USERNAME: test
          MONGO_INITDB_ROOT_PASSWORD: test
        options: >-
          --health-cmd "mongosh --eval 'db.runCommand(\"ping\")'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 27017:27017

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-asyncio pytest-integration

    - name: Run integration tests
      env:
        REDIS_URL: redis://localhost:6379
        MONGO_USER: test
        MONGO_PASSWORD: test
        MONGO_HOST: localhost
        MONGO_PORT: 27017
        MONGO_DATABASE: test
      run: |
        pytest tests/integration/ -v --tb=short

    - name: Test Redis-Celery compatibility
      env:
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/integration/test_redis_celery_compatibility.py -v

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-security, helm-security, integration-tests]
    if: github.ref == 'refs/heads/develop'
    environment: staging
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1

    - name: Build and push Docker image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: weget-scraper
        IMAGE_TAG: ${{ github.sha }}
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

    - name: Deploy to staging with Helm
      run: |
        aws eks update-kubeconfig --name staging-cluster
        helm upgrade --install weget-staging ./weget-chart \
          --namespace weget-staging \
          --create-namespace \
          --values ./weget-chart/values-staging.yaml \
          --set image.tag=${{ github.sha }} \
          --wait --timeout=10m

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [docker-security, helm-security, integration-tests]
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to production
      run: |
        # ç”Ÿäº§éƒ¨ç½²é€»è¾‘
        echo "Deploying to production..."
```

#### 23.2 è´¨é‡é—¨ç¦é…ç½®
```yaml
# .github/workflows/quality-gates.yml
name: Quality Gates

on:
  pull_request:
    branches: [ main, develop ]

jobs:
  quality-check:
    name: Quality Gate Check
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff pytest pytest-cov complexity-checker
        pip install -r requirements.txt

    - name: Strict placeholder code check
      run: |
        # ä¸¥æ ¼æ£€æŸ¥å ä½ç¬¦ä»£ç 
        echo "ğŸ” Checking for placeholder code..."
        placeholder_count=0

        # æ£€æŸ¥å„ç§å ä½ç¬¦æ¨¡å¼
        if grep -r "pass  # TODO\|FIXME\|NotImplementedError\|raise NotImplementedError" --include="*.py" .; then
          echo "âŒ Found placeholder code that must be implemented"
          placeholder_count=$((placeholder_count + 1))
        fi

        # æ£€æŸ¥ç©ºå‡½æ•°ä½“
        if grep -r "def.*:$" --include="*.py" . | grep -v "__init__\|__str__\|__repr__"; then
          echo "âŒ Found functions with empty bodies"
          placeholder_count=$((placeholder_count + 1))
        fi

        # æ£€æŸ¥TODOæ³¨é‡Š
        if grep -r "# TODO\|# FIXME\|# HACK" --include="*.py" .; then
          echo "âŒ Found TODO/FIXME/HACK comments that should be resolved"
          placeholder_count=$((placeholder_count + 1))
        fi

        if [ $placeholder_count -gt 0 ]; then
          echo "âŒ Quality gate failed: Found $placeholder_count types of placeholder code"
          exit 1
        fi
        echo "âœ… No placeholder code found"

    - name: Advanced complexity check
      run: |
        # å®‰è£…å¤æ‚åº¦æ£€æŸ¥å·¥å…·
        pip install radon xenon

        # æ£€æŸ¥åœˆå¤æ‚åº¦
        echo "ğŸ” Checking cyclomatic complexity..."
        radon cc . --min B --show-complexity

        # æ£€æŸ¥ç»´æŠ¤æ€§æŒ‡æ•°
        echo "ğŸ” Checking maintainability index..."
        radon mi . --min B --show

        # ä½¿ç”¨ xenon è¿›è¡Œä¸¥æ ¼æ£€æŸ¥
        xenon --max-absolute B --max-modules A --max-average A .

    - name: Comprehensive security baseline check
      run: |
        echo "ğŸ” Running comprehensive security checks..."

        # æ£€æŸ¥ç¡¬ç¼–ç å¯†ç 
        if grep -r -i "password.*=.*['\"][^$].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" .; then
          echo "âŒ Found potential hardcoded passwords"
          exit 1
        fi

        # æ£€æŸ¥APIå¯†é’¥
        if grep -r -i "api_key.*=.*['\"][^$].*['\"]" --include="*.py" --include="*.yaml" --include="*.yml" .; then
          echo "âŒ Found potential hardcoded API keys"
          exit 1
        fi

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        if grep -r "mongodb://.*:.*@\|redis://.*:.*@" --include="*.py" --include="*.yaml" --include="*.yml" . | grep -v "\${"; then
          echo "âŒ Found hardcoded database connection strings"
          exit 1
        fi

        # æ£€æŸ¥ç§é’¥æ¨¡å¼
        if grep -r "BEGIN.*PRIVATE.*KEY\|BEGIN.*RSA.*PRIVATE" --include="*.py" --include="*.pem" --include="*.key" .; then
          echo "âŒ Found potential private keys in code"
          exit 1
        fi

        echo "âœ… No hardcoded secrets found"

    - name: Strict test coverage gate
      run: |
        echo "ğŸ” Running strict test coverage analysis..."

        # è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        pytest --cov=. --cov-report=xml --cov-report=term-missing --cov-report=html --cov-fail-under=80 --cov-branch

        # æ£€æŸ¥å…³é”®æ¨¡å—çš„è¦†ç›–ç‡
        python -c "
import xml.etree.ElementTree as ET
tree = ET.parse('coverage.xml')
root = tree.getroot()

critical_modules = ['core/', 'modules/', 'utils/']
for package in root.findall('.//package'):
    name = package.get('name')
    if any(name.startswith(mod) for mod in critical_modules):
        line_rate = float(package.get('line-rate'))
        branch_rate = float(package.get('branch-rate'))
        if line_rate < 0.85 or branch_rate < 0.80:
            print(f'âŒ Critical module {name} has insufficient coverage: {line_rate:.1%} lines, {branch_rate:.1%} branches')
            exit(1)
print('âœ… All critical modules meet coverage requirements')
        "

    - name: Performance regression test
      run: |
        # å®‰è£…æ€§èƒ½æµ‹è¯•å·¥å…·
        pip install pytest-benchmark memory-profiler

        echo "ğŸ” Running performance regression tests..."

        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        pytest tests/performance/ --benchmark-only --benchmark-json=benchmark.json

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        python -c "
import json
import sys

# æ£€æŸ¥åŸºå‡†æµ‹è¯•ç»“æœ
try:
    with open('benchmark.json', 'r') as f:
        data = json.load(f)

    for benchmark in data['benchmarks']:
        name = benchmark['name']
        mean_time = benchmark['stats']['mean']

        # è®¾ç½®æ€§èƒ½é˜ˆå€¼
        if 'scrape' in name.lower() and mean_time > 5.0:  # 5ç§’é˜ˆå€¼
            print(f'âŒ Performance regression in {name}: {mean_time:.2f}s > 5.0s')
            sys.exit(1)
        elif 'parse' in name.lower() and mean_time > 1.0:  # 1ç§’é˜ˆå€¼
            print(f'âŒ Performance regression in {name}: {mean_time:.2f}s > 1.0s')
            sys.exit(1)

    print('âœ… All performance tests passed')
except FileNotFoundError:
    print('âš ï¸  No benchmark results found')
        "

    - name: Code quality metrics
      run: |
        # ç”Ÿæˆä»£ç è´¨é‡æŠ¥å‘Š
        pip install flake8 mccabe

        echo "ğŸ” Generating code quality metrics..."

        # Flake8 æ£€æŸ¥
        flake8 . --max-complexity=10 --max-line-length=88 --statistics

        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        python -c "
import subprocess
import sys

# è·å–ä»£ç è¡Œæ•°
result = subprocess.run(['find', '.', '-name', '*.py', '-exec', 'wc', '-l', '{}', '+'],
                       capture_output=True, text=True)
total_lines = sum(int(line.split()[0]) for line in result.stdout.strip().split('\n')[:-1])

# è·å–æµ‹è¯•è¦†ç›–ç‡
try:
    result = subprocess.run(['coverage', 'report', '--format=total'],
                           capture_output=True, text=True)
    coverage = float(result.stdout.strip())
except:
    coverage = 0

print(f'ğŸ“Š Code Quality Metrics:')
print(f'   Total Lines of Code: {total_lines:,}')
print(f'   Test Coverage: {coverage:.1f}%')
print(f'   Quality Score: {min(100, (coverage + (100 - total_lines/1000))):.1f}/100')

if coverage < 80:
    print('âŒ Quality gate failed: Coverage below 80%')
    sys.exit(1)
        "
```

#### 23.3 Ruff è‡ªå®šä¹‰é…ç½®
```toml
# pyproject.toml - Ruff é…ç½®
[tool.ruff]
# å¯ç”¨æ‰€æœ‰ç›¸å…³è§„åˆ™
select = [
    "I",    # isort
    "E",    # pycodestyle errors
    "F",    # pyflakes
    "W",    # pycodestyle warnings
    "N",    # pep8-naming
    "UP",   # pyupgrade
    "B",    # flake8-bugbear
    "A",    # flake8-builtins
    "C4",   # flake8-comprehensions
    "T20",  # flake8-print
    "S",    # flake8-bandit
    "BLE",  # flake8-blind-except
    "FBT",  # flake8-boolean-trap
    "ARG",  # flake8-unused-arguments
    "PTH",  # flake8-use-pathlib
    "PD",   # pandas-vet
    "PL",   # pylint
    "TRY",  # tryceratops
    "NPY",  # numpy
    "PERF", # perflint
    "FURB", # refurb
    "LOG",  # flake8-logging
    "RUF",  # ruff-specific
]

# å¿½ç•¥ç‰¹å®šè§„åˆ™
ignore = [
    "S101",   # assert è¯­å¥åœ¨æµ‹è¯•ä¸­æ˜¯å…è®¸çš„
    "S603",   # subprocess è°ƒç”¨åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯å¿…è¦çš„
    "PLR0913", # å‡½æ•°å‚æ•°è¿‡å¤šåœ¨æŸäº›æƒ…å†µä¸‹æ˜¯åˆç†çš„
]

# æ–‡ä»¶é•¿åº¦é™åˆ¶
line-length = 88

# ç›®æ ‡Pythonç‰ˆæœ¬
target-version = "py311"

# æ’é™¤æ–‡ä»¶
exclude = [
    ".git",
    "__pycache__",
    "build",
    "dist",
    "*.egg-info",
    ".venv",
    "migrations",
]

[tool.ruff.per-file-ignores]
# æµ‹è¯•æ–‡ä»¶å¯ä»¥ä½¿ç”¨assertå’ŒæŸäº›ä¸å®‰å…¨çš„æ“ä½œ
"tests/**/*.py" = ["S101", "S106", "S108"]
# é…ç½®æ–‡ä»¶å¯ä»¥æœ‰ç¡¬ç¼–ç çš„å€¼
"config/**/*.py" = ["S105", "S106"]

[tool.ruff.flake8-quotes]
inline-quotes = "double"

[tool.ruff.isort]
known-first-party = ["core", "modules", "utils", "tests"]
force-single-line = false
```

#### 23.4 è‡ªå®šä¹‰Ruffæ’ä»¶
```python
# scripts/ruff_weget_plugin.py
# WeGet é¡¹ç›®ç‰¹å®šçš„ä»£ç æ£€æŸ¥è§„åˆ™

import ast
import re
from typing import Iterator, List, Tuple

class WeGetChecker:
    """WeGet é¡¹ç›®ç‰¹å®šçš„ä»£ç æ£€æŸ¥å™¨"""

    def __init__(self, tree: ast.AST, filename: str):
        self.tree = tree
        self.filename = filename
        self.errors: List[Tuple[int, int, str]] = []

    def check_sync_redis_imports(self) -> Iterator[Tuple[int, int, str]]:
        """æ£€æŸ¥åŒæ­¥Rediså¯¼å…¥"""
        for node in ast.walk(self.tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name == "redis" and "sync_wrapper" not in self.filename:
                        yield (
                            node.lineno,
                            node.col_offset,
                            "WG001 Use 'import redis.asyncio as redis' instead of sync redis"
                        )

            elif isinstance(node, ast.ImportFrom):
                if node.module == "redis" and "sync_wrapper" not in self.filename:
                    yield (
                        node.lineno,
                        node.col_offset,
                        "WG001 Use 'from redis.asyncio import ...' instead of sync redis"
                    )

    def check_playwright_sync_usage(self) -> Iterator[Tuple[int, int, str]]:
        """æ£€æŸ¥PlaywrightåŒæ­¥APIä½¿ç”¨"""
        for node in ast.walk(self.tree):
            if isinstance(node, ast.ImportFrom):
                if node.module and "playwright.sync_api" in node.module:
                    yield (
                        node.lineno,
                        node.col_offset,
                        "WG002 Use async Playwright API instead of sync_api"
                    )

            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name) and node.func.id == "sync_playwright":
                    yield (
                        node.lineno,
                        node.col_offset,
                        "WG002 Use async_playwright() instead of sync_playwright()"
                    )

    def check_blocking_redis_calls(self) -> Iterator[Tuple[int, int, str]]:
        """æ£€æŸ¥å¯èƒ½é˜»å¡çš„Redisè°ƒç”¨"""
        blocking_methods = {"get", "set", "hget", "hset", "sadd", "srem", "lpush", "rpop"}

        for node in ast.walk(self.tree):
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Attribute):
                    if node.func.attr in blocking_methods:
                        # æ£€æŸ¥æ˜¯å¦åœ¨asyncå‡½æ•°ä¸­ä¸”æ²¡æœ‰await
                        parent_func = self._find_parent_function(node)
                        if parent_func and isinstance(parent_func, ast.AsyncFunctionDef):
                            # æ£€æŸ¥æ˜¯å¦æœ‰await
                            if not self._is_awaited(node):
                                yield (
                                    node.lineno,
                                    node.col_offset,
                                    f"WG003 Redis method '{node.func.attr}' should be awaited in async function"
                                )

    def check_hardcoded_connections(self) -> Iterator[Tuple[int, int, str]]:
        """æ£€æŸ¥ç¡¬ç¼–ç çš„æ•°æ®åº“è¿æ¥"""
        for node in ast.walk(self.tree):
            if isinstance(node, ast.Str):
                value = node.s
                if re.search(r"mongodb://.*:.*@|redis://.*:.*@", value):
                    if "${" not in value:  # ä¸æ˜¯ç¯å¢ƒå˜é‡
                        yield (
                            node.lineno,
                            node.col_offset,
                            "WG004 Use environment variables for database connections"
                        )

    def _find_parent_function(self, node: ast.AST) -> ast.AST:
        """æŸ¥æ‰¾èŠ‚ç‚¹çš„çˆ¶å‡½æ•°"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦éå†ASTæ ‘
        return None

    def _is_awaited(self, node: ast.AST) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢«await"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ£€æŸ¥çˆ¶èŠ‚ç‚¹
        return False

    def run_checks(self) -> List[Tuple[int, int, str]]:
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        errors = []
        errors.extend(self.check_sync_redis_imports())
        errors.extend(self.check_playwright_sync_usage())
        errors.extend(self.check_blocking_redis_calls())
        errors.extend(self.check_hardcoded_connections())
        return errors

def check_file(filename: str, content: str) -> List[Tuple[int, int, str]]:
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶"""
    try:
        tree = ast.parse(content, filename=filename)
        checker = WeGetChecker(tree, filename)
        return checker.run_checks()
    except SyntaxError:
        return []

if __name__ == "__main__":
    import sys
    import os

    errors_found = False

    for root, dirs, files in os.walk("."):
        # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
        dirs[:] = [d for d in dirs if d not in {".venv", "__pycache__", ".git", "node_modules"}]

        for file in files:
            if file.endswith(".py"):
                filepath = os.path.join(root, file)
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        content = f.read()

                    errors = check_file(filepath, content)
                    for line, col, msg in errors:
                        print(f"{filepath}:{line}:{col}: {msg}")
                        errors_found = True

                except Exception as e:
                    print(f"Error checking {filepath}: {e}")

    if errors_found:
        sys.exit(1)
    else:
        print("âœ… All WeGet-specific checks passed")
```

#### 23.5 Pre-commit é’©å­é…ç½®
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 23.7.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.0.287
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix, --output-format=github]

  - repo: local
    hooks:
      - id: weget-custom-checks
        name: WeGet Custom Code Checks
        entry: python scripts/ruff_weget_plugin.py
        language: system
        types: [python]
        pass_filenames: false

  - repo: local
    hooks:
      - id: check-hardcoded-secrets
        name: Check Hardcoded Secrets
        entry: bash scripts/check-hardcoded-secrets.sh
        language: system
        pass_filenames: false

  - repo: https://github.com/trufflesecurity/trufflehog
    rev: v3.54.0
    hooks:
      - id: trufflehog
        name: TruffleHog
        description: Detect secrets in your data.
        entry: bash -c 'trufflehog git file://. --since-commit HEAD --only-verified --fail'
        language: system
        stages: ["commit", "push"]
```

### 24. å¼‚æ­¥Redisä¸CeleryåŒæ ˆé›†æˆæµ‹è¯•

#### 24.1 Rediså…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
```python
# tests/integration/test_redis_celery_compatibility.py
import pytest
import asyncio
import redis.asyncio as redis
import redis as sync_redis
from celery import Celery
from celery.result import AsyncResult
import json
import time
from datetime import datetime, timedelta
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class TestRedisCeleryCompatibility:
    """æµ‹è¯•å¼‚æ­¥Rediså®¢æˆ·ç«¯ä¸Celery backendçš„å…¼å®¹æ€§"""

    @pytest.fixture
    async def async_redis_client(self):
        """å¼‚æ­¥Rediså®¢æˆ·ç«¯"""
        client = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
            health_check_interval=30
        )
        yield client
        await client.close()

    @pytest.fixture
    def sync_redis_client(self):
        """åŒæ­¥Rediså®¢æˆ·ç«¯ï¼ˆCeleryä½¿ç”¨ï¼‰"""
        client = sync_redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
            health_check_interval=30
        )
        yield client
        client.close()

    @pytest.fixture
    def celery_app(self, sync_redis_client):
        """Celeryåº”ç”¨å®ä¾‹"""
        app = Celery('test_app')
        app.conf.update(
            broker_url='redis://localhost:6379/0',
            result_backend='redis://localhost:6379/0',
            task_serializer='json',
            accept_content=['json'],
            result_serializer='json',
            timezone='UTC',
            enable_utc=True,
            task_track_started=True,
            task_time_limit=30 * 60,
            task_soft_time_limit=60,
            worker_prefetch_multiplier=1,
            task_acks_late=True,
            worker_disable_rate_limits=False,
            task_compression='gzip',
            result_compression='gzip',
            result_expires=3600,
        )

        @app.task(bind=True)
        def test_task(self, data: Dict[str, Any]):
            """æµ‹è¯•ä»»åŠ¡"""
            task_id = self.request.id
            logger.info(f"Processing task {task_id} with data: {data}")

            # æ¨¡æ‹Ÿä¸€äº›å¤„ç†æ—¶é—´
            time.sleep(0.1)

            # è¿”å›å¤„ç†ç»“æœ
            return {
                'task_id': task_id,
                'processed_at': datetime.utcnow().isoformat(),
                'input_data': data,
                'status': 'completed'
            }

        app.test_task = test_task
        yield app

    @pytest.mark.asyncio
    async def test_basic_redis_operations(self, async_redis_client, sync_redis_client):
        """æµ‹è¯•åŸºæœ¬Redisæ“ä½œçš„å…¼å®¹æ€§"""
        test_key = "test:compatibility"
        test_value = {"message": "hello", "timestamp": datetime.utcnow().isoformat()}

        # å¼‚æ­¥å®¢æˆ·ç«¯å†™å…¥
        await async_redis_client.set(test_key, json.dumps(test_value))

        # åŒæ­¥å®¢æˆ·ç«¯è¯»å–
        sync_result = sync_redis_client.get(test_key)
        assert sync_result is not None
        sync_data = json.loads(sync_result)
        assert sync_data["message"] == test_value["message"]

        # åŒæ­¥å®¢æˆ·ç«¯å†™å…¥
        new_value = {"message": "world", "timestamp": datetime.utcnow().isoformat()}
        sync_redis_client.set(test_key, json.dumps(new_value))

        # å¼‚æ­¥å®¢æˆ·ç«¯è¯»å–
        async_result = await async_redis_client.get(test_key)
        assert async_result is not None
        async_data = json.loads(async_result)
        assert async_data["message"] == new_value["message"]

        # æ¸…ç†
        await async_redis_client.delete(test_key)

    @pytest.mark.asyncio
    async def test_celery_task_execution(self, async_redis_client, celery_app):
        """æµ‹è¯•Celeryä»»åŠ¡æ‰§è¡Œä¸å¼‚æ­¥Redisçš„å…¼å®¹æ€§"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = {
            "user_id": "test_user_123",
            "action": "scrape_profile",
            "timestamp": datetime.utcnow().isoformat()
        }

        # æäº¤Celeryä»»åŠ¡
        result = celery_app.test_task.delay(test_data)
        task_id = result.id

        # ä½¿ç”¨å¼‚æ­¥Redisç›‘æ§ä»»åŠ¡çŠ¶æ€
        max_wait = 30  # æœ€å¤§ç­‰å¾…30ç§’
        start_time = time.time()

        while time.time() - start_time < max_wait:
            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            task_status = await async_redis_client.get(f"celery-task-meta-{task_id}")

            if task_status:
                status_data = json.loads(task_status)
                if status_data.get("status") == "SUCCESS":
                    break

            await asyncio.sleep(0.1)

        # éªŒè¯ä»»åŠ¡å®Œæˆ
        assert result.ready(), "Task should be completed"
        assert result.successful(), "Task should be successful"

        task_result = result.get(timeout=10)
        assert task_result["input_data"] == test_data
        assert task_result["status"] == "completed"

    @pytest.mark.asyncio
    async def test_concurrent_operations(self, async_redis_client, sync_redis_client, celery_app):
        """æµ‹è¯•å¹¶å‘æ“ä½œä¸‹çš„å…¼å®¹æ€§"""
        num_tasks = 10
        tasks = []

        # å¹¶å‘æäº¤å¤šä¸ªCeleryä»»åŠ¡
        for i in range(num_tasks):
            test_data = {
                "batch_id": f"batch_{i}",
                "timestamp": datetime.utcnow().isoformat()
            }
            result = celery_app.test_task.delay(test_data)
            tasks.append(result)

        # ä½¿ç”¨å¼‚æ­¥Rediså¹¶å‘ç›‘æ§æ‰€æœ‰ä»»åŠ¡
        async def monitor_task(task_result):
            task_id = task_result.id
            max_wait = 30
            start_time = time.time()

            while time.time() - start_time < max_wait:
                task_status = await async_redis_client.get(f"celery-task-meta-{task_id}")
                if task_status:
                    status_data = json.loads(task_status)
                    if status_data.get("status") == "SUCCESS":
                        return task_result.get(timeout=5)
                await asyncio.sleep(0.1)

            raise TimeoutError(f"Task {task_id} did not complete in time")

        # å¹¶å‘ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*[monitor_task(task) for task in tasks])

        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ
        assert len(results) == num_tasks
        for i, result in enumerate(results):
            assert result["input_data"]["batch_id"] == f"batch_{i}"
            assert result["status"] == "completed"

    @pytest.mark.asyncio
    async def test_connection_pool_isolation(self, async_redis_client, sync_redis_client):
        """æµ‹è¯•è¿æ¥æ± éš”ç¦»"""
        # è·å–è¿æ¥æ± ä¿¡æ¯
        async_pool_info = await async_redis_client.connection_pool.get_connection('_')
        sync_pool_info = sync_redis_client.connection_pool.get_connection('_')

        # éªŒè¯è¿æ¥æ± æ˜¯ç‹¬ç«‹çš„
        assert async_pool_info != sync_pool_info

        # æµ‹è¯•é«˜å¹¶å‘ä¸‹çš„è¿æ¥æ± ç¨³å®šæ€§
        async def stress_test_async():
            for _ in range(100):
                await async_redis_client.ping()
                await asyncio.sleep(0.001)

        def stress_test_sync():
            for _ in range(100):
                sync_redis_client.ping()
                time.sleep(0.001)

        # å¹¶å‘æ‰§è¡Œå‹åŠ›æµ‹è¯•
        import threading
        sync_thread = threading.Thread(target=stress_test_sync)
        sync_thread.start()

        await stress_test_async()
        sync_thread.join()

        # éªŒè¯è¿æ¥æ± ä»ç„¶æ­£å¸¸
        assert await async_redis_client.ping()
        assert sync_redis_client.ping()

    @pytest.mark.asyncio
    async def test_memory_leak_detection(self, async_redis_client, sync_redis_client, celery_app):
        """æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹"""
        import psutil
        import gc

        process = psutil.Process()
        initial_memory = process.memory_info().rss

        # æ‰§è¡Œå¤§é‡æ“ä½œ
        for batch in range(5):
            # æäº¤æ‰¹é‡ä»»åŠ¡
            tasks = []
            for i in range(20):
                test_data = {"batch": batch, "item": i}
                result = celery_app.test_task.delay(test_data)
                tasks.append(result)

            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            for task in tasks:
                task.get(timeout=30)

            # æ‰§è¡Œå¤§é‡Redisæ“ä½œ
            for i in range(100):
                key = f"test:memory:{batch}:{i}"
                await async_redis_client.set(key, f"value_{i}")
                value = await async_redis_client.get(key)
                await async_redis_client.delete(key)

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            current_memory = process.memory_info().rss
            memory_growth = current_memory - initial_memory
            memory_growth_mb = memory_growth / 1024 / 1024

            logger.info(f"Batch {batch}: Memory growth: {memory_growth_mb:.2f} MB")

            # å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡100MB
            assert memory_growth_mb < 100, f"Memory leak detected: {memory_growth_mb:.2f} MB growth"

    @pytest.mark.asyncio
    async def test_error_handling_compatibility(self, async_redis_client, sync_redis_client, celery_app):
        """æµ‹è¯•é”™è¯¯å¤„ç†å…¼å®¹æ€§"""

        @celery_app.task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3})
        def failing_task(self, should_fail=True):
            if should_fail:
                raise ValueError("Intentional failure for testing")
            return {"status": "success"}

        # æµ‹è¯•ä»»åŠ¡å¤±è´¥å¤„ç†
        result = failing_task.delay(should_fail=True)

        # ä½¿ç”¨å¼‚æ­¥Redisç›‘æ§å¤±è´¥ä»»åŠ¡
        max_wait = 60  # ç­‰å¾…é‡è¯•å®Œæˆ
        start_time = time.time()

        while time.time() - start_time < max_wait:
            task_status = await async_redis_client.get(f"celery-task-meta-{result.id}")
            if task_status:
                status_data = json.loads(task_status)
                if status_data.get("status") in ["FAILURE", "RETRY"]:
                    break
            await asyncio.sleep(0.1)

        # éªŒè¯ä»»åŠ¡æœ€ç»ˆå¤±è´¥
        with pytest.raises(ValueError):
            result.get(timeout=10)

        # éªŒè¯é”™è¯¯ä¿¡æ¯å¯ä»¥é€šè¿‡å¼‚æ­¥Redisè¯»å–
        task_status = await async_redis_client.get(f"celery-task-meta-{result.id}")
        assert task_status is not None
        status_data = json.loads(task_status)
        assert status_data["status"] == "FAILURE"
```

#### 24.2 æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
# tests/integration/test_redis_performance.py
import pytest
import asyncio
import redis.asyncio as redis
import redis as sync_redis
import time
import statistics
from typing import List
import logging

logger = logging.getLogger(__name__)

class TestRedisPerformance:
    """Redisæ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    async def async_redis_client(self):
        client = redis.Redis(
            host='localhost',
            port=6379,
            db=1,  # ä½¿ç”¨ä¸åŒçš„æ•°æ®åº“é¿å…å†²çª
            decode_responses=True,
            max_connections=20
        )
        yield client
        await client.close()

    @pytest.fixture
    def sync_redis_client(self):
        client = sync_redis.Redis(
            host='localhost',
            port=6379,
            db=1,
            decode_responses=True,
            max_connections=20
        )
        yield client
        client.close()

    @pytest.mark.asyncio
    async def test_throughput_comparison(self, async_redis_client, sync_redis_client):
        """æ¯”è¾ƒå¼‚æ­¥å’ŒåŒæ­¥Rediså®¢æˆ·ç«¯çš„ååé‡"""
        num_operations = 1000

        # æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯
        start_time = time.time()
        tasks = []
        for i in range(num_operations):
            task = async_redis_client.set(f"async:test:{i}", f"value_{i}")
            tasks.append(task)

        await asyncio.gather(*tasks)
        async_duration = time.time() - start_time

        # æ¸…ç†
        await async_redis_client.flushdb()

        # æµ‹è¯•åŒæ­¥å®¢æˆ·ç«¯
        start_time = time.time()
        for i in range(num_operations):
            sync_redis_client.set(f"sync:test:{i}", f"value_{i}")
        sync_duration = time.time() - start_time

        # æ¸…ç†
        sync_redis_client.flushdb()

        async_ops_per_sec = num_operations / async_duration
        sync_ops_per_sec = num_operations / sync_duration

        logger.info(f"Async Redis: {async_ops_per_sec:.2f} ops/sec")
        logger.info(f"Sync Redis: {sync_ops_per_sec:.2f} ops/sec")

        # å¼‚æ­¥å®¢æˆ·ç«¯åº”è¯¥æœ‰æ›´å¥½çš„å¹¶å‘æ€§èƒ½
        assert async_ops_per_sec > sync_ops_per_sec * 0.8  # è‡³å°‘80%çš„æ€§èƒ½

    @pytest.mark.asyncio
    async def test_latency_distribution(self, async_redis_client):
        """æµ‹è¯•å»¶è¿Ÿåˆ†å¸ƒ"""
        num_operations = 1000
        latencies = []

        for i in range(num_operations):
            start_time = time.time()
            await async_redis_client.set(f"latency:test:{i}", f"value_{i}")
            latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            latencies.append(latency)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th percentile

        logger.info(f"Mean latency: {mean_latency:.2f}ms")
        logger.info(f"P95 latency: {p95_latency:.2f}ms")
        logger.info(f"P99 latency: {p99_latency:.2f}ms")

        # æ€§èƒ½è¦æ±‚
        assert mean_latency < 10.0, f"Mean latency too high: {mean_latency:.2f}ms"
        assert p95_latency < 50.0, f"P95 latency too high: {p95_latency:.2f}ms"
        assert p99_latency < 100.0, f"P99 latency too high: {p99_latency:.2f}ms"

        # æ¸…ç†
        await async_redis_client.flushdb()

    @pytest.mark.asyncio
    async def test_connection_pool_efficiency(self, async_redis_client):
        """æµ‹è¯•è¿æ¥æ± æ•ˆç‡"""
        num_concurrent = 50
        operations_per_task = 20

        async def worker_task(worker_id: int):
            """å·¥ä½œä»»åŠ¡"""
            for i in range(operations_per_task):
                key = f"worker:{worker_id}:op:{i}"
                await async_redis_client.set(key, f"value_{i}")
                value = await async_redis_client.get(key)
                assert value == f"value_{i}"
                await async_redis_client.delete(key)

        # å¹¶å‘æ‰§è¡Œå·¥ä½œä»»åŠ¡
        start_time = time.time()
        tasks = [worker_task(i) for i in range(num_concurrent)]
        await asyncio.gather(*tasks)
        duration = time.time() - start_time

        total_operations = num_concurrent * operations_per_task * 3  # set, get, delete
        ops_per_sec = total_operations / duration

        logger.info(f"Connection pool efficiency: {ops_per_sec:.2f} ops/sec with {num_concurrent} concurrent workers")

        # åº”è¯¥èƒ½å¤Ÿå¤„ç†é«˜å¹¶å‘
        assert ops_per_sec > 1000, f"Connection pool efficiency too low: {ops_per_sec:.2f} ops/sec"
```

### 25. æ•°æ®å½’æ¡£ä½œä¸šå’Œç›‘æ§ç³»ç»Ÿ

#### 25.1 Parquet æ•°æ®å½’æ¡£ä»»åŠ¡
```python
# core/archive_old_data.py
import asyncio
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
import os
from pathlib import Path
import boto3
from botocore.exceptions import ClientError
import gzip
import json
from mongoengine import connect, disconnect
from models.twitter_models import Tweet, User, Relationship
import redis.asyncio as redis
from prometheus_client import Counter, Histogram, Gauge

logger = logging.getLogger(__name__)

# Prometheus æŒ‡æ ‡
archive_processed_total = Counter('archive_processed_total', 'Total archived records', ['data_type'])
archive_duration_seconds = Histogram('archive_duration_seconds', 'Archive operation duration', ['operation'])
archive_compression_ratio = Gauge('archive_compression_ratio', 'Archive compression ratio', ['data_type'])
archive_storage_bytes = Gauge('archive_storage_bytes', 'Archive storage size in bytes', ['data_type'])

class DataArchiver:
    """æ•°æ®å½’æ¡£å™¨ - å°†è€æ•°æ®å½’æ¡£åˆ° Parquet + S3"""

    def __init__(self,
                 mongodb_uri: str,
                 s3_endpoint: str,
                 s3_access_key: str,
                 s3_secret_key: str,
                 s3_bucket: str,
                 redis_client: Optional[redis.Redis] = None):

        self.mongodb_uri = mongodb_uri
        self.s3_bucket = s3_bucket
        self.redis = redis_client

        # S3 å®¢æˆ·ç«¯é…ç½®
        self.s3_client = boto3.client(
            's3',
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_access_key=s3_secret_key,
            region_name='us-east-1'
        )

        # å½’æ¡£é…ç½®
        self.archive_config = {
            'tweets': {
                'retention_days': 90,
                'batch_size': 10000,
                'model': Tweet
            },
            'users': {
                'retention_days': 180,
                'batch_size': 5000,
                'model': User
            },
            'relationships': {
                'retention_days': 365,
                'batch_size': 20000,
                'model': Relationship
            }
        }

    async def archive_old_data(self, data_type: str) -> Dict[str, any]:
        """å½’æ¡£æŒ‡å®šç±»å‹çš„è€æ•°æ®"""
        with archive_duration_seconds.labels(operation='full_archive').time():
            logger.info(f"Starting archive process for {data_type}")

            config = self.archive_config.get(data_type)
            if not config:
                raise ValueError(f"Unknown data type: {data_type}")

            # è¿æ¥MongoDB
            connect(host=self.mongodb_uri)

            try:
                # è®¡ç®—æˆªæ­¢æ—¥æœŸ
                cutoff_date = datetime.utcnow() - timedelta(days=config['retention_days'])

                # æŸ¥è¯¢éœ€è¦å½’æ¡£çš„æ•°æ®
                model = config['model']
                query = model.objects(created_at__lt=cutoff_date)
                total_count = query.count()

                if total_count == 0:
                    logger.info(f"No {data_type} data to archive")
                    return {'archived_count': 0, 'file_size': 0, 'compression_ratio': 0}

                logger.info(f"Found {total_count} {data_type} records to archive")

                # åˆ†æ‰¹å¤„ç†
                archived_count = 0
                batch_size = config['batch_size']

                for offset in range(0, total_count, batch_size):
                    batch_data = query.skip(offset).limit(batch_size)

                    # è½¬æ¢ä¸ºDataFrame
                    df = await self._convert_to_dataframe(batch_data, data_type)

                    # ç”Ÿæˆæ–‡ä»¶å
                    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                    filename = f"{data_type}/year={cutoff_date.year}/month={cutoff_date.month:02d}/batch_{offset}_{timestamp}.parquet"

                    # ä¿å­˜åˆ°Parquetå¹¶ä¸Šä¼ S3
                    file_info = await self._save_and_upload_parquet(df, filename, data_type)

                    # åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
                    batch_ids = [doc.id for doc in batch_data]
                    model.objects(id__in=batch_ids).delete()

                    archived_count += len(batch_ids)

                    # æ›´æ–°è¿›åº¦
                    if self.redis:
                        progress = (archived_count / total_count) * 100
                        await self.redis.set(f"archive:progress:{data_type}", f"{progress:.1f}")

                    logger.info(f"Archived batch {offset//batch_size + 1}, total: {archived_count}/{total_count}")

                # æ›´æ–°æŒ‡æ ‡
                archive_processed_total.labels(data_type=data_type).inc(archived_count)

                # è®¡ç®—æ€»ä½“ç»Ÿè®¡
                total_stats = await self._calculate_archive_stats(data_type, cutoff_date)

                logger.info(f"Archive completed for {data_type}: {archived_count} records")
                return total_stats

            finally:
                disconnect()

    async def _convert_to_dataframe(self, documents, data_type: str) -> pd.DataFrame:
        """å°†MongoDBæ–‡æ¡£è½¬æ¢ä¸ºDataFrame"""
        with archive_duration_seconds.labels(operation='convert_dataframe').time():
            data = []

            for doc in documents:
                # è½¬æ¢ä¸ºå­—å…¸
                doc_dict = doc.to_mongo().to_dict()

                # å¤„ç†ç‰¹æ®Šå­—æ®µ
                if '_id' in doc_dict:
                    doc_dict['_id'] = str(doc_dict['_id'])

                # å¤„ç†æ—¥æœŸå­—æ®µ
                for field, value in doc_dict.items():
                    if isinstance(value, datetime):
                        doc_dict[field] = value.isoformat()
                    elif isinstance(value, dict):
                        doc_dict[field] = json.dumps(value)
                    elif isinstance(value, list):
                        doc_dict[field] = json.dumps(value)

                data.append(doc_dict)

            return pd.DataFrame(data)

    async def _save_and_upload_parquet(self, df: pd.DataFrame, filename: str, data_type: str) -> Dict:
        """ä¿å­˜ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°S3"""
        with archive_duration_seconds.labels(operation='save_upload').time():
            # æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
            local_path = f"/tmp/{filename.replace('/', '_')}"
            os.makedirs(os.path.dirname(local_path), exist_ok=True)

            # ä¿å­˜ä¸ºParquet
            table = pa.Table.from_pandas(df)

            # ä½¿ç”¨å‹ç¼©
            pq.write_table(
                table,
                local_path,
                compression='snappy',
                use_dictionary=True,
                row_group_size=5000
            )

            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(local_path)

            # è®¡ç®—å‹ç¼©æ¯”ï¼ˆä¼°ç®—ï¼‰
            estimated_raw_size = len(df) * 1000  # ä¼°ç®—æ¯è¡Œ1KB
            compression_ratio = estimated_raw_size / file_size if file_size > 0 else 0

            # ä¸Šä¼ åˆ°S3
            try:
                self.s3_client.upload_file(local_path, self.s3_bucket, filename)
                logger.info(f"Uploaded {filename} to S3 ({file_size} bytes)")

                # æ›´æ–°æŒ‡æ ‡
                archive_compression_ratio.labels(data_type=data_type).set(compression_ratio)
                archive_storage_bytes.labels(data_type=data_type).inc(file_size)

            except ClientError as e:
                logger.error(f"Failed to upload {filename} to S3: {str(e)}")
                raise
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(local_path):
                    os.remove(local_path)

            return {
                'filename': filename,
                'file_size': file_size,
                'compression_ratio': compression_ratio,
                'record_count': len(df)
            }

    async def _calculate_archive_stats(self, data_type: str, cutoff_date: datetime) -> Dict:
        """è®¡ç®—å½’æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        # ä»S3è·å–å½’æ¡£æ–‡ä»¶ç»Ÿè®¡
        prefix = f"{data_type}/year={cutoff_date.year}/month={cutoff_date.month:02d}/"

        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=prefix
            )

            total_size = 0
            file_count = 0

            if 'Contents' in response:
                for obj in response['Contents']:
                    total_size += obj['Size']
                    file_count += 1

            return {
                'archived_files': file_count,
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'archive_date': cutoff_date.isoformat(),
                'data_type': data_type
            }

        except ClientError as e:
            logger.error(f"Failed to get S3 stats: {str(e)}")
            return {'error': str(e)}

# Celery ä»»åŠ¡å®šä¹‰
from celery import Celery

app = Celery('archive_tasks')

@app.task(bind=True, max_retries=3)
def archive_data_task(self, data_type: str):
    """Celery å½’æ¡£ä»»åŠ¡"""
    import os

    try:
        archiver = DataArchiver(
            mongodb_uri=os.getenv('MONGODB_URI'),
            s3_endpoint=os.getenv('S3_ENDPOINT'),
            s3_access_key=os.getenv('S3_ACCESS_KEY'),
            s3_secret_key=os.getenv('S3_SECRET_KEY'),
            s3_bucket=os.getenv('S3_BUCKET', 'weget-archive')
        )

        # è¿è¡Œå½’æ¡£
        result = asyncio.run(archiver.archive_old_data(data_type))

        logger.info(f"Archive task completed for {data_type}: {result}")
        return result

    except Exception as e:
        logger.error(f"Archive task failed for {data_type}: {str(e)}")

        # é‡è¯•é€»è¾‘
        if self.request.retries < self.max_retries:
            # æŒ‡æ•°é€€é¿
            countdown = 2 ** self.request.retries * 60  # 1åˆ†é’Ÿ, 2åˆ†é’Ÿ, 4åˆ†é’Ÿ
            raise self.retry(countdown=countdown, exc=e)
        else:
            # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•åˆ°æ­»ä¿¡é˜Ÿåˆ—
            logger.error(f"Archive task permanently failed for {data_type} after {self.max_retries} retries")
            raise
```

#### 25.2 Celery Beat è°ƒåº¦é…ç½®
```python
# core/celery_beat_config.py
from celery.schedules import crontab

# Celery Beat è°ƒåº¦é…ç½®
CELERYBEAT_SCHEDULE = {
    # æ¯å¤©å‡Œæ™¨2ç‚¹å½’æ¡£æ¨æ–‡æ•°æ®
    'archive-tweets-daily': {
        'task': 'core.archive_old_data.archive_data_task',
        'schedule': crontab(hour=2, minute=0),
        'args': ('tweets',),
        'options': {
            'queue': 'archive',
            'priority': 3,  # ä½ä¼˜å…ˆçº§
            'expires': 3600 * 6,  # 6å°æ—¶è¿‡æœŸ
        }
    },

    # æ¯å‘¨å½’æ¡£ç”¨æˆ·æ•°æ®
    'archive-users-weekly': {
        'task': 'core.archive_old_data.archive_data_task',
        'schedule': crontab(hour=3, minute=0, day_of_week=0),  # å‘¨æ—¥å‡Œæ™¨3ç‚¹
        'args': ('users',),
        'options': {
            'queue': 'archive',
            'priority': 3,
            'expires': 3600 * 12,  # 12å°æ—¶è¿‡æœŸ
        }
    },

    # æ¯æœˆå½’æ¡£å…³ç³»æ•°æ®
    'archive-relationships-monthly': {
        'task': 'core.archive_old_data.archive_data_task',
        'schedule': crontab(hour=4, minute=0, day=1),  # æ¯æœˆ1å·å‡Œæ™¨4ç‚¹
        'args': ('relationships',),
        'options': {
            'queue': 'archive',
            'priority': 3,
            'expires': 3600 * 24,  # 24å°æ—¶è¿‡æœŸ
        }
    },

    # æ¯å°æ—¶æ¸…ç†ä¸´æ—¶æ•°æ®
    'cleanup-temp-data': {
        'task': 'core.cleanup_tasks.cleanup_temp_data',
        'schedule': crontab(minute=0),  # æ¯å°æ—¶æ•´ç‚¹
        'options': {
            'queue': 'maintenance',
            'priority': 2,
        }
    },

    # æ¯å¤©ç”Ÿæˆå½’æ¡£æŠ¥å‘Š
    'generate-archive-report': {
        'task': 'core.archive_old_data.generate_archive_report',
        'schedule': crontab(hour=6, minute=0),  # æ¯å¤©æ—©ä¸Š6ç‚¹
        'options': {
            'queue': 'reports',
            'priority': 4,
        }
    }
}

# Celery é…ç½®
CELERY_CONFIG = {
    'beat_schedule': CELERYBEAT_SCHEDULE,
    'timezone': 'UTC',
    'enable_utc': True,

    # é˜Ÿåˆ—é…ç½®
    'task_routes': {
        'core.archive_old_data.*': {'queue': 'archive'},
        'core.cleanup_tasks.*': {'queue': 'maintenance'},
        'core.reports.*': {'queue': 'reports'},
    },

    # å·¥ä½œå™¨é…ç½®
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_max_tasks_per_child': 100,

    # ç»“æœåç«¯
    'result_backend': 'redis://redis:6379/1',
    'result_expires': 3600 * 24,  # ç»“æœä¿å­˜24å°æ—¶

    # ä»»åŠ¡åºåˆ—åŒ–
    'task_serializer': 'json',
    'accept_content': ['json'],
    'result_serializer': 'json',

    # ç›‘æ§
    'worker_send_task_events': True,
    'task_send_sent_event': True,
}
```

#### 25.3 Helm CronJob é…ç½®
```yaml
# weget-chart/templates/cronjob-archive.yaml
{{- if .Values.archive.enabled }}
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "weget.fullname" . }}-archive-tweets
  labels:
    {{- include "weget.labels" . | nindent 4 }}
    component: archive
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            {{- include "weget.selectorLabels" . | nindent 12 }}
            component: archive
        spec:
          restartPolicy: OnFailure
          containers:
          - name: archive-tweets
            image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
            command: ["python", "-m", "core.archive_old_data"]
            args: ["tweets"]
            env:
            - name: MONGODB_URI
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-database-secret
                  key: mongodb-uri
            - name: S3_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-endpoint
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-access-key
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-secret-key
            - name: S3_BUCKET
              value: {{ .Values.archive.s3Bucket | default "weget-archive" }}
            resources:
              limits:
                cpu: {{ .Values.archive.resources.limits.cpu | default "1000m" }}
                memory: {{ .Values.archive.resources.limits.memory | default "2Gi" }}
              requests:
                cpu: {{ .Values.archive.resources.requests.cpu | default "500m" }}
                memory: {{ .Values.archive.resources.requests.memory | default "1Gi" }}
            volumeMounts:
            - name: tmp-volume
              mountPath: /tmp
          volumes:
          - name: tmp-volume
            emptyDir:
              sizeLimit: 10Gi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "weget.fullname" . }}-archive-users
  labels:
    {{- include "weget.labels" . | nindent 4 }}
    component: archive
spec:
  schedule: "0 3 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            {{- include "weget.selectorLabels" . | nindent 12 }}
            component: archive
        spec:
          restartPolicy: OnFailure
          containers:
          - name: archive-users
            image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
            command: ["python", "-m", "core.archive_old_data"]
            args: ["users"]
            env:
            - name: MONGODB_URI
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-database-secret
                  key: mongodb-uri
            - name: S3_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-endpoint
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-access-key
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "weget.fullname" . }}-storage-secret
                  key: s3-secret-key
            - name: S3_BUCKET
              value: {{ .Values.archive.s3Bucket | default "weget-archive" }}
            resources:
              limits:
                cpu: {{ .Values.archive.resources.limits.cpu | default "1000m" }}
                memory: {{ .Values.archive.resources.limits.memory | default "2Gi" }}
              requests:
                cpu: {{ .Values.archive.resources.requests.cpu | default "500m" }}
                memory: {{ .Values.archive.resources.requests.memory | default "1Gi" }}
            volumeMounts:
            - name: tmp-volume
              mountPath: /tmp
          volumes:
          - name: tmp-volume
            emptyDir:
              sizeLimit: 5Gi
{{- end }}
```

#### 25.4 Prometheus ç›‘æ§è§„åˆ™
```yaml
# weget-chart/templates/prometheusrule-archive.yaml
{{- if .Values.monitoring.prometheus.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "weget.fullname" . }}-archive-rules
  labels:
    {{- include "weget.labels" . | nindent 4 }}
    component: monitoring
spec:
  groups:
  - name: weget.archive
    rules:
    # å½’æ¡£ä»»åŠ¡å¤±è´¥å‘Šè­¦
    - alert: ArchiveTaskFailed
      expr: |
        increase(celery_task_failed_total{task=~".*archive.*"}[1h]) > 0
      for: 5m
      labels:
        severity: critical
        service: weget
        component: archive
      annotations:
        summary: "Archive task failed"
        description: "Archive task {{ $labels.task }} has failed {{ $value }} times in the last hour"

    # å½’æ¡£ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿
    - alert: ArchiveTaskTooSlow
      expr: |
        histogram_quantile(0.95, rate(archive_duration_seconds_bucket[1h])) > 3600
      for: 10m
      labels:
        severity: warning
        service: weget
        component: archive
      annotations:
        summary: "Archive task taking too long"
        description: "Archive task P95 duration is {{ $value | humanizeDuration }}, exceeding 1 hour threshold"

    # æ•°æ®åº“å¢é•¿è¿‡å¿«
    - alert: DatabaseGrowthTooFast
      expr: |
        rate(mongodb_collection_size_bytes[1h]) * 3600 * 24 > 10737418240  # 10GB per day
      for: 30m
      labels:
        severity: warning
        service: weget
        component: database
      annotations:
        summary: "Database growing too fast"
        description: "Database is growing at {{ $value | humanizeBytes }}/day, may need more frequent archiving"

    # S3 å­˜å‚¨ç©ºé—´å‘Šè­¦
    - alert: ArchiveStorageHigh
      expr: |
        sum(archive_storage_bytes) > 1099511627776  # 1TB
      for: 1h
      labels:
        severity: warning
        service: weget
        component: storage
      annotations:
        summary: "Archive storage usage high"
        description: "Archive storage usage is {{ $value | humanizeBytes }}, approaching limits"

    # å‹ç¼©æ¯”å¼‚å¸¸
    - alert: ArchiveCompressionRatioLow
      expr: |
        avg(archive_compression_ratio) < 5
      for: 1h
      labels:
        severity: info
        service: weget
        component: archive
      annotations:
        summary: "Archive compression ratio is low"
        description: "Average compression ratio is {{ $value }}, may indicate data quality issues"

  - name: weget.archive.recording
    rules:
    # è®°å½•è§„åˆ™ï¼šæ¯å°æ—¶å½’æ¡£æ•°æ®é‡
    - record: weget:archive_hourly_rate
      expr: |
        rate(archive_processed_total[1h])

    # è®°å½•è§„åˆ™ï¼šå½’æ¡£æ•ˆç‡ï¼ˆè®°å½•æ•°/ç§’ï¼‰
    - record: weget:archive_efficiency
      expr: |
        rate(archive_processed_total[5m]) / rate(archive_duration_seconds_count[5m])

    # è®°å½•è§„åˆ™ï¼šå­˜å‚¨å¢é•¿ç‡
    - record: weget:storage_growth_rate
      expr: |
        rate(archive_storage_bytes[1h])
{{- end }}
```

#### 25.5 Grafana ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "id": null,
    "title": "WeGet Archive Monitoring",
    "tags": ["weget", "archive"],
    "timezone": "UTC",
    "panels": [
      {
        "id": 1,
        "title": "Archive Processing Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(archive_processed_total[5m])",
            "legendFormat": "{{ data_type }} records/sec"
          }
        ],
        "yAxes": [
          {
            "label": "Records/sec",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Archive Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(archive_duration_seconds_bucket[5m]))",
            "legendFormat": "P95 Duration"
          },
          {
            "expr": "histogram_quantile(0.50, rate(archive_duration_seconds_bucket[5m]))",
            "legendFormat": "P50 Duration"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds",
            "min": 0
          }
        ]
      },
      {
        "id": 3,
        "title": "Storage Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "sum(archive_storage_bytes)",
            "legendFormat": "Total Storage"
          }
        ],
        "format": "bytes"
      },
      {
        "id": 4,
        "title": "Compression Ratio",
        "type": "graph",
        "targets": [
          {
            "expr": "archive_compression_ratio",
            "legendFormat": "{{ data_type }}"
          }
        ],
        "yAxes": [
          {
            "label": "Ratio",
            "min": 0
          }
        ]
      },
      {
        "id": 5,
        "title": "Database Collection Sizes",
        "type": "graph",
        "targets": [
          {
            "expr": "mongodb_collection_size_bytes",
            "legendFormat": "{{ collection }}"
          }
        ],
        "yAxes": [
          {
            "label": "Bytes",
            "min": 0
          }
        ]
      },
      {
        "id": 6,
        "title": "Archive Task Status",
        "type": "table",
        "targets": [
          {
            "expr": "celery_task_total{task=~\".*archive.*\"}",
            "format": "table"
          }
        ]
      }
    ],
    "time": {
      "from": "now-24h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## æ€»ç»“ä¸å®æ–½è·¯çº¿å›¾

### å…³é”®ä¼˜åŒ–æˆæœ

æœ¬æŠ€æœ¯å®æ–½æ–¹æ¡ˆé€šè¿‡ **6 ä¸ªå…³é”®ä¼˜åŒ–é¡¹** å°† WeGet ç³»ç»Ÿä»åŸå‹å‡çº§ä¸ºä¼ä¸šçº§ç”Ÿäº§ç³»ç»Ÿï¼š

#### âœ… 1. æ¶ˆé™¤æ˜æ–‡å‡­æ®å®‰å…¨æ¼æ´ (é—®é¢˜#1)
- **å®ç°**: External Secrets + HashiCorp Vault ç»Ÿä¸€å¯†é’¥ç®¡ç†ï¼Œå…¨é¢æ›¿æ¢ç¡¬ç¼–ç è¿æ¥å­—ç¬¦ä¸²
- **éªŒæ”¶æ ‡å‡†**: `grep -R "mongodb://.*:.*@"` è¿”å› 0ï¼›CI é›†æˆ TruffleHog æ‰«æ
- **å®‰å…¨æå‡**: 100% æ¶ˆé™¤ç¡¬ç¼–ç å¯†ç ï¼Œå®ç°å¯†é’¥è½®æ¢å’Œå®¡è®¡
- **é…ç½®å•æºåŒ–**: Helm è‡ªåŠ¨æ¸²æŸ“å”¯ä¸€ Docker Compose æ–‡ä»¶

#### âœ… 2. ä¿®å¤å¼‚æ­¥Rediså®¢æˆ·ç«¯é—®é¢˜ (é—®é¢˜#2-#4)
- **å®ç°**: ç»Ÿä¸€ä½¿ç”¨ `redis.asyncio`ï¼Œé‡æ„ Cookie/Proxy/BrowserPool ç®¡ç†å™¨
- **éªŒæ”¶æ ‡å‡†**: `grep "import redis"` ä»…åœ¨ SyncRedisWrapper å‡ºç°
- **æ€§èƒ½æå‡**: 5000+ è´¦å·åœºæ™¯ä¸‹ P95 å»¶è¿Ÿ < 50msï¼Œæ¶ˆé™¤äº‹ä»¶å¾ªç¯é˜»å¡
- **æ¶æ„ç»Ÿä¸€**: åˆå¹¶ä¸ºå•ä¸€ AsyncRedisManagerï¼Œç»Ÿä¸€ API

#### âœ… 3. å¼ºåŒ–CIè´¨é‡é—¨ç¦çœŸæ­£å µæ¼æ´ (é—®é¢˜#3)
- **å®ç°**: ç§»é™¤ `--exit-zero`ï¼Œæ·»åŠ è‡ªå®šä¹‰ Ruff è§„åˆ™å’Œä¸¥æ ¼æ£€æŸ¥
- **éªŒæ”¶æ ‡å‡†**: ç ´åæ€§ PR ç«‹å³ CI å¤±è´¥ï¼Œè¦†ç›–ç‡ â‰¥ 80%ï¼Œæ— å ä½ç¬¦ä»£ç 
- **è´¨é‡ä¿éšœ**: å¤šå±‚æ¬¡è´¨é‡æ£€æŸ¥ï¼Œè‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ï¼ŒWeGet ç‰¹å®šè§„åˆ™
- **é›¶å®¹é”™**: åŒæ­¥ Redis å¯¼å…¥ã€Playwright åŒæ­¥è°ƒç”¨ç­‰ç«‹å³å¤±è´¥

#### âœ… 4. ç»Ÿä¸€Redisæ¶æ„å’Œä¿®å¤BrowserPoolé˜»å¡ (é—®é¢˜#4-#5)
- **å®ç°**: BrowserPool æ”¹ç”¨ `redis.asyncio`ï¼Œåˆå¹¶å¼‚æ­¥ Redis å°è£…
- **éªŒæ”¶æ ‡å‡†**: æ‰€æœ‰ Redis æ“ä½œä½¿ç”¨ç»Ÿä¸€ AsyncRedisManager
- **æ€§èƒ½ä¼˜åŒ–**: æ¶ˆé™¤æµè§ˆå™¨æ± ä¸­çš„åŒæ­¥é˜»å¡è°ƒç”¨
- **æ¶æ„ç®€åŒ–**: å•ä¸€ Redis å®¢æˆ·ç«¯ APIï¼Œæ¶ˆé™¤é‡å¤å°è£…

#### âœ… 5. å®ç°æ•°æ®å½’æ¡£ä½œä¸šå’Œç›‘æ§ (é—®é¢˜#6)
- **å®ç°**: Parquet å½’æ¡£ä»»åŠ¡ï¼ŒCelery Beat è°ƒåº¦ï¼ŒS3 å­˜å‚¨ï¼ŒPrometheus ç›‘æ§
- **éªŒæ”¶æ ‡å‡†**: 90å¤©æ•°æ®è‡ªåŠ¨å½’æ¡£ï¼Œå‹ç¼©ç‡ > 5:1ï¼ŒMongo é›†åˆå¤§å°ç¨³å®š
- **å­˜å‚¨ä¼˜åŒ–**: å†·çƒ­æ•°æ®åˆ†ç¦»ï¼Œè‡ªåŠ¨æ¸…ç†è€æ•°æ®ï¼Œé˜²æ­¢æ•°æ®åº“æš´æ¶¨
- **ç›‘æ§å®Œå–„**: å½’æ¡£è¿›åº¦ã€å‹ç¼©æ¯”ã€å­˜å‚¨ä½¿ç”¨é‡å…¨é¢ç›‘æ§

#### âœ… 6. å¼‚æ­¥Redisä¸CeleryåŒæ ˆé›†æˆæµ‹è¯• (åŸé—®é¢˜#7)
- **å®ç°**: å®Œæ•´çš„å…¼å®¹æ€§æµ‹è¯•å¥—ä»¶ï¼Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå†…å­˜æ³„æ¼æ£€æµ‹
- **éªŒæ”¶æ ‡å‡†**: 1k RPS å‹åŠ›ä¸‹æ— è¿æ¥æ³„æ¼ï¼Œå»¶è¿Ÿ P95 < 50ms
- **ç¨³å®šæ€§**: ç”Ÿäº§çº§åŒæ ˆæ¶æ„éªŒè¯ï¼Œ50å¹¶å‘/5000IP/3å°æ—¶é•¿å‹æµ‹é€šè¿‡

### ä¸‹ä¸€æ­¥å†²åˆºè®¡åˆ’

#### ğŸš€ Sprint 1: å®‰å…¨ä¸åŸºç¡€æ¶æ„ (1å‘¨)
```bash
# ä¼˜å…ˆçº§1: å®‰å…¨æ¼æ´ä¿®å¤
â–¡ éƒ¨ç½² HashiCorp Vault é›†ç¾¤
â–¡ é…ç½® External Secrets Operator
â–¡ è¿ç§»æ‰€æœ‰å¯†é’¥åˆ° Vault
â–¡ é›†æˆ CI å®‰å…¨æ‰«æ (TruffleHog, Semgrep, Bandit)
â–¡ éªŒæ”¶: grep -R "password@" è¿”å› 0

# ä¼˜å…ˆçº§2: å¼‚æ­¥æ¶æ„ä¿®å¤
â–¡ é‡æ„ CookieManager ä½¿ç”¨ redis.asyncio
â–¡ é‡æ„ ProxyManager å¼‚æ­¥æ“ä½œ
â–¡ ä¿®å¤ TaskScheduler Redis è°ƒç”¨
â–¡ éªŒæ”¶: ruff check æ— å¼‚æ­¥è¿è§„æŠ¥å‘Š
```

#### ğŸš€ Sprint 2: é…ç½®ç®¡ç†ä¸CI (1å‘¨)
```bash
# ä¼˜å…ˆçº§3: ç»Ÿä¸€é…ç½®ç®¡ç†
â–¡ å®ç° Helm â†’ Docker Compose æ¸²æŸ“è„šæœ¬
â–¡ é…ç½®å¼€å‘ç¯å¢ƒ values æ–‡ä»¶
â–¡ é›†æˆé…ç½®éªŒè¯è„šæœ¬
â–¡ éªŒæ”¶: å•ä¸€é…ç½®æºï¼Œè‡ªåŠ¨æ¸²æŸ“

# ä¼˜å…ˆçº§4: CIè´¨é‡é—¨ç¦
â–¡ å‡çº§ GitHub Actions å·¥ä½œæµ
â–¡ é…ç½®ä¸¥æ ¼çš„ Ruff/Black/MyPy æ£€æŸ¥
â–¡ æ·»åŠ è¦†ç›–ç‡è¦æ±‚å’Œæ€§èƒ½æµ‹è¯•
â–¡ éªŒæ”¶: è´¨é‡é—¨ç¦ 100% ç”Ÿæ•ˆ
```

#### ğŸš€ Sprint 3: é›†æˆæµ‹è¯•ä¸éªŒè¯ (1å‘¨)
```bash
# ä¼˜å…ˆçº§5: åŒæ ˆå…¼å®¹æ€§æµ‹è¯•
â–¡ å®ç° Redis-Celery å…¼å®¹æ€§æµ‹è¯•å¥—ä»¶
â–¡ æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
â–¡ é…ç½®å†…å­˜æ³„æ¼æ£€æµ‹
â–¡ éªŒæ”¶: 1k RPS æ— è¿æ¥æ³„æ¼

# æœ€ç»ˆéªŒè¯
â–¡ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
â–¡ è´Ÿè½½å‹åŠ›æµ‹è¯•
â–¡ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²éªŒè¯
```

### æŠ€æœ¯å€ºåŠ¡æ¸…é›¶æ£€æŸ¥è¡¨

#### ğŸ” ä»£ç è´¨é‡æ£€æŸ¥
- [ ] **å ä½ç¬¦æ¸…é›¶**: æ—  `pass # TODO`ã€`FIXME`ã€`NotImplementedError`
- [ ] **ç±»å‹æ³¨è§£å®Œæ•´**: æ‰€æœ‰å…¬å…±æ–¹æ³•æœ‰ç±»å‹æç¤º
- [ ] **æ–‡æ¡£è¦†ç›–**: æ‰€æœ‰æ¨¡å—æœ‰ docstring
- [ ] **æµ‹è¯•è¦†ç›–**: æ ¸å¿ƒæ¨¡å—è¦†ç›–ç‡ â‰¥ 85%

#### ğŸ”’ å®‰å…¨åŸºçº¿æ£€æŸ¥
- [ ] **å¯†é’¥ç®¡ç†**: 100% ä½¿ç”¨ Vaultï¼Œæ— ç¡¬ç¼–ç å¯†ç 
- [ ] **æƒé™æ§åˆ¶**: RBAC é…ç½®ï¼Œæœ€å°æƒé™åŸåˆ™
- [ ] **ç½‘ç»œå®‰å…¨**: NetworkPolicy éš”ç¦»ï¼ŒTLS åŠ å¯†
- [ ] **å®¡è®¡æ—¥å¿—**: å®Œæ•´çš„æ“ä½œå®¡è®¡é“¾

#### âš¡ æ€§èƒ½åŸºçº¿æ£€æŸ¥
- [ ] **å“åº”æ—¶é—´**: API P95 < 200msï¼ŒP99 < 500ms
- [ ] **ååé‡**: å•å®ä¾‹ â‰¥ 1000 RPS
- [ ] **èµ„æºä½¿ç”¨**: CPU < 70%ï¼Œå†…å­˜ < 80%
- [ ] **è¿æ¥æ± **: æ— æ³„æ¼ï¼Œé«˜æ•ˆå¤ç”¨

#### ğŸ› ï¸ è¿ç»´å°±ç»ªæ£€æŸ¥
- [ ] **ç›‘æ§å‘Šè­¦**: Prometheus + Grafana å®Œæ•´ç›‘æ§
- [ ] **æ—¥å¿—èšåˆ**: ELK Stack é›†ä¸­æ—¥å¿—
- [ ] **å¥åº·æ£€æŸ¥**: å¤šå±‚æ¬¡å¥åº·æ£€æŸ¥
- [ ] **æ•…éšœæ¢å¤**: è‡ªåŠ¨é‡å¯ï¼Œä¼˜é›…é™çº§

### é£é™©è¯„ä¼°ä¸ç¼“è§£

#### ğŸš¨ é«˜é£é™©é¡¹ç›®
1. **æ•°æ®åº“è¿ç§»é£é™©**
   - ç¼“è§£: åˆ†é˜¶æ®µè¿ç§»ï¼Œå®æ—¶åŒæ­¥éªŒè¯
   - å›æ»š: ä¿ç•™åŸç³»ç»Ÿï¼Œå¿«é€Ÿåˆ‡æ¢

2. **å¼‚æ­¥æ¶æ„æ”¹é€ é£é™©**
   - ç¼“è§£: æ¸è¿›å¼é‡æ„ï¼Œä¿æŒå‘åå…¼å®¹
   - éªŒè¯: å®Œæ•´çš„é›†æˆæµ‹è¯•è¦†ç›–

3. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é£é™©**
   - ç¼“è§£: è“ç»¿éƒ¨ç½²ï¼Œé‡‘ä¸é›€å‘å¸ƒ
   - ç›‘æ§: å®æ—¶æ€§èƒ½ç›‘æ§ï¼Œè‡ªåŠ¨å›æ»š

#### âš ï¸ ä¸­é£é™©é¡¹ç›®
1. **ç¬¬ä¸‰æ–¹ä¾èµ–å‡çº§**
   - ç¼“è§£: ç‰ˆæœ¬é”å®šï¼Œå…¼å®¹æ€§æµ‹è¯•

2. **é…ç½®ç®¡ç†è¿ç§»**
   - ç¼“è§£: é…ç½®éªŒè¯è„šæœ¬ï¼Œè‡ªåŠ¨åŒ–æµ‹è¯•

### æˆåŠŸæ ‡å‡†

#### ğŸ“Š é‡åŒ–æŒ‡æ ‡
- **å¯ç”¨æ€§**: 99.9% SLA (æœˆåº¦åœæœº < 43åˆ†é’Ÿ)
- **æ€§èƒ½**: P95 å“åº”æ—¶é—´ < 200ms
- **å®‰å…¨**: 0 ä¸ªé«˜å±æ¼æ´ï¼Œ100% å¯†é’¥ç®¡ç†
- **è´¨é‡**: ä»£ç è¦†ç›–ç‡ â‰¥ 80%ï¼ŒæŠ€æœ¯å€ºåŠ¡æ¸…é›¶

#### ğŸ¯ ä¸šåŠ¡ç›®æ ‡
- **æ•°æ®é‡‡é›†èƒ½åŠ›**: æ”¯æŒ 10ä¸‡+ è´¦å·å¹¶å‘é‡‡é›†
- **ç³»ç»Ÿç¨³å®šæ€§**: 7x24 å°æ—¶ç¨³å®šè¿è¡Œ
- **æ‰©å±•èƒ½åŠ›**: æ”¯æŒæ°´å¹³æ‰©å±•åˆ° 100+ å®ä¾‹
- **è¿ç»´æ•ˆç‡**: è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œä¸€é”®æ‰©ç¼©å®¹

### åç»­æ¼”è¿›æ–¹å‘

#### ğŸ”® æŠ€æœ¯æ¼”è¿›
1. **AI å¢å¼º**: é›†æˆ LLM è¿›è¡Œå†…å®¹ç†è§£å’Œåˆ†æ
2. **å®æ—¶æµå¤„ç†**: Kafka + Flink å®æ—¶æ•°æ®ç®¡é“
3. **è¾¹ç¼˜è®¡ç®—**: CDN è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½²ï¼Œé™ä½å»¶è¿Ÿ
4. **å¤šäº‘æ¶æ„**: è·¨äº‘éƒ¨ç½²ï¼Œæé«˜å¯ç”¨æ€§

#### ğŸ“ˆ ä¸šåŠ¡æ‰©å±•
1. **å¤šå¹³å°æ”¯æŒ**: æ‰©å±•åˆ° Instagramã€TikTok ç­‰å¹³å°
2. **æ•°æ®äº§å“åŒ–**: æä¾› API æœåŠ¡ï¼Œæ•°æ®è®¢é˜…
3. **æ™ºèƒ½åˆ†æ**: æƒ…æ„Ÿåˆ†æï¼Œè¶‹åŠ¿é¢„æµ‹ï¼Œç”¨æˆ·ç”»åƒ
4. **åˆè§„å¢å¼º**: GDPRã€CCPA ç­‰æ³•è§„éµå¾ª

---

## ğŸ“‹ é¡¹ç›®éªŒæ”¶

è¯¦ç»†çš„éªŒæ”¶æ£€æŸ¥æ¸…å•å’Œè„šæœ¬å·²ç§»è‡³ç‹¬ç«‹æ–‡æ¡£ï¼š**[WeGet_check.md](./WeGet_check.md)**

è¯¥æ–‡æ¡£åŒ…å«ï¼š

- ğŸ” å®Œæ•´çš„éªŒæ”¶æ£€æŸ¥æ¸…å•ï¼ˆå®‰å…¨ã€æ€§èƒ½ã€è´¨é‡ç­‰ï¼‰
- ğŸ¯ æœ€ç»ˆæˆåŠŸæ ‡å‡†å’ŒæŒ‡æ ‡
- ğŸš€ ä¸€é”®éªŒæ”¶è„šæœ¬
- âœ… é«˜é£é™©é—®é¢˜ä¿®å¤æ€»ç»“

è¯·åœ¨éƒ¨ç½²å‰è¿è¡ŒéªŒæ”¶è„šæœ¬ç¡®ä¿ç³»ç»Ÿè¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€ã€‚
